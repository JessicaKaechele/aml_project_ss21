{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwZJFe5-pCLV",
    "outputId": "34e05e6a-ae90-4a63-89e5-8d6ea1ca7996"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets\n",
    "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBD5W-SJWERa",
    "outputId": "1396852a-e233-49bb-80a1-f3fb16bf7c9f"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TraicDeCpHii",
    "outputId": "b6cdd2b7-7c26-44a8-be17-7fbbe8ce7574"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "od.download(\"https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\")\n",
    "od.download(\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WvAJ8eTnTRkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JPFiGGjbhta_"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001 # 0.0001\n",
    "MAX_EPOCHS = 10\n",
    "TARGET_FOLDER = \"weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7Z_GY07pSzQT"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((244, 244))\n",
    "                                , transforms.ToTensor()]\n",
    "                               #, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # find mean and std of dataset\n",
    "                              )\n",
    "\n",
    "test_set = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/test', transform=transform)\n",
    "\n",
    "train_set = dataset = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3Z9lep1yVWY3"
   },
   "outputs": [],
   "source": [
    "def label_preparation(labels):\n",
    "    labels = np.array(labels)\n",
    "    labels[labels > 0] = 1\n",
    "    return list(labels)\n",
    "\n",
    "def label_preparation_tensor(labels):\n",
    "    labels[labels > 0] = 1\n",
    "    return labels\n",
    "\n",
    "train_set.targets = label_preparation(train_set.targets)\n",
    "\n",
    "test_set.targets = label_preparation(test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "icqqTTRN-ht6"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(result, labels):\n",
    "    result = torch.sigmoid(result).round()\n",
    "    \n",
    "    correct_results_sum = (result == labels).sum().float()\n",
    "    acc = correct_results_sum/labels.shape[0]\n",
    "    acc *= 100\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BNNFC2o-VWZC"
   },
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, loss):\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(data_loader, 1):\n",
    "        labels = label_preparation_tensor(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result = model(images)\n",
    "        targets = labels.float()\n",
    "        \n",
    "        # normal dataloader and custom dataloader return different sized targets\n",
    "        # normal has a shape of [32], while custom dataloader (correctly) uses [32, 1]\n",
    "        if len(targets.shape) == 1:\n",
    "            targets = targets.unsqueeze(1)\n",
    "\n",
    "        loss_value = loss(result.float(), targets)\n",
    "\n",
    "        # backpropagation\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        #if step % 10 == 0:\n",
    "        #    accuracy += calc_accuracy(result, targets)\n",
    "        #    print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nQ_ek0ayVWZC"
   },
   "outputs": [],
   "source": [
    "def test_fn(model, test_loader, loss):\n",
    "    with torch.no_grad():\n",
    "        loss_value = 0\n",
    "        accuracy = 0\n",
    "        for step, [images, labels] in enumerate(test_loader, 1):\n",
    "            labels = label_preparation_tensor(labels)\n",
    "\n",
    "            result = model(images)\n",
    "            targets = labels.detach().unsqueeze(1).float()\n",
    "\n",
    "            loss_value += loss(result.detach(), targets)\n",
    "            accuracy += calc_accuracy(result.detach(), targets)\n",
    "\n",
    "        loss_value /= step\n",
    "        accuracy /=  step\n",
    "        \n",
    "    #print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
    "    return f\"Test accuracy is {accuracy.item():.3f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWithTransform(torch.utils.data.Dataset):\n",
    "    dataset: torch.utils.data.Dataset\n",
    "    transform: torchvision.transforms.Compose\n",
    "\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, transform: torchvision.transforms.Compose) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "                      \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        x = self.transform(x)\n",
    "        \n",
    "        if isinstance(y, int):\n",
    "            y = torch.IntTensor([y])\n",
    "        elif isinstance(y, long):\n",
    "            y = torch.LongTensor([y])\n",
    "        elif isinstance(y, float):\n",
    "            y = torch.FloatTensor([y])\n",
    "        elif isinstance(y, double):\n",
    "            y = torch.DoubleTensor([y])\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Tuple, Callable, Any\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import time\n",
    "\n",
    "class FederatedLearningTest:\n",
    "    __batch_size = 32\n",
    "    __shuffle_train_data = True\n",
    "    __num_workers = 0\n",
    "    __pin_memory = True\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_dataset: torch.utils.data.Dataset,\n",
    "        test_dataset: torch.utils.data.Dataset,\n",
    "        train_epoch_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader, torch.optim.Optimizer, torch.nn.modules.loss._Loss], None], \n",
    "        test_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader, torch.nn.modules.loss._Loss], Any],\n",
    "        use_gpu: bool,\n",
    "        epochs_to_train: int,\n",
    "        local_epochs_to_train: int\n",
    "    ):\n",
    "        self.__model = model\n",
    "        self.__train_dataset = train_dataset\n",
    "        self.__test_dataset = test_dataset\n",
    "        self.__train_epoch_fn = train_epoch_fn\n",
    "        self.__test_fn = test_fn\n",
    "        self.__use_gpu = use_gpu\n",
    "        self.__epochs_to_train = epochs_to_train\n",
    "        self.__local_epochs_to_train = local_epochs_to_train\n",
    "\n",
    "    def __update_client_model(self, federated_model, client_model):\n",
    "        client_model.load_state_dict(federated_model.state_dict(), True)\n",
    "        return client_model\n",
    "\n",
    "    def __federated_average(self, federated_model, client_models):\n",
    "        average_weights = OrderedDict()\n",
    "\n",
    "        number_of_clients = len(client_models)\n",
    "        for client_model in client_models:\n",
    "            for key, value in client_model.state_dict().items():\n",
    "                if key in average_weights:\n",
    "                    average_weights[key] += (1./number_of_clients) * value.clone()\n",
    "                else:\n",
    "                    average_weights[key] = (1./number_of_clients) * value.clone()\n",
    "\n",
    "\n",
    "        federated_model.load_state_dict(average_weights, True)\n",
    "        return federated_model\n",
    "        \n",
    "    # TODO future: currently only works with image data, make it more generic\n",
    "    def __prepare_train_data(\n",
    "        self,\n",
    "        train_dataset: torch.utils.data.Dataset,\n",
    "        no_of_clients: int,\n",
    "        augment_data: bool,\n",
    "        full_data_on_each_client: bool,\n",
    "        batch_size: int, shuffle: bool, num_workers: int, pin_memory: bool\n",
    "    ) -> List[torch.utils.data.DataLoader]:\n",
    "    \n",
    "        if augment_data:\n",
    "            # flip, then rotate and shift\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "        data_loaders = []\n",
    "        if full_data_on_each_client:\n",
    "            data_loaders = [\n",
    "                torch.utils.data.DataLoader(\n",
    "                    DatasetWithTransform(train_dataset, transform),\n",
    "                    batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory\n",
    "                )\n",
    "            for _ in range(no_of_clients)]\n",
    "        else:\n",
    "            chunk_size_train = len(train_dataset) // no_of_clients\n",
    "            indices_train = np.random.permutation(np.arange(chunk_size_train * no_of_clients)) \n",
    "\n",
    "            for idx in range(no_of_clients):\n",
    "                data_loader_train = torch.utils.data.Subset(train_dataset, indices_train[idx*chunk_size_train:(idx+1)*chunk_size_train])\n",
    "                data_loaders += [\n",
    "                    torch.utils.data.DataLoader(\n",
    "                        DatasetWithTransform(data_loader_train, transform),\n",
    "                        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "        return data_loaders\n",
    "    \n",
    "    def __get_device(self):\n",
    "        if not self.__use_gpu:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(\"Using GPU\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"You requested to use GPU, but CUDA is not available. Using CPU instead\")\n",
    "        \n",
    "        return device\n",
    "    \n",
    "    def __wrap_data_loader(self, loader, device):\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            yield x, y\n",
    "    \n",
    "    def set_batch_size(self, batch_size: int) -> FederatedLearningTest:\n",
    "        self.__batch_size = batch_size\n",
    "        return self\n",
    "    \n",
    "    def set_shuffle_train_data(self, shuffle_train_data: bool) -> FederatedLearningTest:\n",
    "        self.__shuffle_train_data = shuffle_train_data\n",
    "        return self\n",
    "    \n",
    "    def set_dataloader_workers(self, num_workers: int) -> FederatedLearningTest:\n",
    "        self.__num_workers = num_workers\n",
    "        return self\n",
    "    \n",
    "    def set_pin_training_memory(self, pin_memory: bool) -> FederatedLearningTest:\n",
    "        self.__pin_memory = pin_memory\n",
    "        return self\n",
    "    \n",
    "    def compare(\n",
    "        self,\n",
    "        augment_data: bool,\n",
    "        full_data_on_each_client: bool,\n",
    "        no_of_clients: int,\n",
    "        construct_optimizer_fn: Callable[[torch.nn.Module], torch.optim.Optimizer],\n",
    "        construct_loss_fn: Callable[[], torch.nn.modules.loss._Loss],\n",
    "        test_after_each_epoch: bool = False\n",
    "    ):\n",
    "        device = self.__get_device()\n",
    "        if device.type == \"cuda\": \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        sequential_training_start = time.time()\n",
    "        print(\"Training sequential model\")\n",
    "        sequential_model = copy.deepcopy(self.__model).to(device)\n",
    "        \n",
    "        sequential_optimizer = construct_optimizer_fn(sequential_model)\n",
    "        sequential_loss = construct_loss_fn().to(device)\n",
    "        \n",
    "        sequential_train_loader = torch.utils.data.DataLoader(\n",
    "            self.__train_dataset,\n",
    "            batch_size=self.__batch_size,\n",
    "            shuffle=self.__shuffle_train_data,\n",
    "            num_workers=self.__num_workers,\n",
    "            pin_memory=self.__pin_memory\n",
    "        )\n",
    "        \n",
    "        sequential_test_loader = torch.utils.data.DataLoader(\n",
    "            self.__test_dataset,\n",
    "            batch_size=self.__batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.__num_workers\n",
    "        )\n",
    "        \n",
    "        for idx in range(self.__epochs_to_train):\n",
    "            print(f\"Epoch: {idx+1}\")\n",
    "            sequential_model.train()\n",
    "            sequential_train_loader_device = self.__wrap_data_loader(sequential_train_loader, device)\n",
    "            self.__train_epoch_fn(sequential_model, sequential_train_loader_device, sequential_optimizer, sequential_loss)\n",
    "            sequential_train_loader_device = None\n",
    "            \n",
    "            if test_after_each_epoch:\n",
    "                sequential_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    print(f\"Test results for epoch {idx+1}:\")\n",
    "                    sequential_test_loader_device = self.__wrap_data_loader(sequential_test_loader, device)\n",
    "                    print(self.__test_fn(sequential_model, sequential_test_loader_device, sequential_loss))\n",
    "                    sequential_test_loader_device = None\n",
    "        \n",
    "        print(f\"Sequential training complete after {time.time() - sequential_training_start:.2f} seconds, testing ...\")\n",
    "        \n",
    "        sequential_test_start = time.time()\n",
    "        sequential_model.eval()\n",
    "        with torch.no_grad():\n",
    "            sequential_test_loader = self.__wrap_data_loader(sequential_test_loader, device)\n",
    "            sequential_test_results = self.__test_fn(sequential_model, sequential_test_loader, sequential_loss)\n",
    "        \n",
    "        sequential_model.to(\"cpu\")\n",
    "        sequential_train_loader = None\n",
    "        sequential_test_loader = None\n",
    "        sequential_loss = None\n",
    "        sequential_optimizer = None\n",
    "        sequential_model = None\n",
    "        \n",
    "        print(f\"Sequential testing complete after {time.time() - sequential_test_start:.2f} seconds, results:\")\n",
    "        print(sequential_test_results)\n",
    "        \n",
    "        if device.type == \"cuda\": \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(\"\\n++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "        \n",
    "        federated_training_start = time.time()\n",
    "        print(\"Training federated model(s)\")\n",
    "        federated_model = copy.deepcopy(self.__model)\n",
    "        client_models = [copy.deepcopy(self.__model) for _ in range(no_of_clients)]\n",
    "        \n",
    "        federated_loss = construct_loss_fn().to(device)\n",
    "        \n",
    "        federated_training_dataloaders = self.__prepare_train_data(\n",
    "            self.__train_dataset,\n",
    "            no_of_clients,\n",
    "            augment_data,\n",
    "            full_data_on_each_client,\n",
    "            self.__batch_size,\n",
    "            self.__shuffle_train_data,\n",
    "            self.__num_workers,\n",
    "            self.__pin_memory\n",
    "        )\n",
    "        \n",
    "        federated_test_loader = torch.utils.data.DataLoader(\n",
    "            self.__test_dataset,\n",
    "            batch_size=self.__batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.__num_workers\n",
    "        )\n",
    "\n",
    "        for idx in range(self.__epochs_to_train):\n",
    "            for client_idx in range (no_of_clients):\n",
    "                print(f\"Epoch: {idx+1}, client {client_idx+1}\")\n",
    "                \n",
    "                client_model = client_models[client_idx].to(device)\n",
    "                \n",
    "                client_model = self.__update_client_model(federated_model, client_model)\n",
    "                client_optimizer = construct_optimizer_fn(client_model)\n",
    "                \n",
    "                for local_epoch_idx in range(self.__local_epochs_to_train): \n",
    "                    client_training_loader_device = self.__wrap_data_loader(federated_training_dataloaders[client_idx], device)\n",
    "                    self.__train_epoch_fn(client_model, client_training_loader_device, client_optimizer, federated_loss)\n",
    "                    client_training_loader_device = None\n",
    "\n",
    "                if device.type == \"cuda\": \n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                client_model.to(\"cpu\")\n",
    "                client_optimizer = None\n",
    "            \n",
    "            federated_model = self.__federated_average(federated_model, client_models)\n",
    "\n",
    "            if test_after_each_epoch:\n",
    "                federated_model.to(device)\n",
    "                federated_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    print(f\"Test results for epoch {idx+1}:\")\n",
    "                    federated_test_loader_device = self.__wrap_data_loader(federated_test_loader, device)\n",
    "                    print(self.__test_fn(federated_model, federated_test_loader_device, federated_loss))\n",
    "                    federated_test_loader_device = None\n",
    "                    \n",
    "                federated_model.to(\"cpu\")\n",
    "        \n",
    "        \n",
    "        print(f\"Federated training complete after {time.time() - federated_training_start:.2f} seconds, testing ...\")\n",
    "        \n",
    "        federated_test_start = time.time()\n",
    "        federated_model.to(device)\n",
    "        federated_model.eval()\n",
    "        with torch.no_grad():\n",
    "            federated_test_loader = self.__wrap_data_loader(federated_test_loader, device)\n",
    "            federated_test_results = self.__test_fn(federated_model, federated_test_loader, federated_loss)\n",
    "        \n",
    "        federated_model.to(\"cpu\")\n",
    "        federated_training_dataloaders = None\n",
    "        federated_test_loader = None\n",
    "        federated_loss = None\n",
    "        client_models = None\n",
    "        federated_model = None\n",
    "        \n",
    "        print(f\"Federated testing complete after {time.time() - federated_test_start:.2f} seconds, results:\")\n",
    "        print(federated_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Training sequential model\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Sequential training complete after 713.35 seconds, testing ...\n",
      "Sequential testing complete after 34.34 seconds, results:\n",
      "Test accuracy is 88.491%\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Training federated model(s)\n",
      "Epoch: 1, client 1\n",
      "Epoch: 1, client 2\n",
      "Epoch: 1, client 3\n",
      "Epoch: 1, client 4\n",
      "Epoch: 2, client 1\n",
      "Epoch: 2, client 2\n",
      "Epoch: 2, client 3\n",
      "Epoch: 2, client 4\n",
      "Epoch: 3, client 1\n",
      "Epoch: 3, client 2\n",
      "Epoch: 3, client 3\n",
      "Epoch: 3, client 4\n",
      "Epoch: 4, client 1\n",
      "Epoch: 4, client 2\n",
      "Epoch: 4, client 3\n",
      "Epoch: 4, client 4\n",
      "Epoch: 5, client 1\n",
      "Epoch: 5, client 2\n",
      "Epoch: 5, client 3\n",
      "Epoch: 5, client 4\n",
      "Federated training complete after 756.69 seconds, testing ...\n",
      "Federated testing complete after 36.42 seconds, results:\n",
      "Test accuracy is 96.265%\n"
     ]
    }
   ],
   "source": [
    "number_of_clients = 4\n",
    "\n",
    "def construct_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters())\n",
    "\n",
    "def construct_loss():\n",
    "    # use pos weights because of unbalanced data set\n",
    "    return torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])) # binary crossentropy\n",
    "\n",
    "\n",
    "model_to_test = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "\n",
    "# local_epochs_to_train=1 means using FedSGD\n",
    "federated_test = FederatedLearningTest(\n",
    "    model_to_test, train_set, test_set,\n",
    "    train_epoch_fn=train_fn, \n",
    "    test_fn=test_fn,\n",
    "    use_gpu=True, epochs_to_train=5, local_epochs_to_train=1\n",
    ").set_batch_size(32).set_shuffle_train_data(True).set_dataloader_workers(0).set_pin_training_memory(True)\n",
    "\n",
    "federated_test.compare(\n",
    "    augment_data=False,\n",
    "    full_data_on_each_client=False,\n",
    "    no_of_clients=number_of_clients,\n",
    "    construct_optimizer_fn = construct_optimizer,\n",
    "    construct_loss_fn = construct_loss,\n",
    "    test_after_each_epoch = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Training sequential model\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Sequential training complete after 642.70 seconds, testing ...\n",
      "Sequential testing complete after 31.87 seconds, results:\n",
      "Test accuracy is 96.494%\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Training federated model(s)\n",
      "Epoch: 1, client 1\n",
      "Epoch: 1, client 2\n",
      "Epoch: 1, client 3\n",
      "Epoch: 1, client 4\n",
      "Epoch: 2, client 1\n",
      "Epoch: 2, client 2\n",
      "Epoch: 2, client 3\n",
      "Epoch: 2, client 4\n",
      "Epoch: 3, client 1\n",
      "Epoch: 3, client 2\n",
      "Epoch: 3, client 3\n",
      "Epoch: 3, client 4\n",
      "Epoch: 4, client 1\n",
      "Epoch: 4, client 2\n",
      "Epoch: 4, client 3\n",
      "Epoch: 4, client 4\n",
      "Epoch: 5, client 1\n",
      "Epoch: 5, client 2\n",
      "Epoch: 5, client 3\n",
      "Epoch: 5, client 4\n",
      "Federated training complete after 3510.09 seconds, testing ...\n",
      "Federated testing complete after 31.76 seconds, results:\n",
      "Test accuracy is 92.226%\n"
     ]
    }
   ],
   "source": [
    "number_of_clients = 4\n",
    "\n",
    "def construct_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters())\n",
    "\n",
    "def construct_loss():\n",
    "    # use pos weights because of unbalanced data set\n",
    "    return torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])) # binary crossentropy\n",
    "\n",
    "\n",
    "model_to_test = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "\n",
    "# local_epochs_to_train > 1 means using FedAvg, should perform better than FedSGD\n",
    "federated_test = FederatedLearningTest(\n",
    "    model_to_test, train_set, test_set,\n",
    "    train_epoch_fn=train_fn, \n",
    "    test_fn=test_fn,\n",
    "    use_gpu=True, epochs_to_train=5, local_epochs_to_train=5\n",
    ").set_batch_size(32).set_shuffle_train_data(True).set_dataloader_workers(0).set_pin_training_memory(True)\n",
    "\n",
    "federated_test.compare(\n",
    "    augment_data=False,\n",
    "    full_data_on_each_client=False,\n",
    "    no_of_clients=number_of_clients,\n",
    "    construct_optimizer_fn = construct_optimizer,\n",
    "    construct_loss_fn = construct_loss,\n",
    "    test_after_each_epoch = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "federated_learning_with_save.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
