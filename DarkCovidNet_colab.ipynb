{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DarkCovidNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ya5ZEuZpSnE","executionInfo":{"status":"ok","timestamp":1633256110683,"user_tz":-120,"elapsed":18239,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"43241e6f-65ef-40b9-e54a-d3db8b7a4665"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"JPFiGGjbhta_","executionInfo":{"status":"ok","timestamp":1633258654198,"user_tz":-120,"elapsed":220,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["LEARNING_RATE = 3e-3 #0.14\n","EPOCHS = 25\n","TARGET_FOLDER = \"weights\"\n","K_FOLDS = 3"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvAJ8eTnTRkP","executionInfo":{"status":"ok","timestamp":1633258655105,"user_tz":-120,"elapsed":577,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["import torch.nn as nn\n","import torch\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import KFold"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Z_GY07pSzQT","executionInfo":{"status":"ok","timestamp":1633258679122,"user_tz":-120,"elapsed":225,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["transform = transforms.Compose([transforms.Resize((256, 256)),\n","                                 transforms.ToTensor()])\n","\n","data_set = datasets.ImageFolder('/content/drive/MyDrive/data_aml/X-Ray Image DataSet', transform=transform)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAi62OkRTZfV","executionInfo":{"status":"ok","timestamp":1633256116369,"user_tz":-120,"elapsed":32,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["data_loader = torch.utils.data.DataLoader(data_set, batch_size=32, shuffle=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWhXD92ZRzNP","executionInfo":{"status":"ok","timestamp":1633258657442,"user_tz":-120,"elapsed":236,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"ce6d8dff-21ad-439a-baf8-4c42354adb24"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(f\"Current device: {device}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Current device: cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"2Qihhc_PTgVr","executionInfo":{"status":"ok","timestamp":1633258788848,"user_tz":-120,"elapsed":225,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n","        super(ConvBlock, self).__init__()\n","        for_pad = lambda s: s if s > 2 else 3\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=(for_pad(kernel_size) - 1)//2, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True) \n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return out\n","\n","class TripleConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(TripleConvBlock, self).__init__()\n","        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n","        self.conv_block_2 = ConvBlock(out_channels, in_channels, kernel_size=1)  \n","        self.conv_block_3 = ConvBlock(in_channels, out_channels)\n","\n","    def forward(self, x):\n","        out = self.conv_block_1(x)\n","        out = self.conv_block_2(out)\n","        out = self.conv_block_3(out)\n","        return out\n","\n","class Model2(nn.Module):\n","    def __init__(self):\n","        super(Model2, self).__init__()\n","        self.seq = nn.Sequential(\n","        ConvBlock(3, 8),\n","        nn.MaxPool2d(2, stride=2),\n","        ConvBlock(8, 16),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(16, 32),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(32,64),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(64,128),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(128,256),\n","        ConvBlock(256, 128, kernel_size=1),\n","        ConvBlock(128, 256),\n","        nn.Conv2d(256, 2, 3, padding=(3-1)//2, stride=1), # Ich bekomme es nicht hin, dass das identisch ist \n","        nn.ReLU(),\n","        nn.BatchNorm2d(2),\n","        nn.Flatten(),\n","        nn.Linear(338,2)\n","        )\n","\n","    def forward(self, x):\n","        return self.seq(x)\n","\n","class Model3(nn.Module):\n","    def __init__(self):\n","        super(Model3, self).__init__()\n","        self.seq = nn.Sequential(\n","        ConvBlock(3, 8),\n","        nn.MaxPool2d(2, stride=2),\n","        ConvBlock(8, 16),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(16, 32),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(32,64),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(64,128),\n","        nn.MaxPool2d(2, stride=2),\n","        TripleConvBlock(128,256),\n","        ConvBlock(256, 128, kernel_size=1),\n","        ConvBlock(128, 256),\n","        nn.Conv2d(256, 3, 3, padding=(3-1)//2, stride=1), # Ich bekomme es nicht hin, dass das identisch ist \n","        nn.ReLU(),\n","        nn.BatchNorm2d(3),\n","        nn.Flatten(),\n","        nn.Linear(507,3)\n","        )\n","\n","    def forward(self, x):\n","        return self.seq(x)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAItZORR0N0H","executionInfo":{"status":"ok","timestamp":1633256124423,"user_tz":-120,"elapsed":7747,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"96dbb277-2c52-41db-d73e-9ab46eb5b5fa"},"source":["from torchsummary import summary\n","model = Model3().to(device)\n","summary(model, (3, 256, 256))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 8, 256, 256]             216\n","       BatchNorm2d-2          [-1, 8, 256, 256]              16\n","         LeakyReLU-3          [-1, 8, 256, 256]               0\n","         ConvBlock-4          [-1, 8, 256, 256]               0\n","         MaxPool2d-5          [-1, 8, 128, 128]               0\n","            Conv2d-6         [-1, 16, 128, 128]           1,152\n","       BatchNorm2d-7         [-1, 16, 128, 128]              32\n","         LeakyReLU-8         [-1, 16, 128, 128]               0\n","         ConvBlock-9         [-1, 16, 128, 128]               0\n","        MaxPool2d-10           [-1, 16, 64, 64]               0\n","           Conv2d-11           [-1, 32, 64, 64]           4,608\n","      BatchNorm2d-12           [-1, 32, 64, 64]              64\n","        LeakyReLU-13           [-1, 32, 64, 64]               0\n","        ConvBlock-14           [-1, 32, 64, 64]               0\n","           Conv2d-15           [-1, 16, 66, 66]             512\n","      BatchNorm2d-16           [-1, 16, 66, 66]              32\n","        LeakyReLU-17           [-1, 16, 66, 66]               0\n","        ConvBlock-18           [-1, 16, 66, 66]               0\n","           Conv2d-19           [-1, 32, 66, 66]           4,608\n","      BatchNorm2d-20           [-1, 32, 66, 66]              64\n","        LeakyReLU-21           [-1, 32, 66, 66]               0\n","        ConvBlock-22           [-1, 32, 66, 66]               0\n","  TripleConvBlock-23           [-1, 32, 66, 66]               0\n","        MaxPool2d-24           [-1, 32, 33, 33]               0\n","           Conv2d-25           [-1, 64, 33, 33]          18,432\n","      BatchNorm2d-26           [-1, 64, 33, 33]             128\n","        LeakyReLU-27           [-1, 64, 33, 33]               0\n","        ConvBlock-28           [-1, 64, 33, 33]               0\n","           Conv2d-29           [-1, 32, 35, 35]           2,048\n","      BatchNorm2d-30           [-1, 32, 35, 35]              64\n","        LeakyReLU-31           [-1, 32, 35, 35]               0\n","        ConvBlock-32           [-1, 32, 35, 35]               0\n","           Conv2d-33           [-1, 64, 35, 35]          18,432\n","      BatchNorm2d-34           [-1, 64, 35, 35]             128\n","        LeakyReLU-35           [-1, 64, 35, 35]               0\n","        ConvBlock-36           [-1, 64, 35, 35]               0\n","  TripleConvBlock-37           [-1, 64, 35, 35]               0\n","        MaxPool2d-38           [-1, 64, 17, 17]               0\n","           Conv2d-39          [-1, 128, 17, 17]          73,728\n","      BatchNorm2d-40          [-1, 128, 17, 17]             256\n","        LeakyReLU-41          [-1, 128, 17, 17]               0\n","        ConvBlock-42          [-1, 128, 17, 17]               0\n","           Conv2d-43           [-1, 64, 19, 19]           8,192\n","      BatchNorm2d-44           [-1, 64, 19, 19]             128\n","        LeakyReLU-45           [-1, 64, 19, 19]               0\n","        ConvBlock-46           [-1, 64, 19, 19]               0\n","           Conv2d-47          [-1, 128, 19, 19]          73,728\n","      BatchNorm2d-48          [-1, 128, 19, 19]             256\n","        LeakyReLU-49          [-1, 128, 19, 19]               0\n","        ConvBlock-50          [-1, 128, 19, 19]               0\n","  TripleConvBlock-51          [-1, 128, 19, 19]               0\n","        MaxPool2d-52            [-1, 128, 9, 9]               0\n","           Conv2d-53            [-1, 256, 9, 9]         294,912\n","      BatchNorm2d-54            [-1, 256, 9, 9]             512\n","        LeakyReLU-55            [-1, 256, 9, 9]               0\n","        ConvBlock-56            [-1, 256, 9, 9]               0\n","           Conv2d-57          [-1, 128, 11, 11]          32,768\n","      BatchNorm2d-58          [-1, 128, 11, 11]             256\n","        LeakyReLU-59          [-1, 128, 11, 11]               0\n","        ConvBlock-60          [-1, 128, 11, 11]               0\n","           Conv2d-61          [-1, 256, 11, 11]         294,912\n","      BatchNorm2d-62          [-1, 256, 11, 11]             512\n","        LeakyReLU-63          [-1, 256, 11, 11]               0\n","        ConvBlock-64          [-1, 256, 11, 11]               0\n","  TripleConvBlock-65          [-1, 256, 11, 11]               0\n","           Conv2d-66          [-1, 128, 13, 13]          32,768\n","      BatchNorm2d-67          [-1, 128, 13, 13]             256\n","        LeakyReLU-68          [-1, 128, 13, 13]               0\n","        ConvBlock-69          [-1, 128, 13, 13]               0\n","           Conv2d-70          [-1, 256, 13, 13]         294,912\n","      BatchNorm2d-71          [-1, 256, 13, 13]             512\n","        LeakyReLU-72          [-1, 256, 13, 13]               0\n","        ConvBlock-73          [-1, 256, 13, 13]               0\n","           Conv2d-74            [-1, 3, 13, 13]           6,915\n","             ReLU-75            [-1, 3, 13, 13]               0\n","      BatchNorm2d-76            [-1, 3, 13, 13]               6\n","          Flatten-77                  [-1, 507]               0\n","           Linear-78                    [-1, 3]           1,524\n","================================================================\n","Total params: 1,167,589\n","Trainable params: 1,167,589\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 51.62\n","Params size (MB): 4.45\n","Estimated Total Size (MB): 56.83\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"id":"A5Me-0EHR_md","executionInfo":{"status":"ok","timestamp":1633256124424,"user_tz":-120,"elapsed":19,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["def train(model, data_loader, optimizer):\n","    \"\"\"\n","    model -- neural net\n","    data_loader -- dataloader for train images\n","    optimizer -- optimizer\n","    \"\"\"\n","    model.train()\n","    \n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","    \n","    for step, [images, labels] in enumerate(data_loader,0):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","                \n","        optimizer.zero_grad()\n","        result = model(images)\n","        loss = criterion(result, labels)\n","                \n","        # backpropagation\n","        loss.backward()\n","        optimizer.step()\n","                                    \n","        if step%10 == 0:\n","            print(f\"Step: {step}, loss: {loss}\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"icqqTTRN-ht6","executionInfo":{"status":"ok","timestamp":1633258836144,"user_tz":-120,"elapsed":257,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["def calc_accuracy(result, labels):\n","    result = torch.round(result)\n","    probs = torch.softmax(result, dim=1)\n","\n","    correct_results_sum = (probs.argmax(dim=1) == labels).sum().float()\n","    acc = correct_results_sum/labels.shape[0]\n","    acc = torch.round(acc * 100)\n","    \n","    return acc\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xk2CR_whWTxw","executionInfo":{"status":"ok","timestamp":1633258834510,"user_tz":-120,"elapsed":229,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["def test(model, test_loader, epoch):\n","    \"\"\"    \n","    model -- neural net \n","    test_loader -- dataloader of test images\n","    epoch -- current epoch\n","    \"\"\"\n","    model.eval()\n","    loss = 0\n","    accuracy = 0\n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","    for step, [images, labels] in enumerate(test_loader,0):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        result = model(images)\n","        loss += criterion(result.detach(), labels.detach())\n","        accuracy += calc_accuracy(result.detach(), labels.detach())\n","    loss /= step\n","    accuracy /=  step\n","  \n","    print(f\"Loss: {loss}, Accuracy: {accuracy}\")"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"kls3WJQpRmwf","executionInfo":{"status":"ok","timestamp":1633258775369,"user_tz":-120,"elapsed":236,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["kfold = KFold(n_splits=K_FOLDS, shuffle=True)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzfyMxdBVoB1","executionInfo":{"status":"ok","timestamp":1633258454791,"user_tz":-120,"elapsed":2094794,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"86886dbe-92a0-44e3-826c-7b0fe0ccfaeb"},"source":["for fold, (train_ids, test_ids) in enumerate(kfold.split(data_set)):\n","    \n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n","    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","    \n","    train_loader = torch.utils.data.DataLoader(\n","                      data_set, \n","                      batch_size=32, sampler=train_subsampler)\n","    test_loader = torch.utils.data.DataLoader(\n","                      data_set,\n","                      batch_size=32, sampler=test_subsampler)\n","    \n","    model = Model3().to(device)\n","    \n","    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n","    \n","    for epoch in range(0, EPOCHS):\n","\n","      train(model, train_loader, optimizer)\n","      \n","      #torch.save(model.state_dict(), f'{TARGET_FOLDER}/fold_{fold}_epoch_{epoch}.ckpt')\n","\n","      test(model, test_loader, epoch)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["FOLD 0\n","--------------------------------\n","Step: 0, loss: 1.1709269285202026\n","Step: 10, loss: 0.85526043176651\n","Step: 20, loss: 0.8224045634269714\n","Loss: 1.5488566160202026, Accuracy: 8.545454978942871\n","Step: 0, loss: 0.7802308201789856\n","Step: 10, loss: 0.6580170392990112\n","Step: 20, loss: 0.5885074734687805\n","Loss: 0.8503888249397278, Accuracy: 72.36363983154297\n","Step: 0, loss: 0.5039673447608948\n","Step: 10, loss: 0.4858916401863098\n","Step: 20, loss: 0.6375385522842407\n","Loss: 0.6914443969726562, Accuracy: 77.81818389892578\n","Step: 0, loss: 0.5469644069671631\n","Step: 10, loss: 0.7296732664108276\n","Step: 20, loss: 0.5378980040550232\n","Loss: 0.7396879196166992, Accuracy: 63.54545593261719\n","Step: 0, loss: 0.5758719444274902\n","Step: 10, loss: 0.5874923467636108\n","Step: 20, loss: 0.7288925647735596\n","Loss: 0.8652558922767639, Accuracy: 75.18182373046875\n","Step: 0, loss: 0.6100776195526123\n","Step: 10, loss: 0.547454833984375\n","Step: 20, loss: 0.38228917121887207\n","Loss: 0.8177903294563293, Accuracy: 77.54545593261719\n","Step: 0, loss: 0.39376023411750793\n","Step: 10, loss: 0.37536343932151794\n","Step: 20, loss: 0.32528963685035706\n","Loss: 0.8975638151168823, Accuracy: 59.0\n","Step: 0, loss: 0.4623853862285614\n","Step: 10, loss: 0.6352342367172241\n","Step: 20, loss: 0.5341445803642273\n","Loss: 0.5706127882003784, Accuracy: 83.0\n","Step: 0, loss: 0.25128254294395447\n","Step: 10, loss: 0.562084436416626\n","Step: 20, loss: 0.36018264293670654\n","Loss: 1.1199932098388672, Accuracy: 69.81818389892578\n","Step: 0, loss: 0.33519265055656433\n","Step: 10, loss: 0.5224392414093018\n","Step: 20, loss: 0.4429354965686798\n","Loss: 3.195629596710205, Accuracy: 54.45454788208008\n","Step: 0, loss: 0.22703023254871368\n","Step: 10, loss: 0.3719431459903717\n","Step: 20, loss: 0.38564419746398926\n","Loss: 0.6270687580108643, Accuracy: 85.0\n","Step: 0, loss: 0.21842582523822784\n","Step: 10, loss: 0.29180797934532166\n","Step: 20, loss: 0.4699576497077942\n","Loss: 0.7143543362617493, Accuracy: 73.90909576416016\n","Step: 0, loss: 0.34177860617637634\n","Step: 10, loss: 0.5094517469406128\n","Step: 20, loss: 0.3652941584587097\n","Loss: 0.9008516073226929, Accuracy: 68.18182373046875\n","Step: 0, loss: 0.7852322459220886\n","Step: 10, loss: 0.40383052825927734\n","Step: 20, loss: 0.18452544510364532\n","Loss: 0.6346303820610046, Accuracy: 84.18182373046875\n","Step: 0, loss: 0.5370601415634155\n","Step: 10, loss: 0.15683993697166443\n","Step: 20, loss: 0.2603425979614258\n","Loss: 0.5105112791061401, Accuracy: 82.09091186523438\n","Step: 0, loss: 0.34937232732772827\n","Step: 10, loss: 0.22372789680957794\n","Step: 20, loss: 0.29205432534217834\n","Loss: 1.2248607873916626, Accuracy: 71.09091186523438\n","Step: 0, loss: 0.5011192560195923\n","Step: 10, loss: 0.2545168995857239\n","Step: 20, loss: 0.18817773461341858\n","Loss: 0.5679759979248047, Accuracy: 84.0\n","Step: 0, loss: 0.11882267892360687\n","Step: 10, loss: 0.38097336888313293\n","Step: 20, loss: 0.3744955360889435\n","Loss: 1.1786367893218994, Accuracy: 70.36363983154297\n","Step: 0, loss: 0.41623300313949585\n","Step: 10, loss: 0.21382993459701538\n","Step: 20, loss: 0.1745569407939911\n","Loss: 0.9173257350921631, Accuracy: 80.81818389892578\n","Step: 0, loss: 0.15292629599571228\n","Step: 10, loss: 0.17534507811069489\n","Step: 20, loss: 0.30082181096076965\n","Loss: 2.6936933994293213, Accuracy: 62.090911865234375\n","Step: 0, loss: 0.19836533069610596\n","Step: 10, loss: 0.25255411863327026\n","Step: 20, loss: 0.3338371217250824\n","Loss: 1.0565063953399658, Accuracy: 83.2727279663086\n","Step: 0, loss: 0.22481878101825714\n","Step: 10, loss: 0.32152530550956726\n","Step: 20, loss: 0.20020098984241486\n","Loss: 0.6353257894515991, Accuracy: 87.63636779785156\n","Step: 0, loss: 0.5539432764053345\n","Step: 10, loss: 0.25017249584198\n","Step: 20, loss: 0.1851414442062378\n","Loss: 0.584222674369812, Accuracy: 85.81818389892578\n","Step: 0, loss: 0.1572551280260086\n","Step: 10, loss: 0.18393543362617493\n","Step: 20, loss: 0.2589793801307678\n","Loss: 0.7364913821220398, Accuracy: 81.54545593261719\n","Step: 0, loss: 0.2549189627170563\n","Step: 10, loss: 0.19481955468654633\n","Step: 20, loss: 0.12773080170154572\n","Loss: 0.8121305108070374, Accuracy: 85.54545593261719\n","FOLD 1\n","--------------------------------\n","Step: 0, loss: 1.0781917572021484\n","Step: 10, loss: 1.0956039428710938\n","Step: 20, loss: 1.0518971681594849\n","Loss: 1.2301857471466064, Accuracy: 46.81818389892578\n","Step: 0, loss: 0.8033306002616882\n","Step: 10, loss: 0.8221958875656128\n","Step: 20, loss: 0.58323073387146\n","Loss: 0.8883129954338074, Accuracy: 68.09091186523438\n","Step: 0, loss: 0.5127525329589844\n","Step: 10, loss: 0.5524095892906189\n","Step: 20, loss: 0.8495143055915833\n","Loss: 1.0942295789718628, Accuracy: 55.90909194946289\n","Step: 0, loss: 0.6000530123710632\n","Step: 10, loss: 0.7630861401557922\n","Step: 20, loss: 0.8208389282226562\n","Loss: 1.2307764291763306, Accuracy: 63.272727966308594\n","Step: 0, loss: 0.6969485282897949\n","Step: 10, loss: 0.39069852232933044\n","Step: 20, loss: 0.5870816111564636\n","Loss: 1.341532588005066, Accuracy: 51.272727966308594\n","Step: 0, loss: 0.4183558225631714\n","Step: 10, loss: 0.9811162948608398\n","Step: 20, loss: 0.5702478289604187\n","Loss: 0.9280819892883301, Accuracy: 70.7272720336914\n","Step: 0, loss: 0.4733637571334839\n","Step: 10, loss: 0.3759652376174927\n","Step: 20, loss: 0.3318074643611908\n","Loss: 1.289414882659912, Accuracy: 59.90909194946289\n","Step: 0, loss: 0.5327259302139282\n","Step: 10, loss: 0.4170083701610565\n","Step: 20, loss: 0.38482192158699036\n","Loss: 0.6514098048210144, Accuracy: 77.90909576416016\n","Step: 0, loss: 0.33797210454940796\n","Step: 10, loss: 0.3425529897212982\n","Step: 20, loss: 0.21763291954994202\n","Loss: 0.7249296307563782, Accuracy: 77.45454406738281\n","Step: 0, loss: 0.3638511300086975\n","Step: 10, loss: 0.30619367957115173\n","Step: 20, loss: 0.42087996006011963\n","Loss: 4.255042552947998, Accuracy: 54.36363983154297\n","Step: 0, loss: 0.4838278293609619\n","Step: 10, loss: 0.37467291951179504\n","Step: 20, loss: 0.3189323842525482\n","Loss: 0.7299272418022156, Accuracy: 83.54545593261719\n","Step: 0, loss: 0.5061096549034119\n","Step: 10, loss: 0.36913833022117615\n","Step: 20, loss: 0.6418669819831848\n","Loss: 0.6024808883666992, Accuracy: 82.81818389892578\n","Step: 0, loss: 0.49017974734306335\n","Step: 10, loss: 0.2762676477432251\n","Step: 20, loss: 0.4085911512374878\n","Loss: 0.9463367462158203, Accuracy: 73.81818389892578\n","Step: 0, loss: 0.23368163406848907\n","Step: 10, loss: 0.3028412163257599\n","Step: 20, loss: 0.41603273153305054\n","Loss: 2.3633480072021484, Accuracy: 48.272727966308594\n","Step: 0, loss: 0.40952181816101074\n","Step: 10, loss: 0.3108088970184326\n","Step: 20, loss: 0.14789126813411713\n","Loss: 0.6843296885490417, Accuracy: 79.0\n","Step: 0, loss: 0.29472166299819946\n","Step: 10, loss: 0.3075195252895355\n","Step: 20, loss: 0.18464338779449463\n","Loss: 0.7298488616943359, Accuracy: 80.7272720336914\n","Step: 0, loss: 0.22352349758148193\n","Step: 10, loss: 0.10497928410768509\n","Step: 20, loss: 0.5101297497749329\n","Loss: 1.2842068672180176, Accuracy: 70.45454406738281\n","Step: 0, loss: 0.3207443654537201\n","Step: 10, loss: 0.21536527574062347\n","Step: 20, loss: 0.5907972455024719\n","Loss: 2.2377877235412598, Accuracy: 44.81818389892578\n","Step: 0, loss: 0.397691935300827\n","Step: 10, loss: 0.4282936155796051\n","Step: 20, loss: 0.2824365198612213\n","Loss: 0.7282537221908569, Accuracy: 79.54545593261719\n","Step: 0, loss: 0.3031534254550934\n","Step: 10, loss: 0.22277985513210297\n","Step: 20, loss: 0.24889856576919556\n","Loss: 1.449568748474121, Accuracy: 63.181819915771484\n","Step: 0, loss: 0.3151303231716156\n","Step: 10, loss: 0.4207344651222229\n","Step: 20, loss: 0.32191604375839233\n","Loss: 0.6087986826896667, Accuracy: 81.45454406738281\n","Step: 0, loss: 0.27994468808174133\n","Step: 10, loss: 0.20009955763816833\n","Step: 20, loss: 0.1965644210577011\n","Loss: 0.7883355021476746, Accuracy: 82.09091186523438\n","Step: 0, loss: 0.14785420894622803\n","Step: 10, loss: 0.19466175138950348\n","Step: 20, loss: 0.5519837737083435\n","Loss: 0.6852065324783325, Accuracy: 83.2727279663086\n","Step: 0, loss: 0.26315072178840637\n","Step: 10, loss: 0.31329867243766785\n","Step: 20, loss: 0.26714959740638733\n","Loss: 1.2115590572357178, Accuracy: 80.7272720336914\n","Step: 0, loss: 0.26331764459609985\n","Step: 10, loss: 0.3459940552711487\n","Step: 20, loss: 0.16496635973453522\n","Loss: 3.6811347007751465, Accuracy: 49.81818389892578\n","FOLD 2\n","--------------------------------\n","Step: 0, loss: 1.3135020732879639\n","Step: 10, loss: 0.7530450224876404\n","Step: 20, loss: 0.87391197681427\n","Loss: 1.4720302820205688, Accuracy: 46.090911865234375\n","Step: 0, loss: 0.8230815529823303\n","Step: 10, loss: 0.7777490615844727\n","Step: 20, loss: 0.7030062675476074\n","Loss: 1.0204960107803345, Accuracy: 54.90909194946289\n","Step: 0, loss: 0.4735875427722931\n","Step: 10, loss: 0.5996823906898499\n","Step: 20, loss: 0.6100970506668091\n","Loss: 0.8951057195663452, Accuracy: 75.63636779785156\n","Step: 0, loss: 0.49056267738342285\n","Step: 10, loss: 0.7175407409667969\n","Step: 20, loss: 0.3412003517150879\n","Loss: 0.7683078050613403, Accuracy: 74.90909576416016\n","Step: 0, loss: 0.48904699087142944\n","Step: 10, loss: 0.5170467495918274\n","Step: 20, loss: 0.8085933923721313\n","Loss: 0.7277571558952332, Accuracy: 73.36363983154297\n","Step: 0, loss: 0.6998604536056519\n","Step: 10, loss: 0.6133139729499817\n","Step: 20, loss: 0.4426056742668152\n","Loss: 0.7274118065834045, Accuracy: 73.7272720336914\n","Step: 0, loss: 0.4731135368347168\n","Step: 10, loss: 0.4562941789627075\n","Step: 20, loss: 0.5164034366607666\n","Loss: 0.70660400390625, Accuracy: 78.54545593261719\n","Step: 0, loss: 0.30325236916542053\n","Step: 10, loss: 0.5120012760162354\n","Step: 20, loss: 0.44094428420066833\n","Loss: 0.7672902941703796, Accuracy: 76.54545593261719\n","Step: 0, loss: 0.37990838289260864\n","Step: 10, loss: 0.5184220671653748\n","Step: 20, loss: 0.3383992314338684\n","Loss: 0.6428643465042114, Accuracy: 80.81818389892578\n","Step: 0, loss: 0.2603665888309479\n","Step: 10, loss: 0.33210915327072144\n","Step: 20, loss: 0.45649176836013794\n","Loss: 0.6103076934814453, Accuracy: 84.09091186523438\n","Step: 0, loss: 0.2605611979961395\n","Step: 10, loss: 0.4533486068248749\n","Step: 20, loss: 0.44788405299186707\n","Loss: 0.6028648614883423, Accuracy: 81.45454406738281\n","Step: 0, loss: 0.3637219965457916\n","Step: 10, loss: 0.31828317046165466\n","Step: 20, loss: 0.30175483226776123\n","Loss: 0.5759099721908569, Accuracy: 83.36363983154297\n","Step: 0, loss: 0.4819181561470032\n","Step: 10, loss: 0.20113718509674072\n","Step: 20, loss: 0.3965005874633789\n","Loss: 0.6257333159446716, Accuracy: 86.90909576416016\n","Step: 0, loss: 0.3432908356189728\n","Step: 10, loss: 0.2477869689464569\n","Step: 20, loss: 0.3161974251270294\n","Loss: 0.6073582172393799, Accuracy: 84.90909576416016\n","Step: 0, loss: 0.28035107254981995\n","Step: 10, loss: 0.395428329706192\n","Step: 20, loss: 0.154036745429039\n","Loss: 0.7167561650276184, Accuracy: 77.18182373046875\n","Step: 0, loss: 0.279888778924942\n","Step: 10, loss: 0.23568592965602875\n","Step: 20, loss: 0.365800678730011\n","Loss: 0.6362495422363281, Accuracy: 81.45454406738281\n","Step: 0, loss: 0.19448377192020416\n","Step: 10, loss: 0.28431153297424316\n","Step: 20, loss: 0.28050270676612854\n","Loss: 0.6782035827636719, Accuracy: 80.45454406738281\n","Step: 0, loss: 0.26315006613731384\n","Step: 10, loss: 0.16526801884174347\n","Step: 20, loss: 0.5240532755851746\n","Loss: 1.0377836227416992, Accuracy: 76.7272720336914\n","Step: 0, loss: 0.1465490609407425\n","Step: 10, loss: 0.3305153548717499\n","Step: 20, loss: 0.2530875504016876\n","Loss: 0.6880419254302979, Accuracy: 84.36363983154297\n","Step: 0, loss: 0.22233614325523376\n","Step: 10, loss: 0.5152626037597656\n","Step: 20, loss: 0.29268524050712585\n","Loss: 0.5895096063613892, Accuracy: 86.09091186523438\n","Step: 0, loss: 0.4317277669906616\n","Step: 10, loss: 0.2394343912601471\n","Step: 20, loss: 0.11103823035955429\n","Loss: 0.8083940148353577, Accuracy: 82.54545593261719\n","Step: 0, loss: 0.2307477593421936\n","Step: 10, loss: 0.5046072006225586\n","Step: 20, loss: 0.20373670756816864\n","Loss: 0.513279914855957, Accuracy: 87.18182373046875\n","Step: 0, loss: 0.3156496286392212\n","Step: 10, loss: 0.1413438618183136\n","Step: 20, loss: 0.40716132521629333\n","Loss: 0.5820958614349365, Accuracy: 83.7272720336914\n","Step: 0, loss: 0.19988799095153809\n","Step: 10, loss: 0.21156498789787292\n","Step: 20, loss: 0.1647930145263672\n","Loss: 0.6161140203475952, Accuracy: 84.63636779785156\n","Step: 0, loss: 0.0850197970867157\n","Step: 10, loss: 0.18943510949611664\n","Step: 20, loss: 0.2654329240322113\n","Loss: 0.7148110270500183, Accuracy: 84.45455169677734\n"]}]},{"cell_type":"markdown","metadata":{"id":"Uc9Z1kAKniiG"},"source":["# Federated\n","https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n","\n","Neuste Version (0.3.x) hat kein TorchHook -> Lösung finden?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NUJv9PNBuANj","executionInfo":{"status":"ok","timestamp":1633258584162,"user_tz":-120,"elapsed":111145,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"49c0d72a-7c2d-4d20-a38c-d5a750eba4ca"},"source":["!pip install syft==0.2.9 "],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting syft==0.2.9\n","  Downloading syft-0.2.9-py3-none-any.whl (433 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 235 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 245 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 286 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 327 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 348 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 389 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 399 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 430 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 433 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.1)\n","Collecting phe~=1.4.0\n","  Downloading phe-1.4.0.tar.gz (35 kB)\n","Collecting numpy~=1.18.1\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 8.2 MB/s \n","\u001b[?25hCollecting syft-proto~=0.5.2\n","  Downloading syft_proto-0.5.3-py3-none-any.whl (66 kB)\n","\u001b[K     |████████████████████████████████| 66 kB 5.9 MB/s \n","\u001b[?25hCollecting flask-socketio~=4.2.1\n","  Downloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\n","Collecting websocket-client~=0.57.0\n","  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 78.2 MB/s \n","\u001b[?25hRequirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.4)\n","Collecting aiortc==0.9.28\n","  Downloading aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 45.4 MB/s \n","\u001b[?25hCollecting tornado==4.5.3\n","  Downloading tornado-4.5.3.tar.gz (484 kB)\n","\u001b[K     |████████████████████████████████| 484 kB 73.0 MB/s \n","\u001b[?25hCollecting torchvision~=0.5.0\n","  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 30.4 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.2)\n","Collecting requests~=2.22.0\n","  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 6.5 MB/s \n","\u001b[?25hCollecting requests-toolbelt==0.9.1\n","  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n","\u001b[?25hCollecting tblib~=1.6.0\n","  Downloading tblib-1.6.0-py2.py3-none-any.whl (12 kB)\n","Collecting lz4~=3.0.2\n","  Downloading lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 60.4 MB/s \n","\u001b[?25hCollecting importlib-resources~=1.5.0\n","  Downloading importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n","Requirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n","Collecting websockets~=8.1.0\n","  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n","\u001b[?25hCollecting notebook==5.7.8\n","  Downloading notebook-5.7.8-py2.py3-none-any.whl (9.0 MB)\n","\u001b[K     |████████████████████████████████| 9.0 MB 65.6 MB/s \n","\u001b[?25hCollecting openmined.threepio==0.2.0\n","  Downloading openmined.threepio-0.2.0.tar.gz (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 2.6 MB/s \n","\u001b[?25hCollecting shaloop==0.2.1-alpha.11\n","  Downloading shaloop-0.2.1_alpha.11-py3-none-manylinux1_x86_64.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 70.2 MB/s \n","\u001b[?25hCollecting RestrictedPython~=5.0\n","  Downloading RestrictedPython-5.1-py2.py3-none-any.whl (27 kB)\n","Collecting torch~=1.4.0\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[K     |████████████████████████████████| 753.4 MB 7.0 kB/s \n","\u001b[?25hRequirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.4)\n","Collecting psutil==5.7.0\n","  Downloading psutil-5.7.0.tar.gz (449 kB)\n","\u001b[K     |████████████████████████████████| 449 kB 63.8 MB/s \n","\u001b[?25hCollecting cryptography>=2.2\n","  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 26.0 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (1.14.6)\n","Collecting pyee>=6.0.0\n","  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n","Collecting av<9.0.0,>=8.0.0\n","  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n","\u001b[K     |████████████████████████████████| 37.2 MB 56 kB/s \n","\u001b[?25hCollecting pylibsrtp>=0.5.6\n","  Downloading pylibsrtp-0.6.8-cp37-cp37m-manylinux2010_x86_64.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 3.9 MB/s \n","\u001b[?25hCollecting crc32c\n","  Downloading crc32c-2.2.post0-cp37-cp37m-manylinux2010_x86_64.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.9 MB/s \n","\u001b[?25hCollecting aioice<0.7.0,>=0.6.17\n","  Downloading aioice-0.6.18-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (22.3.0)\n","Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.3.5)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.12.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (2.11.3)\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (1.8.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.11.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.2.0)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.8.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.10.1)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.3)\n","Requirement already satisfied: pycparser>=2 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (2.20)\n","Collecting netifaces\n","  Downloading netifaces-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (32 kB)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (7.1.2)\n","Collecting python-socketio>=4.3.0\n","  Downloading python_socketio-5.4.0-py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.5.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (4.8.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook==5.7.8->syft==0.2.9) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (1.15.0)\n","Collecting bidict>=0.21.0\n","  Downloading bidict-0.21.3-py3-none-any.whl (36 kB)\n","Collecting python-engineio>=4.1.0\n","  Downloading python_engineio-4.2.1-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 810 kB/s \n","\u001b[?25hCollecting idna<2.9,>=2.5\n","  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (3.0.4)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.5.2->syft==0.2.9) (3.17.3)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft==0.2.9) (0.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->importlib-resources~=1.5.0->syft==0.2.9) (3.7.4.3)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook==5.7.8->syft==0.2.9) (5.5.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.7.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (1.0.18)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.2.5)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (1.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (4.1.0)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook==5.7.8->syft==0.2.9) (2.6.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (21.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (2.4.7)\n","Building wheels for collected packages: openmined.threepio, psutil, tornado, phe\n","  Building wheel for openmined.threepio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openmined.threepio: filename=openmined.threepio-0.2.0-py3-none-any.whl size=80095 sha256=edd9c88d25e3aa525df55e0403840a3d038978f5563e50c0b1c8e68724f87d17\n","  Stored in directory: /root/.cache/pip/wheels/97/3d/ce/4ca4386006e622cb87d5116e5e65026ec021d3cf906a9b3d5d\n","  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=276495 sha256=13c5001fa1ee8b00e192fe098d00a476a448a6da05c05110262f84d27b5767b3\n","  Stored in directory: /root/.cache/pip/wheels/b6/e7/50/aee9cc966163d74430f13f208171dee22f11efa4a4a826661c\n","  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434046 sha256=4520d0d75752bdec9891c06d8167e33a32d75e6d81f24f6b96a37a9a48a52f64\n","  Stored in directory: /root/.cache/pip/wheels/a2/45/43/36ec7a893e16c1212a6b1505ded0a2d73cf8e863a0227c8e04\n","  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=c6c6f20304ae7393312c01b0bc293b6a1f575a960f43f3bf3c463b675ec9020c\n","  Stored in directory: /root/.cache/pip/wheels/bb/ac/9b/b07a04fe6bb1418ab4ee06d6652757aef848b80363c4dac507\n","Successfully built openmined.threepio psutil tornado phe\n","Installing collected packages: tornado, python-engineio, netifaces, idna, bidict, torch, requests, python-socketio, pylibsrtp, pyee, numpy, cryptography, crc32c, av, aioice, websockets, websocket-client, torchvision, tblib, syft-proto, shaloop, RestrictedPython, requests-toolbelt, psutil, phe, openmined.threepio, notebook, lz4, importlib-resources, flask-socketio, aiortc, syft\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: idna\n","    Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","  Attempting uninstall: tblib\n","    Found existing installation: tblib 1.7.0\n","    Uninstalling tblib-1.7.0:\n","      Successfully uninstalled tblib-1.7.0\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","  Attempting uninstall: notebook\n","    Found existing installation: notebook 5.3.1\n","    Uninstalling notebook-5.3.1:\n","      Successfully uninstalled notebook-5.3.1\n","  Attempting uninstall: importlib-resources\n","    Found existing installation: importlib-resources 5.2.2\n","    Uninstalling importlib-resources-5.2.2:\n","      Successfully uninstalled importlib-resources-5.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.4.0 which is incompatible.\n","tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.18.5 which is incompatible.\n","google-colab 1.0.0 requires notebook~=5.3.0; python_version >= \"3.0\", but you have notebook 5.7.8 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n","google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 4.5.3 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","bokeh 2.3.3 requires tornado>=5.1, but you have tornado 4.5.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed RestrictedPython-5.1 aioice-0.6.18 aiortc-0.9.28 av-8.0.3 bidict-0.21.3 crc32c-2.2.post0 cryptography-35.0.0 flask-socketio-4.2.1 idna-2.8 importlib-resources-1.5.0 lz4-3.0.2 netifaces-0.11.0 notebook-5.7.8 numpy-1.18.5 openmined.threepio-0.2.0 phe-1.4.0 psutil-5.7.0 pyee-8.2.2 pylibsrtp-0.6.8 python-engineio-4.2.1 python-socketio-5.4.0 requests-2.22.0 requests-toolbelt-0.9.1 shaloop-0.2.1a11 syft-0.2.9 syft-proto-0.5.3 tblib-1.6.0 torch-1.4.0 torchvision-0.5.0 tornado-4.5.3 websocket-client-0.57.0 websockets-8.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","psutil","torch","torchvision","tornado"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"RQFhBN-Fnlgp","executionInfo":{"status":"ok","timestamp":1633258643592,"user_tz":-120,"elapsed":2103,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["import syft as sy"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOoNGgmunvJA","executionInfo":{"status":"ok","timestamp":1633258666199,"user_tz":-120,"elapsed":912,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["hook = sy.TorchHook(torch)\n","nr_of_instances = 4\n","instances = []\n","for i in range(nr_of_instances):\n","  instances.append(sy.VirtualWorker(hook, id=str(i)))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"066yRC19oVN8","executionInfo":{"status":"ok","timestamp":1633258726902,"user_tz":-120,"elapsed":327,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["class Arguments():\n","    def __init__(self):\n","        self.batch_size = 32\n","        self.test_batch_size = 1000\n","        self.epochs = 25\n","        self.lr = 3e-3\n","        self.seed = 1\n","        self.log_interval = 10\n","        self.save_model = False\n","args = Arguments()\n","\n","torch.manual_seed(args.seed)\n","kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-tPO3rCpDqB","executionInfo":{"status":"ok","timestamp":1633258760534,"user_tz":-120,"elapsed":31599,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"9ff8b844-0f8f-42f0-f60b-e6acdf7bd8f8"},"source":["federated_data_loader = sy.FederatedDataLoader(data_set.federate(instances),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:The following options are not supported: num_workers: 1, pin_memory: True\n"]}]},{"cell_type":"code","metadata":{"id":"13oQqXfHrDX5","executionInfo":{"status":"ok","timestamp":1633259211381,"user_tz":-120,"elapsed":217,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":["def train(args, model, fed_train_loader, optims, epoch):\n","    model.train()\n","\n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","    for batch_idx, (data, target) in enumerate(fed_train_loader): # <-- now it is a distributed dataset\n","        model.send(data.location)\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer = optims.get_optim(data.location.id)\n","        optimizer.zero_grad()\n","        \n","        output = model(data)\n","\n","        loss = criterion(output, target)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.get()\n","        if batch_idx % args.log_interval == 0:\n","            loss = loss.get()\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * args.batch_size, len(fed_train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(fed_train_loader), loss.item()))\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hR2YMmTLs49U","executionInfo":{"status":"ok","timestamp":1633261325402,"user_tz":-120,"elapsed":2113698,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}},"outputId":"b14d36a6-b5ee-4596-83bf-0c034fb97a17"},"source":["\n","from syft.federated.floptimizer import Optims\n","\n","torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","\n","for fold, (train_ids, test_ids) in enumerate(kfold.split(data_set)):\n","    \n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n","    \n","    test_loader = torch.utils.data.DataLoader(\n","                      data_set,\n","                      batch_size=32, sampler=test_subsampler)\n","    \n","    federated_model = Model3().to(device)\n","    \n","    federated_optimizer = Optims(list(range(nr_of_instances)), optim=torch.optim.Adam(params=federated_model.parameters(),lr=args.lr))\n","    \n","    for epoch in range(1, args.epochs + 1):\n","\n","      train(args, federated_model, federated_data_loader, federated_optimizer, epoch)\n","\n","      if (args.save_model):\n","        torch.save(federated_model.state_dict(), f'{TARGET_FOLDER}/federated_fold_{fold}_epoch_{epoch}.ckpt')\n","\n","      test(federated_model, test_loader, epoch)\n","\n","\n","\n","torch.set_default_tensor_type(torch.FloatTensor)\n"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["FOLD 0\n","--------------------------------\n","Train Epoch: 1 [0/1152 (0%)]\tLoss: 1.030318\n","Train Epoch: 1 [320/1152 (28%)]\tLoss: 0.022952\n","Train Epoch: 1 [640/1152 (56%)]\tLoss: 1.238003\n","Train Epoch: 1 [960/1152 (83%)]\tLoss: 0.000879\n","Loss: 7.61448335647583, Accuracy: 43.181819915771484\n","Train Epoch: 2 [0/1152 (0%)]\tLoss: 10.793051\n","Train Epoch: 2 [320/1152 (28%)]\tLoss: 0.280800\n","Train Epoch: 2 [640/1152 (56%)]\tLoss: 2.237763\n","Train Epoch: 2 [960/1152 (83%)]\tLoss: 0.005692\n","Loss: 6.579258441925049, Accuracy: 43.090911865234375\n","Train Epoch: 3 [0/1152 (0%)]\tLoss: 9.422319\n","Train Epoch: 3 [320/1152 (28%)]\tLoss: 0.355277\n","Train Epoch: 3 [640/1152 (56%)]\tLoss: 5.091043\n","Train Epoch: 3 [960/1152 (83%)]\tLoss: 0.005065\n","Loss: 6.6749796867370605, Accuracy: 43.3636360168457\n","Train Epoch: 4 [0/1152 (0%)]\tLoss: 8.836844\n","Train Epoch: 4 [320/1152 (28%)]\tLoss: 0.323474\n","Train Epoch: 4 [640/1152 (56%)]\tLoss: 4.819616\n","Train Epoch: 4 [960/1152 (83%)]\tLoss: 0.023365\n","Loss: 4.348712921142578, Accuracy: 43.090911865234375\n","Train Epoch: 5 [0/1152 (0%)]\tLoss: 6.786171\n","Train Epoch: 5 [320/1152 (28%)]\tLoss: 0.128516\n","Train Epoch: 5 [640/1152 (56%)]\tLoss: 5.668103\n","Train Epoch: 5 [960/1152 (83%)]\tLoss: 0.008052\n","Loss: 4.313689231872559, Accuracy: 43.0\n","Train Epoch: 6 [0/1152 (0%)]\tLoss: 7.513593\n","Train Epoch: 6 [320/1152 (28%)]\tLoss: 0.346172\n","Train Epoch: 6 [640/1152 (56%)]\tLoss: 4.310794\n","Train Epoch: 6 [960/1152 (83%)]\tLoss: 0.019894\n","Loss: 3.3594815731048584, Accuracy: 43.090911865234375\n","Train Epoch: 7 [0/1152 (0%)]\tLoss: 7.772257\n","Train Epoch: 7 [320/1152 (28%)]\tLoss: 0.360881\n","Train Epoch: 7 [640/1152 (56%)]\tLoss: 4.879443\n","Train Epoch: 7 [960/1152 (83%)]\tLoss: 0.051786\n","Loss: 4.380886077880859, Accuracy: 42.90909194946289\n","Train Epoch: 8 [0/1152 (0%)]\tLoss: 7.458767\n","Train Epoch: 8 [320/1152 (28%)]\tLoss: 0.307616\n","Train Epoch: 8 [640/1152 (56%)]\tLoss: 4.289909\n","Train Epoch: 8 [960/1152 (83%)]\tLoss: 0.004896\n","Loss: 6.447132587432861, Accuracy: 43.090911865234375\n","Train Epoch: 9 [0/1152 (0%)]\tLoss: 9.079932\n","Train Epoch: 9 [320/1152 (28%)]\tLoss: 0.190344\n","Train Epoch: 9 [640/1152 (56%)]\tLoss: 4.815752\n","Train Epoch: 9 [960/1152 (83%)]\tLoss: 0.013909\n","Loss: 6.493903636932373, Accuracy: 43.45454788208008\n","Train Epoch: 10 [0/1152 (0%)]\tLoss: 9.287166\n","Train Epoch: 10 [320/1152 (28%)]\tLoss: 0.034118\n","Train Epoch: 10 [640/1152 (56%)]\tLoss: 3.663532\n","Train Epoch: 10 [960/1152 (83%)]\tLoss: 0.007121\n","Loss: 5.197165012359619, Accuracy: 43.090911865234375\n","Train Epoch: 11 [0/1152 (0%)]\tLoss: 8.697941\n","Train Epoch: 11 [320/1152 (28%)]\tLoss: 0.252154\n","Train Epoch: 11 [640/1152 (56%)]\tLoss: 3.707188\n","Train Epoch: 11 [960/1152 (83%)]\tLoss: 0.027777\n","Loss: 4.867053985595703, Accuracy: 43.0\n","Train Epoch: 12 [0/1152 (0%)]\tLoss: 8.359986\n","Train Epoch: 12 [320/1152 (28%)]\tLoss: 0.143037\n","Train Epoch: 12 [640/1152 (56%)]\tLoss: 4.417411\n","Train Epoch: 12 [960/1152 (83%)]\tLoss: 0.015275\n","Loss: 3.228560209274292, Accuracy: 43.181819915771484\n","Train Epoch: 13 [0/1152 (0%)]\tLoss: 7.789771\n","Train Epoch: 13 [320/1152 (28%)]\tLoss: 0.120306\n","Train Epoch: 13 [640/1152 (56%)]\tLoss: 5.128081\n","Train Epoch: 13 [960/1152 (83%)]\tLoss: 0.013518\n","Loss: 2.11275577545166, Accuracy: 43.0\n","Train Epoch: 14 [0/1152 (0%)]\tLoss: 8.509157\n","Train Epoch: 14 [320/1152 (28%)]\tLoss: 0.197842\n","Train Epoch: 14 [640/1152 (56%)]\tLoss: 4.231730\n","Train Epoch: 14 [960/1152 (83%)]\tLoss: 0.017988\n","Loss: 1.8316538333892822, Accuracy: 43.0\n","Train Epoch: 15 [0/1152 (0%)]\tLoss: 7.744580\n","Train Epoch: 15 [320/1152 (28%)]\tLoss: 0.889179\n","Train Epoch: 15 [640/1152 (56%)]\tLoss: 3.404031\n","Train Epoch: 15 [960/1152 (83%)]\tLoss: 0.029493\n","Loss: 3.808781147003174, Accuracy: 43.090911865234375\n","Train Epoch: 16 [0/1152 (0%)]\tLoss: 8.952789\n","Train Epoch: 16 [320/1152 (28%)]\tLoss: 0.391491\n","Train Epoch: 16 [640/1152 (56%)]\tLoss: 3.669521\n","Train Epoch: 16 [960/1152 (83%)]\tLoss: 0.015349\n","Loss: 4.3609185218811035, Accuracy: 42.6363639831543\n","Train Epoch: 17 [0/1152 (0%)]\tLoss: 8.997241\n","Train Epoch: 17 [320/1152 (28%)]\tLoss: 0.326306\n","Train Epoch: 17 [640/1152 (56%)]\tLoss: 4.573423\n","Train Epoch: 17 [960/1152 (83%)]\tLoss: 0.047104\n","Loss: 4.608251571655273, Accuracy: 43.090911865234375\n","Train Epoch: 18 [0/1152 (0%)]\tLoss: 8.800382\n","Train Epoch: 18 [320/1152 (28%)]\tLoss: 0.477712\n","Train Epoch: 18 [640/1152 (56%)]\tLoss: 3.244841\n","Train Epoch: 18 [960/1152 (83%)]\tLoss: 0.057247\n","Loss: 4.442568778991699, Accuracy: 42.6363639831543\n","Train Epoch: 19 [0/1152 (0%)]\tLoss: 8.915538\n","Train Epoch: 19 [320/1152 (28%)]\tLoss: 0.464449\n","Train Epoch: 19 [640/1152 (56%)]\tLoss: 4.540884\n","Train Epoch: 19 [960/1152 (83%)]\tLoss: 0.055559\n","Loss: 4.835024833679199, Accuracy: 43.181819915771484\n","Train Epoch: 20 [0/1152 (0%)]\tLoss: 7.999158\n","Train Epoch: 20 [320/1152 (28%)]\tLoss: 0.223006\n","Train Epoch: 20 [640/1152 (56%)]\tLoss: 4.898367\n","Train Epoch: 20 [960/1152 (83%)]\tLoss: 0.029953\n","Loss: 6.107692718505859, Accuracy: 42.90909194946289\n","Train Epoch: 21 [0/1152 (0%)]\tLoss: 9.125622\n","Train Epoch: 21 [320/1152 (28%)]\tLoss: 0.358946\n","Train Epoch: 21 [640/1152 (56%)]\tLoss: 3.097407\n","Train Epoch: 21 [960/1152 (83%)]\tLoss: 0.029471\n","Loss: 4.000046730041504, Accuracy: 43.090911865234375\n","Train Epoch: 22 [0/1152 (0%)]\tLoss: 7.917891\n","Train Epoch: 22 [320/1152 (28%)]\tLoss: 0.176785\n","Train Epoch: 22 [640/1152 (56%)]\tLoss: 2.540366\n","Train Epoch: 22 [960/1152 (83%)]\tLoss: 0.033892\n","Loss: 3.5434317588806152, Accuracy: 43.181819915771484\n","Train Epoch: 23 [0/1152 (0%)]\tLoss: 8.342074\n","Train Epoch: 23 [320/1152 (28%)]\tLoss: 0.219294\n","Train Epoch: 23 [640/1152 (56%)]\tLoss: 2.563795\n","Train Epoch: 23 [960/1152 (83%)]\tLoss: 0.047865\n","Loss: 3.2603790760040283, Accuracy: 42.81818389892578\n","Train Epoch: 24 [0/1152 (0%)]\tLoss: 8.729256\n","Train Epoch: 24 [320/1152 (28%)]\tLoss: 0.187197\n","Train Epoch: 24 [640/1152 (56%)]\tLoss: 2.636755\n","Train Epoch: 24 [960/1152 (83%)]\tLoss: 0.032470\n","Loss: 2.450578212738037, Accuracy: 42.81818389892578\n","Train Epoch: 25 [0/1152 (0%)]\tLoss: 8.738503\n","Train Epoch: 25 [320/1152 (28%)]\tLoss: 0.844909\n","Train Epoch: 25 [640/1152 (56%)]\tLoss: 2.695233\n","Train Epoch: 25 [960/1152 (83%)]\tLoss: 0.049874\n","Loss: 4.227480411529541, Accuracy: 43.0\n","FOLD 1\n","--------------------------------\n","Train Epoch: 1 [0/1152 (0%)]\tLoss: 1.402820\n","Train Epoch: 1 [320/1152 (28%)]\tLoss: 0.050989\n","Train Epoch: 1 [640/1152 (56%)]\tLoss: 1.164823\n","Train Epoch: 1 [960/1152 (83%)]\tLoss: 0.000738\n","Loss: 5.035303592681885, Accuracy: 52.36363983154297\n","Train Epoch: 2 [0/1152 (0%)]\tLoss: 8.753311\n","Train Epoch: 2 [320/1152 (28%)]\tLoss: 0.039413\n","Train Epoch: 2 [640/1152 (56%)]\tLoss: 2.025403\n","Train Epoch: 2 [960/1152 (83%)]\tLoss: 0.001367\n","Loss: 10.8536958694458, Accuracy: 52.90909194946289\n","Train Epoch: 3 [0/1152 (0%)]\tLoss: 12.466160\n","Train Epoch: 3 [320/1152 (28%)]\tLoss: 0.059804\n","Train Epoch: 3 [640/1152 (56%)]\tLoss: 5.289099\n","Train Epoch: 3 [960/1152 (83%)]\tLoss: 0.003873\n","Loss: 7.644347667694092, Accuracy: 52.36363983154297\n","Train Epoch: 4 [0/1152 (0%)]\tLoss: 11.345733\n","Train Epoch: 4 [320/1152 (28%)]\tLoss: 0.180768\n","Train Epoch: 4 [640/1152 (56%)]\tLoss: 5.332809\n","Train Epoch: 4 [960/1152 (83%)]\tLoss: 0.008523\n","Loss: 5.588291168212891, Accuracy: 52.6363639831543\n","Train Epoch: 5 [0/1152 (0%)]\tLoss: 10.549469\n","Train Epoch: 5 [320/1152 (28%)]\tLoss: 0.271364\n","Train Epoch: 5 [640/1152 (56%)]\tLoss: 3.230695\n","Train Epoch: 5 [960/1152 (83%)]\tLoss: 0.005133\n","Loss: 5.506189823150635, Accuracy: 52.72727584838867\n","Train Epoch: 6 [0/1152 (0%)]\tLoss: 9.400462\n","Train Epoch: 6 [320/1152 (28%)]\tLoss: 0.039068\n","Train Epoch: 6 [640/1152 (56%)]\tLoss: 2.964811\n","Train Epoch: 6 [960/1152 (83%)]\tLoss: 0.005111\n","Loss: 6.279172420501709, Accuracy: 52.54545593261719\n","Train Epoch: 7 [0/1152 (0%)]\tLoss: 10.551570\n","Train Epoch: 7 [320/1152 (28%)]\tLoss: 0.085796\n","Train Epoch: 7 [640/1152 (56%)]\tLoss: 5.217137\n","Train Epoch: 7 [960/1152 (83%)]\tLoss: 0.007271\n","Loss: 7.402074337005615, Accuracy: 52.45454788208008\n","Train Epoch: 8 [0/1152 (0%)]\tLoss: 9.308017\n","Train Epoch: 8 [320/1152 (28%)]\tLoss: 0.108658\n","Train Epoch: 8 [640/1152 (56%)]\tLoss: 5.684062\n","Train Epoch: 8 [960/1152 (83%)]\tLoss: 0.013491\n","Loss: 6.822906494140625, Accuracy: 52.6363639831543\n","Train Epoch: 9 [0/1152 (0%)]\tLoss: 10.197165\n","Train Epoch: 9 [320/1152 (28%)]\tLoss: 0.181027\n","Train Epoch: 9 [640/1152 (56%)]\tLoss: 4.598451\n","Train Epoch: 9 [960/1152 (83%)]\tLoss: 0.018900\n","Loss: 1.5232276916503906, Accuracy: 52.6363639831543\n","Train Epoch: 10 [0/1152 (0%)]\tLoss: 8.744188\n","Train Epoch: 10 [320/1152 (28%)]\tLoss: 0.346661\n","Train Epoch: 10 [640/1152 (56%)]\tLoss: 2.336092\n","Train Epoch: 10 [960/1152 (83%)]\tLoss: 0.004065\n","Loss: 4.817382335662842, Accuracy: 53.45454788208008\n","Train Epoch: 11 [0/1152 (0%)]\tLoss: 9.933462\n","Train Epoch: 11 [320/1152 (28%)]\tLoss: 0.095952\n","Train Epoch: 11 [640/1152 (56%)]\tLoss: 3.731914\n","Train Epoch: 11 [960/1152 (83%)]\tLoss: 0.030733\n","Loss: 7.367063045501709, Accuracy: 52.45454788208008\n","Train Epoch: 12 [0/1152 (0%)]\tLoss: 9.292017\n","Train Epoch: 12 [320/1152 (28%)]\tLoss: 0.620530\n","Train Epoch: 12 [640/1152 (56%)]\tLoss: 5.970956\n","Train Epoch: 12 [960/1152 (83%)]\tLoss: 0.013763\n","Loss: 5.109392166137695, Accuracy: 52.36363983154297\n","Train Epoch: 13 [0/1152 (0%)]\tLoss: 8.733371\n","Train Epoch: 13 [320/1152 (28%)]\tLoss: 0.640991\n","Train Epoch: 13 [640/1152 (56%)]\tLoss: 4.800928\n","Train Epoch: 13 [960/1152 (83%)]\tLoss: 0.021085\n","Loss: 4.953221321105957, Accuracy: 52.54545593261719\n","Train Epoch: 14 [0/1152 (0%)]\tLoss: 9.729777\n","Train Epoch: 14 [320/1152 (28%)]\tLoss: 0.313999\n","Train Epoch: 14 [640/1152 (56%)]\tLoss: 4.837162\n","Train Epoch: 14 [960/1152 (83%)]\tLoss: 0.012809\n","Loss: 4.2869415283203125, Accuracy: 52.81818389892578\n","Train Epoch: 15 [0/1152 (0%)]\tLoss: 9.120032\n","Train Epoch: 15 [320/1152 (28%)]\tLoss: 0.443726\n","Train Epoch: 15 [640/1152 (56%)]\tLoss: 3.465301\n","Train Epoch: 15 [960/1152 (83%)]\tLoss: 0.010667\n","Loss: 4.1960320472717285, Accuracy: 52.90909194946289\n","Train Epoch: 16 [0/1152 (0%)]\tLoss: 9.340094\n","Train Epoch: 16 [320/1152 (28%)]\tLoss: 0.317539\n","Train Epoch: 16 [640/1152 (56%)]\tLoss: 4.678630\n","Train Epoch: 16 [960/1152 (83%)]\tLoss: 0.060280\n","Loss: 4.75539493560791, Accuracy: 52.81818389892578\n","Train Epoch: 17 [0/1152 (0%)]\tLoss: 8.547462\n","Train Epoch: 17 [320/1152 (28%)]\tLoss: 0.351867\n","Train Epoch: 17 [640/1152 (56%)]\tLoss: 5.096107\n","Train Epoch: 17 [960/1152 (83%)]\tLoss: 0.033148\n","Loss: 5.467295169830322, Accuracy: 52.54545593261719\n","Train Epoch: 18 [0/1152 (0%)]\tLoss: 9.253301\n","Train Epoch: 18 [320/1152 (28%)]\tLoss: 0.402103\n","Train Epoch: 18 [640/1152 (56%)]\tLoss: 3.634495\n","Train Epoch: 18 [960/1152 (83%)]\tLoss: 0.035203\n","Loss: 4.393133163452148, Accuracy: 52.72727584838867\n","Train Epoch: 19 [0/1152 (0%)]\tLoss: 7.885398\n","Train Epoch: 19 [320/1152 (28%)]\tLoss: 0.360883\n","Train Epoch: 19 [640/1152 (56%)]\tLoss: 4.471363\n","Train Epoch: 19 [960/1152 (83%)]\tLoss: 0.032888\n","Loss: 3.595912456512451, Accuracy: 52.36363983154297\n","Train Epoch: 20 [0/1152 (0%)]\tLoss: 7.375815\n","Train Epoch: 20 [320/1152 (28%)]\tLoss: 0.580177\n","Train Epoch: 20 [640/1152 (56%)]\tLoss: 3.686475\n","Train Epoch: 20 [960/1152 (83%)]\tLoss: 0.055693\n","Loss: 3.853461980819702, Accuracy: 52.54545593261719\n","Train Epoch: 21 [0/1152 (0%)]\tLoss: 7.409242\n","Train Epoch: 21 [320/1152 (28%)]\tLoss: 1.197007\n","Train Epoch: 21 [640/1152 (56%)]\tLoss: 2.725106\n","Train Epoch: 21 [960/1152 (83%)]\tLoss: 0.154853\n","Loss: 3.494300365447998, Accuracy: 52.54545593261719\n","Train Epoch: 22 [0/1152 (0%)]\tLoss: 7.133082\n","Train Epoch: 22 [320/1152 (28%)]\tLoss: 0.797892\n","Train Epoch: 22 [640/1152 (56%)]\tLoss: 2.539772\n","Train Epoch: 22 [960/1152 (83%)]\tLoss: 0.143131\n","Loss: 3.4843692779541016, Accuracy: 52.45454788208008\n","Train Epoch: 23 [0/1152 (0%)]\tLoss: 7.196799\n","Train Epoch: 23 [320/1152 (28%)]\tLoss: 0.871307\n","Train Epoch: 23 [640/1152 (56%)]\tLoss: 1.775775\n","Train Epoch: 23 [960/1152 (83%)]\tLoss: 0.156451\n","Loss: 3.275156021118164, Accuracy: 52.72727584838867\n","Train Epoch: 24 [0/1152 (0%)]\tLoss: 6.986322\n","Train Epoch: 24 [320/1152 (28%)]\tLoss: 0.767936\n","Train Epoch: 24 [640/1152 (56%)]\tLoss: 1.996580\n","Train Epoch: 24 [960/1152 (83%)]\tLoss: 0.190012\n","Loss: 3.0073704719543457, Accuracy: 52.72727584838867\n","Train Epoch: 25 [0/1152 (0%)]\tLoss: 5.160549\n","Train Epoch: 25 [320/1152 (28%)]\tLoss: 0.756092\n","Train Epoch: 25 [640/1152 (56%)]\tLoss: 2.178035\n","Train Epoch: 25 [960/1152 (83%)]\tLoss: 0.208489\n","Loss: 2.938227415084839, Accuracy: 52.45454788208008\n","FOLD 2\n","--------------------------------\n","Train Epoch: 1 [0/1152 (0%)]\tLoss: 1.178118\n","Train Epoch: 1 [320/1152 (28%)]\tLoss: 0.062942\n","Train Epoch: 1 [640/1152 (56%)]\tLoss: 1.557379\n","Train Epoch: 1 [960/1152 (83%)]\tLoss: 0.000200\n","Loss: 5.558719635009766, Accuracy: 51.3636360168457\n","Train Epoch: 2 [0/1152 (0%)]\tLoss: 10.380054\n","Train Epoch: 2 [320/1152 (28%)]\tLoss: 0.042732\n","Train Epoch: 2 [640/1152 (56%)]\tLoss: 3.283520\n","Train Epoch: 2 [960/1152 (83%)]\tLoss: 0.002948\n","Loss: 7.8366851806640625, Accuracy: 50.90909194946289\n","Train Epoch: 3 [0/1152 (0%)]\tLoss: 10.864204\n","Train Epoch: 3 [320/1152 (28%)]\tLoss: 0.073725\n","Train Epoch: 3 [640/1152 (56%)]\tLoss: 6.098913\n","Train Epoch: 3 [960/1152 (83%)]\tLoss: 0.004031\n","Loss: 6.679419994354248, Accuracy: 51.72727584838867\n","Train Epoch: 4 [0/1152 (0%)]\tLoss: 11.147967\n","Train Epoch: 4 [320/1152 (28%)]\tLoss: 0.078208\n","Train Epoch: 4 [640/1152 (56%)]\tLoss: 5.691757\n","Train Epoch: 4 [960/1152 (83%)]\tLoss: 0.003634\n","Loss: 5.813509941101074, Accuracy: 51.3636360168457\n","Train Epoch: 5 [0/1152 (0%)]\tLoss: 10.547580\n","Train Epoch: 5 [320/1152 (28%)]\tLoss: 0.152344\n","Train Epoch: 5 [640/1152 (56%)]\tLoss: 5.845114\n","Train Epoch: 5 [960/1152 (83%)]\tLoss: 0.008174\n","Loss: 6.693334102630615, Accuracy: 51.272727966308594\n","Train Epoch: 6 [0/1152 (0%)]\tLoss: 11.655586\n","Train Epoch: 6 [320/1152 (28%)]\tLoss: 0.109817\n","Train Epoch: 6 [640/1152 (56%)]\tLoss: 5.379168\n","Train Epoch: 6 [960/1152 (83%)]\tLoss: 0.010561\n","Loss: 6.574805736541748, Accuracy: 51.272727966308594\n","Train Epoch: 7 [0/1152 (0%)]\tLoss: 11.140507\n","Train Epoch: 7 [320/1152 (28%)]\tLoss: 0.099896\n","Train Epoch: 7 [640/1152 (56%)]\tLoss: 6.460041\n","Train Epoch: 7 [960/1152 (83%)]\tLoss: 0.005770\n","Loss: 2.8784730434417725, Accuracy: 52.0\n","Train Epoch: 8 [0/1152 (0%)]\tLoss: 10.625160\n","Train Epoch: 8 [320/1152 (28%)]\tLoss: 0.293137\n","Train Epoch: 8 [640/1152 (56%)]\tLoss: 5.404978\n","Train Epoch: 8 [960/1152 (83%)]\tLoss: 0.011548\n","Loss: 4.4653706550598145, Accuracy: 51.6363639831543\n","Train Epoch: 9 [0/1152 (0%)]\tLoss: 9.751967\n","Train Epoch: 9 [320/1152 (28%)]\tLoss: 0.107719\n","Train Epoch: 9 [640/1152 (56%)]\tLoss: 3.486445\n","Train Epoch: 9 [960/1152 (83%)]\tLoss: 0.005621\n","Loss: 5.714650630950928, Accuracy: 51.45454788208008\n","Train Epoch: 10 [0/1152 (0%)]\tLoss: 9.524814\n","Train Epoch: 10 [320/1152 (28%)]\tLoss: 0.115026\n","Train Epoch: 10 [640/1152 (56%)]\tLoss: 2.306003\n","Train Epoch: 10 [960/1152 (83%)]\tLoss: 0.004889\n","Loss: 4.539464950561523, Accuracy: 51.090911865234375\n","Train Epoch: 11 [0/1152 (0%)]\tLoss: 9.768120\n","Train Epoch: 11 [320/1152 (28%)]\tLoss: 0.085857\n","Train Epoch: 11 [640/1152 (56%)]\tLoss: 3.425396\n","Train Epoch: 11 [960/1152 (83%)]\tLoss: 0.005846\n","Loss: 7.224082946777344, Accuracy: 51.6363639831543\n","Train Epoch: 12 [0/1152 (0%)]\tLoss: 9.929094\n","Train Epoch: 12 [320/1152 (28%)]\tLoss: 0.087603\n","Train Epoch: 12 [640/1152 (56%)]\tLoss: 4.119385\n","Train Epoch: 12 [960/1152 (83%)]\tLoss: 0.008604\n","Loss: 4.109356880187988, Accuracy: 51.272727966308594\n","Train Epoch: 13 [0/1152 (0%)]\tLoss: 11.034925\n","Train Epoch: 13 [320/1152 (28%)]\tLoss: 0.044611\n","Train Epoch: 13 [640/1152 (56%)]\tLoss: 4.519742\n","Train Epoch: 13 [960/1152 (83%)]\tLoss: 0.007217\n","Loss: 4.244318962097168, Accuracy: 51.272727966308594\n","Train Epoch: 14 [0/1152 (0%)]\tLoss: 9.719112\n","Train Epoch: 14 [320/1152 (28%)]\tLoss: 0.110262\n","Train Epoch: 14 [640/1152 (56%)]\tLoss: 5.470989\n","Train Epoch: 14 [960/1152 (83%)]\tLoss: 0.017300\n","Loss: 4.075232028961182, Accuracy: 51.54545593261719\n","Train Epoch: 15 [0/1152 (0%)]\tLoss: 10.541148\n","Train Epoch: 15 [320/1152 (28%)]\tLoss: 0.195292\n","Train Epoch: 15 [640/1152 (56%)]\tLoss: 2.906955\n","Train Epoch: 15 [960/1152 (83%)]\tLoss: 0.019563\n","Loss: 3.279235601425171, Accuracy: 51.272727966308594\n","Train Epoch: 16 [0/1152 (0%)]\tLoss: 9.380203\n","Train Epoch: 16 [320/1152 (28%)]\tLoss: 0.056499\n","Train Epoch: 16 [640/1152 (56%)]\tLoss: 6.912731\n","Train Epoch: 16 [960/1152 (83%)]\tLoss: 0.019841\n","Loss: 2.995328903198242, Accuracy: 51.72727584838867\n","Train Epoch: 17 [0/1152 (0%)]\tLoss: 9.113321\n","Train Epoch: 17 [320/1152 (28%)]\tLoss: 0.096487\n","Train Epoch: 17 [640/1152 (56%)]\tLoss: 3.710545\n","Train Epoch: 17 [960/1152 (83%)]\tLoss: 0.019374\n","Loss: 3.077690839767456, Accuracy: 51.54545593261719\n","Train Epoch: 18 [0/1152 (0%)]\tLoss: 9.367993\n","Train Epoch: 18 [320/1152 (28%)]\tLoss: 0.160684\n","Train Epoch: 18 [640/1152 (56%)]\tLoss: 5.490479\n","Train Epoch: 18 [960/1152 (83%)]\tLoss: 0.029832\n","Loss: 3.2616047859191895, Accuracy: 51.54545593261719\n","Train Epoch: 19 [0/1152 (0%)]\tLoss: 9.925797\n","Train Epoch: 19 [320/1152 (28%)]\tLoss: 0.375858\n","Train Epoch: 19 [640/1152 (56%)]\tLoss: 6.273789\n","Train Epoch: 19 [960/1152 (83%)]\tLoss: 0.054892\n","Loss: 3.2687435150146484, Accuracy: 51.272727966308594\n","Train Epoch: 20 [0/1152 (0%)]\tLoss: 10.351751\n","Train Epoch: 20 [320/1152 (28%)]\tLoss: 0.831368\n","Train Epoch: 20 [640/1152 (56%)]\tLoss: 6.865274\n","Train Epoch: 20 [960/1152 (83%)]\tLoss: 0.086405\n","Loss: 3.439805507659912, Accuracy: 50.90909194946289\n","Train Epoch: 21 [0/1152 (0%)]\tLoss: 9.698531\n","Train Epoch: 21 [320/1152 (28%)]\tLoss: 1.004284\n","Train Epoch: 21 [640/1152 (56%)]\tLoss: 4.536445\n","Train Epoch: 21 [960/1152 (83%)]\tLoss: 0.008242\n","Loss: 12.702570915222168, Accuracy: 51.3636360168457\n","Train Epoch: 22 [0/1152 (0%)]\tLoss: 9.453301\n","Train Epoch: 22 [320/1152 (28%)]\tLoss: 0.106238\n","Train Epoch: 22 [640/1152 (56%)]\tLoss: 6.483950\n","Train Epoch: 22 [960/1152 (83%)]\tLoss: 0.004029\n","Loss: 4.679002285003662, Accuracy: 51.181819915771484\n","Train Epoch: 23 [0/1152 (0%)]\tLoss: 11.983921\n","Train Epoch: 23 [320/1152 (28%)]\tLoss: 0.077995\n","Train Epoch: 23 [640/1152 (56%)]\tLoss: 3.271507\n","Train Epoch: 23 [960/1152 (83%)]\tLoss: 0.002114\n","Loss: 6.072860240936279, Accuracy: 51.6363639831543\n","Train Epoch: 24 [0/1152 (0%)]\tLoss: 10.865967\n","Train Epoch: 24 [320/1152 (28%)]\tLoss: 0.126796\n","Train Epoch: 24 [640/1152 (56%)]\tLoss: 3.394018\n","Train Epoch: 24 [960/1152 (83%)]\tLoss: 0.037450\n","Loss: 5.521590232849121, Accuracy: 51.45454788208008\n","Train Epoch: 25 [0/1152 (0%)]\tLoss: 9.172252\n","Train Epoch: 25 [320/1152 (28%)]\tLoss: 0.183512\n","Train Epoch: 25 [640/1152 (56%)]\tLoss: 3.330215\n","Train Epoch: 25 [960/1152 (83%)]\tLoss: 0.054695\n","Loss: 3.7141201496124268, Accuracy: 51.6363639831543\n"]}]},{"cell_type":"code","metadata":{"id":"YLC89DQ2rKQe","executionInfo":{"status":"aborted","timestamp":1633256344073,"user_tz":-120,"elapsed":20,"user":{"displayName":"nameohne","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13436963316072193918"}}},"source":[""],"execution_count":null,"outputs":[]}]}