{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwZJFe5-pCLV",
    "outputId": "5b270b81-a9e7-43d9-cc42-378c0fed3740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from click->opendatasets) (0.4.4)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=514e82bccb09ad66a25a018a02df371acbb91e3cd7ea61b35d18ad62cfb6026a\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: urllib3, text-unidecode, idna, charset-normalizer, tqdm, requests, python-slugify, kaggle, click, opendatasets\n",
      "Successfully installed charset-normalizer-2.0.4 click-8.0.1 idna-3.2 kaggle-1.5.12 opendatasets-0.1.20 python-slugify-5.0.2 requests-2.26.0 text-unidecode-1.3 tqdm-4.62.2 urllib3-1.26.6\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.40.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-win_amd64.whl (909 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=cba4853eab105290088fff00de7b6e4c7f189b2d1e3e9094458f98170f6102df\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=9aa1f355bbc18674d560a501e6ff2e800f6b0896e532c633acd35f327272bdda\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33696 sha256=bd42bf4719ea3009f798680ef6aa900c02b2065cbfb750b42b0272a443e4851b\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-2.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_federated\n",
      "  Downloading tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
      "  Downloading tensorflow_federated-0.18.0-py2.py3-none-any.whl (578 kB)\n",
      "  Downloading tensorflow_federated-0.17.0-py2.py3-none-any.whl (517 kB)\n",
      "Collecting retrying~=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting absl-py~=0.9.0\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting grpcio~=1.29.0\n",
      "  Downloading grpcio-1.29.0-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "Collecting tensorflow-model-optimization~=0.4.0\n",
      "  Downloading tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172 kB)\n",
      "Collecting numpy~=1.18.4\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Collecting tensorflow-privacy~=0.5.0\n",
      "  Downloading tensorflow_privacy-0.5.2-py3-none-any.whl (192 kB)\n",
      "Collecting portpicker~=1.3.1\n",
      "  Downloading portpicker-1.3.9-py3-none-any.whl (13 kB)\n",
      "Collecting tensorflow-addons~=0.11.1\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-win_amd64.whl (911 kB)\n",
      "Collecting tensorflow~=2.3.0\n",
      "  Downloading tensorflow-2.3.4-cp38-cp38-win_amd64.whl (342.7 MB)\n",
      "Collecting semantic-version~=2.8.5\n",
      "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
      "Collecting attrs~=19.3.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting cachetools~=3.1.1\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "Requirement already satisfied: six in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from absl-py~=0.9.0->tensorflow_federated) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.36.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.2)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.6.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.26.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.1.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Collecting scipy>=0.17\n",
      "  Downloading scipy-1.7.1-cp38-cp38-win_amd64.whl (33.7 MB)\n",
      "Collecting mpmath\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Building wheels for collected packages: absl-py, retrying\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121930 sha256=d48858ed8d8c44579afff41bc5db58d440a8598b2bf8ccf023c3260a634e1ea7\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\1d\\10\\8e\\2f79b924179ff1e6510933d63eb851bea01054fff262343b7a\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=76388d1e8bf5d5b6e5964a04050c5aad93635b4b8aa1690deb4f6e60f34ed0c6\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\c4\\a7\\48\\0a434133f6d56e878ca511c0e6c38326907c0792f67b476e56\n",
      "Successfully built absl-py retrying\n",
      "Installing collected packages: cachetools, numpy, grpcio, absl-py, typeguard, tensorflow-estimator, scipy, mpmath, h5py, gast, dm-tree, tensorflow-privacy, tensorflow-model-optimization, tensorflow-addons, tensorflow, semantic-version, retrying, portpicker, attrs, tensorflow-federated\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.40.0\n",
      "    Uninstalling grpcio-1.40.0:\n",
      "      Successfully uninstalled grpcio-1.40.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "Successfully installed absl-py-0.9.0 attrs-19.3.0 cachetools-3.1.1 dm-tree-0.1.6 gast-0.3.3 grpcio-1.29.0 h5py-2.10.0 mpmath-1.2.1 numpy-1.18.5 portpicker-1.3.9 retrying-1.3.3 scipy-1.7.1 semantic-version-2.8.5 tensorflow-2.3.4 tensorflow-addons-0.11.2 tensorflow-estimator-2.3.0 tensorflow-federated-0.17.0 tensorflow-model-optimization-0.4.1 tensorflow-privacy-0.5.2 typeguard-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jpd3iU1El1ZT"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TraicDeCpHii",
    "outputId": "ec940e7b-0a35-4c28-9a85-98f7d7b3f6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\chest-xray-covid19-pneumonia\" (use force=True to force download)\n",
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: donatjr\n",
      "Your Kaggle Key: ········\n",
      "Downloading novel-corona-virus-2019-dataset.zip to .\\novel-corona-virus-2019-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.52M/8.52M [00:00<00:00, 13.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\")\n",
    "od.download(\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JPFiGGjbhta_"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 2\n",
    "TARGET_FOLDER = \"weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WvAJ8eTnTRkP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c1d917fb9552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW1RF4LvUUsG"
   },
   "source": [
    "Resize Images to 244, 244. By using to_tensor the images are already normalized between 0 and 1. \"The image object is an array of (244, 244, 3) should be flattened to be list (178, 608).\" What? wie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Z_GY07pSzQT"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((244, 244)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "test_set = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/test', transform=transform)\n",
    "train_set = dataset = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l9HuvLqC5rMB"
   },
   "outputs": [],
   "source": [
    "train_set.targets = torch.tensor(train_set.targets)\n",
    "train_set.targets[train_set.targets > 0] = 1\n",
    "\n",
    "test_set.targets = torch.tensor(test_set.targets)\n",
    "test_set.targets[test_set.targets > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LYb6MycBPtB"
   },
   "source": [
    "The dataset is extremely unbalanced. Maybe sth which should be handled?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4budf2sh5XQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87o-9Dz5WX-8"
   },
   "source": [
    "Creating Sample Data Dictionary\n",
    "After flattening the data dictionary instance was created for each image sample to represent the image data (features) and its label.\n",
    "Creating Samples and labels Tensors keras Objects\n",
    "To build keras the dataset, the keras tensor object should be built for features and keras tensor object for labels.\n",
    "Create Keras Tensor Dataset\n",
    "Create keras dataset by using from_tensor_slices API.\n",
    "\n",
    "nicht nötig?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCqEJbYaWH8A"
   },
   "source": [
    "Data Repetition\n",
    "Data repeated to simulate the number of clients.\n",
    "Data Shuffling\n",
    "Data shuffled to avoid obtaining the same results.\n",
    "Data Batching\n",
    "Data grouped into batches to enhance their performance.\n",
    "\n",
    "vielleicht nicht nötig mit syft? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VAi62OkRTZfV"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHO2CHSsWvi2"
   },
   "source": [
    "Data Mapping\n",
    "ndarray dataset flattend to 1 darray dataset.\n",
    "Data Prefetching\n",
    "Data cached in memory for better performance.\n",
    "\n",
    "wie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVxct9thFVz2"
   },
   "source": [
    "In Paper aus References haben sie unter anderen ResNet18 benutzt: https://arxiv.org/pdf/2007.05592.pdf\n",
    "\n",
    "Deshalb würde ich das probieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWhXD92ZRzNP",
    "outputId": "b473ca2e-2e15-4480-dab7-4fd210df225a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Qihhc_PTgVr",
    "outputId": "f0cbdde8-51b8-4b39-b9a9-b12b0b5ad7c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(512, 1)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zAItZORR0N0H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UgvIilEB9nYk"
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_labels(labels):\n",
    "  labels[labels>0] = 1\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "A5Me-0EHR_md"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer):\n",
    "    \"\"\"\n",
    "    model -- neural net\n",
    "    data_loader -- dataloader for train images\n",
    "    optimizer -- optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    criterion = torch.nn.BCELoss().to(device)\n",
    "    \n",
    "    for step, [images, labels] in enumerate(data_loader,0):\n",
    "        images = images.to(device)\n",
    "        labels = transform_labels(labels.to(device))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        result = torch.nn.functional.softmax(model(images))\n",
    "      \n",
    "        loss = criterion(result, labels.unsqueeze(1).type(torch.FloatTensor))\n",
    "                \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        if step%10 == 0:\n",
    "            print(f\"Step: {step}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "icqqTTRN-ht6"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(result, labels):\n",
    "    result = torch.round(result)\n",
    "\n",
    "    correct_results_sum = (result == labels).sum().float()\n",
    "    acc = correct_results_sum/labels.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Xk2CR_whWTxw"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch):\n",
    "    \"\"\"    \n",
    "    model -- neural net \n",
    "    test_loader -- dataloader of test images\n",
    "    epoch -- current epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(test_loader,0):\n",
    "        images = images.to(device)\n",
    "        labels = transform_labels(labels.to(device))\n",
    "        result = torch.nn.functional.softmax(model(images))\n",
    "        loss += criterion(result.detach(), labels.detach())\n",
    "        accuracy += calc_accuracy(result.detach(), labels.detach())\n",
    "    loss /= step\n",
    "    accuracy /=  step\n",
    "  \n",
    "    print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "jzfyMxdBVoB1",
    "outputId": "c3a6aff3-d0a6-4a92-fd67-8d72310e227d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, loss: 9.375\n",
      "Step: 10, loss: 9.375\n",
      "Step: 20, loss: 9.375\n",
      "Step: 30, loss: 6.25\n",
      "Step: 40, loss: 9.375\n",
      "Step: 50, loss: 9.375\n",
      "Step: 60, loss: 18.75\n",
      "Step: 70, loss: 9.375\n",
      "Step: 80, loss: 6.25\n",
      "Step: 90, loss: 12.5\n",
      "Step: 100, loss: 12.5\n",
      "Step: 110, loss: 12.5\n",
      "Step: 120, loss: 0.0\n",
      "Step: 130, loss: 9.375\n",
      "Step: 140, loss: 15.625\n",
      "Step: 150, loss: 3.125\n",
      "Step: 160, loss: 8.333333015441895\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-48fda31dfa26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# save interim weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{TARGET_FOLDER}/epoch_{epoch}.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights/epoch_0.ckpt'"
     ]
    }
   ],
   "source": [
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# start training\n",
    "for epoch in range(EPOCHS):\n",
    "    train(model, train_loader, optimizer)\n",
    "    \n",
    "    # save interim weights\n",
    "    torch.save(model.state_dict(), f'{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "    test(model, test_loader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc9Z1kAKniiG"
   },
   "source": [
    "# Federated\n",
    "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
    "\n",
    "Neuste Version (0.3.x) hat kein TorchHook -> Lösung finden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUJv9PNBuANj",
    "outputId": "aae7edfe-b9ce-4cdb-8e6a-10f7a5c3847b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: syft==0.2.9 in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
      "Requirement already satisfied: websockets~=8.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (8.1)\n",
      "Requirement already satisfied: numpy~=1.18.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.18.5)\n",
      "Requirement already satisfied: tblib~=1.6.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.6.0)\n",
      "Requirement already satisfied: lz4~=3.0.2 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (3.0.2)\n",
      "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.4)\n",
      "Requirement already satisfied: torch~=1.4.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.0)\n",
      "Requirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (4.5.3)\n",
      "Requirement already satisfied: requests-toolbelt==0.9.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.9.1)\n",
      "Requirement already satisfied: shaloop==0.2.1-alpha.11 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.2.1a11)\n",
      "Requirement already satisfied: requests~=2.22.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (2.22.0)\n",
      "Requirement already satisfied: flask-socketio~=4.2.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (4.2.1)\n",
      "Requirement already satisfied: psutil==5.7.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (5.7.0)\n",
      "Requirement already satisfied: openmined.threepio==0.2.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.2.0)\n",
      "Requirement already satisfied: phe~=1.4.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.0)\n",
      "Requirement already satisfied: notebook==5.7.8 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (5.7.8)\n",
      "Requirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.2)\n",
      "Requirement already satisfied: RestrictedPython~=5.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (5.1)\n",
      "Requirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.4)\n",
      "Requirement already satisfied: syft-proto~=0.5.2 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.5.3)\n",
      "Requirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.4.1)\n",
      "Requirement already satisfied: importlib-resources~=1.5.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.5.0)\n",
      "Requirement already satisfied: aiortc==0.9.28 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.9.28)\n",
      "Requirement already satisfied: torchvision~=0.5.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.5.0)\n",
      "Requirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n",
      "Requirement already satisfied: websocket-client~=0.57.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.57.0)\n",
      "Requirement already satisfied: av<9.0.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (8.0.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (1.14.6)\n",
      "Requirement already satisfied: pyee>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (8.2.2)\n",
      "Requirement already satisfied: aioice<0.7.0,>=0.6.17 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (0.6.18)\n",
      "Requirement already satisfied: cryptography>=2.2 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (3.4.8)\n",
      "Requirement already satisfied: pylibsrtp>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (0.6.8)\n",
      "Requirement already satisfied: crc32c in /usr/local/lib/python3.7/dist-packages (from aiortc==0.9.28->syft==0.2.9) (2.2.post0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.7.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (2.11.3)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (4.10.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (22.2.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.11.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.0.5)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.3.5)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.6.1)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook==5.7.8->syft==0.2.9) (5.1.3)\n",
      "Requirement already satisfied: pycparser>=2 in /usr/local/lib/python3.7/dist-packages (from shaloop==0.2.1-alpha.11->syft==0.2.9) (2.20)\n",
      "Requirement already satisfied: netifaces in /usr/local/lib/python3.7/dist-packages (from aioice<0.7.0,>=0.6.17->aiortc==0.9.28->syft==0.2.9) (0.11.0)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.9) (1.0.1)\n",
      "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from flask-socketio~=4.2.1->syft==0.2.9) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (4.6.4)\n",
      "Requirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.7/dist-packages (from importlib-resources~=1.5.0->syft==0.2.9) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook==5.7.8->syft==0.2.9) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook==5.7.8->syft==0.2.9) (1.15.0)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from python-socketio>=4.3.0->flask-socketio~=4.2.1->syft==0.2.9) (0.21.3)\n",
      "Requirement already satisfied: python-engineio>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from python-socketio>=4.3.0->flask-socketio~=4.2.1->syft==0.2.9) (4.2.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (2021.5.30)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.9) (3.0.4)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.5.2->syft==0.2.9) (3.17.3)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft==0.2.9) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->importlib-resources~=1.5.0->syft==0.2.9) (3.7.4.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook==5.7.8->syft==0.2.9) (5.5.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (2.6.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.4.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.8.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (1.0.18)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (57.4.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft==0.2.9) (0.2.5)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.7.1)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (4.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook==5.7.8->syft==0.2.9) (0.8.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook==5.7.8->syft==0.2.9) (2.6.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft==0.2.9) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install syft==0.2.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RQFhBN-Fnlgp"
   },
   "outputs": [],
   "source": [
    "import syft as sy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOoNGgmunvJA",
    "outputId": "7ad58512-c9e4-45ef-a5ce-3edc0d423973"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "nr_of_instances = 2\n",
    "instances = []\n",
    "for i in range(nr_of_instances):\n",
    "  instances.append(sy.VirtualWorker(hook, id=str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "066yRC19oVN8"
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.0001\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        self.save_model = False\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5-tPO3rCpDqB"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader(train_set.federate(instances),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "13oQqXfHrDX5"
   },
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optims, epoch):\n",
    "    model.train()\n",
    "\n",
    "    criterion = torch.nn.BCELoss().to(device)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location)\n",
    "        data, target = data.to(device), transform_labels(target.to(device))\n",
    "\n",
    "        optimizer = optims.get_optim(data.location.id)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = torch.nn.functional.softmax(model(data))\n",
    "\n",
    "        loss = criterion(output, target.unsqueeze(1).type(torch.FloatTensor))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.get()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "hR2YMmTLs49U",
    "outputId": "f2c51ce1-3579-46de-9458-7b03e73c8cec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py:414: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  response = command_method(*args_, **kwargs_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5152 (0%)]\tLoss: 3.453878\n",
      "Train Epoch: 1 [320/5152 (6%)]\tLoss: 3.453878\n",
      "Train Epoch: 1 [640/5152 (12%)]\tLoss: 3.453878\n",
      "Train Epoch: 1 [960/5152 (19%)]\tLoss: 7.771225\n",
      "Train Epoch: 1 [1280/5152 (25%)]\tLoss: 4.317347\n",
      "Train Epoch: 1 [1600/5152 (31%)]\tLoss: 2.590408\n",
      "Train Epoch: 1 [1920/5152 (37%)]\tLoss: 7.771225\n",
      "Train Epoch: 1 [2240/5152 (43%)]\tLoss: 3.453878\n",
      "Train Epoch: 1 [2560/5152 (50%)]\tLoss: -6.907755\n",
      "Train Epoch: 1 [2880/5152 (56%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3200/5152 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3520/5152 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3840/5152 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4160/5152 (81%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4480/5152 (87%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4800/5152 (93%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [5120/5152 (99%)]\tLoss: 0.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-57e245db1a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{TARGET_FOLDER}/epoch_{epoch}.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "federated_model = torchvision.models.resnet18(pretrained=False)\n",
    "federated_model.fc = torch.nn.Linear(512, 1)\n",
    "\n",
    "from syft.federated.floptimizer import Optims\n",
    "federated_optimizer = Optims(list(range(nr_of_instances)), optim=torch.optim.Adam(params=federated_model.parameters(),lr=args.lr))\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, federated_model, federated_train_loader, federated_optimizer, epoch)\n",
    "\n",
    "    if (args.save_model):\n",
    "      torch.save(federated_model.state_dict(), f'{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "    test(federated_model, test_loader, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLC89DQ2rKQe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
