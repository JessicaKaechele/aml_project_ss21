\section{Conclusion and Outlook}
\comment{Written by Jessica KÃ¤chele}
The last section will give a brief conclusion regarding the results that we obtained and explore some ideas and concepts that can be used to further improve our automated federation test in the future.  

\subsection{Conclusion}
We were able to replicate results of all but one of the papers that we selected and successfully adopted their approaches to utilize the concept of federated learning. In almost all cases the federated version achieved similar results to the centralized one, indicating that federated learning can indeed be used in COVID-19 research to preserve privacy of sensitive medical data. The adaptation was successful in CNN as well as GNN architectures, showing that not only one type of network can be used with federated learning. This could enable research projects with multiple entities (e.g. government health organizations) working together on one big machine learning model, which seemed to be improbable in the past due to privacy concerns.
We did not deploy our federation client models on different hardware and thus completely ignored the communication task usually necessary for aggregating local models. In addition, we used FedAvg, the most basic aggregation technique in federated learning, instead of a more sophisticated approach like weighted FedAvg or algorithms built on top of it like FedProx\cite{fed_prox} or FedSplit\cite{fed_split}. Nevertheless, our results show that federated learning has a huge potential for COVID-19 and other research and that its potential use should be analysed in more contexts.

Further, we developed a Python class which can be used to quickly test how federated learning performs with a given model and training methodology before having to commit to building a custom solution or integrate a complex framework into existing code for this purpose. This will hopefully lower the barrier-of-entrance to more development with federated learning and result in more projects using it in the future.

\subsection{Outlook}
Even though our automated federation test works quite well on the models we tested it with, we cannot guarantee that it works for every project. It should be tested with a more models and datasets to get a higher confidence in its ability to indeed work with most image based PyTorch models. 
Apart from more testing there is also a lot of room for potential improvement in other areas. First, the current data augmentations for increasing the data available on each client needs to be more configurable and make more than the existing augmentations available. Taking this a step further, we could also support non-image data and their transformations. For being able to test more real-world scenarios, it would also be of value to be able to split and distribute the data in different, non-random ways (e.g. give some clients most of the data of one class and other clients most of the data of other classes).
By supporting PyTorch models we, at least in theory, also support models of frameworks that are based on PyTorch. While this is a big portion of all machine learning models, it is by no means all of them. Support for Tensorflow and its respective dataset format is almost a must if we want to make quick testing of federated learning available to a large percentage of the machine learning community.
