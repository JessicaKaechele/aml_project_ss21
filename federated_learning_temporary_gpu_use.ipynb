{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated_learning_with_save.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zwZJFe5-pCLV",
        "outputId": "34e05e6a-ae90-4a63-89e5-8d6ea1ca7996"
      },
      "source": [
        "!pip install opendatasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG0Achp9VWYt",
        "outputId": "17ddae2f-bef7-492a-c00f-4a2826bc586a"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.40.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RLMHIkDVWYv",
        "outputId": "4475637b-3cda-411c-8010-37205064510b"
      },
      "source": [
        "!pip install tensorflow_federated"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_federated\n",
            "  Downloading tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40 kB 37.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 51 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 61 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 71 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 81 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 92 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 102 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 112 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 122 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 133 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 143 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 153 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 163 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 174 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 184 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 194 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 204 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 215 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 225 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 235 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 245 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 256 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 266 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 276 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 286 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 296 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 307 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 317 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 327 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 337 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 348 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 358 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 368 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 378 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 389 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 399 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 409 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 419 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 430 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 440 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 450 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 460 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 471 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 481 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 491 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 501 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 512 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 522 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 532 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 542 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 552 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 563 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 573 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 583 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 593 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 602 kB 32.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization~=0.5.0\n",
            "  Downloading tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting tqdm~=4.28.1\n",
            "  Downloading tqdm-4.28.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying~=1.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.3.3)\n",
            "Requirement already satisfied: jax~=0.2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.2.19)\n",
            "Collecting tensorflow-privacy~=0.5.0\n",
            "  Downloading tensorflow_privacy-0.5.2-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.1.6)\n",
            "Collecting attrs~=19.3.0\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.19.5)\n",
            "Requirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.3.9)\n",
            "Collecting cachetools~=3.1.1\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.12.0)\n",
            "Collecting semantic-version~=2.8.5\n",
            "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib~=0.1.55 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.1.70+cuda111)\n",
            "Collecting tensorflow~=2.5.0\n",
            "  Downloading tensorflow-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 9.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py~=0.10->tensorflow_federated) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib~=0.1.55->tensorflow_federated) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib~=0.1.55->tensorflow_federated) (1.4.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.37.0)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (3.17.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (2.6.0)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow~=2.5.0->tensorflow_federated) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.1.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow_federated) (1.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.5.0)\n",
            "Installing collected packages: cachetools, grpcio, tensorflow-estimator, keras-nightly, tqdm, tensorflow-privacy, tensorflow-model-optimization, tensorflow, semantic-version, attrs, tensorflow-federated\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.2\n",
            "    Uninstalling cachetools-4.2.2:\n",
            "      Successfully uninstalled cachetools-4.2.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.40.0\n",
            "    Uninstalling grpcio-1.40.0:\n",
            "      Successfully uninstalled grpcio-1.40.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.2\n",
            "    Uninstalling tqdm-4.62.2:\n",
            "      Successfully uninstalled tqdm-4.62.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed attrs-19.3.0 cachetools-3.1.1 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 semantic-version-2.8.5 tensorflow-2.5.1 tensorflow-estimator-2.5.0 tensorflow-federated-0.19.0 tensorflow-model-optimization-0.5.0 tensorflow-privacy-0.5.2 tqdm-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBD5W-SJWERa",
        "outputId": "1396852a-e233-49bb-80a1-f3fb16bf7c9f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 18 09:59:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYanrj2lWM4p",
        "outputId": "21932cc5-b0ff-4ff1-af2d-85a8bbdcf9b1"
      },
      "source": [
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.3 MB/s eta 0:15:19tcmalloc: large alloc 1147494400 bytes == 0x564bd22da000 @  0x7fc626cc7615 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99846d00 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b997d5039 0x564b99818409 0x564b997d3c52 0x564b99846c25 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99842915 0x564b997d4afa 0x564b99842c0d 0x564b998419ee\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 1.3 MB/s eta 0:12:17tcmalloc: large alloc 1434370048 bytes == 0x564c16930000 @  0x7fc626cc7615 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99846d00 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b997d5039 0x564b99818409 0x564b997d3c52 0x564b99846c25 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99842915 0x564b997d4afa 0x564b99842c0d 0x564b998419ee\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.5 MB/s eta 0:07:44tcmalloc: large alloc 1792966656 bytes == 0x564b9b762000 @  0x7fc626cc7615 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99846d00 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b997d5039 0x564b99818409 0x564b997d3c52 0x564b99846c25 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99842915 0x564b997d4afa 0x564b99842c0d 0x564b998419ee\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:04:50tcmalloc: large alloc 2241208320 bytes == 0x564c0654a000 @  0x7fc626cc7615 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99846d00 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b998c5c66 0x564b99842daf 0x564b997d5039 0x564b99818409 0x564b997d3c52 0x564b99846c25 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99842915 0x564b997d4afa 0x564b99842c0d 0x564b998419ee\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x564c8beac000 @  0x7fc626cc61e7 0x564b99806067 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b997d4afa 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee\n",
            "tcmalloc: large alloc 2551685120 bytes == 0x564d05974000 @  0x7fc626cc7615 0x564b997d04cc 0x564b998b047a 0x564b997d32ed 0x564b998c4e1d 0x564b99846e99 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99842c0d 0x564b997d4afa 0x564b99842c0d 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d4bda 0x564b99843737 0x564b998419ee 0x564b997d5271\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 5.6 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting torchaudio===0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.19.5)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpd3iU1El1ZT"
      },
      "source": [
        "import opendatasets as od\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TraicDeCpHii",
        "outputId": "b6cdd2b7-7c26-44a8-be17-7fbbe8ce7574"
      },
      "source": [
        "od.download(\"https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\")\n",
        "od.download(\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nameohne\n",
            "Your Kaggle Key: ··········\n",
            "Downloading chest-xray-covid19-pneumonia.zip to ./chest-xray-covid19-pneumonia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.06G/2.06G [00:15<00:00, 147MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nameohne\n",
            "Your Kaggle Key: ··········\n",
            "Downloading novel-corona-virus-2019-dataset.zip to ./novel-corona-virus-2019-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8.52M/8.52M [00:00<00:00, 61.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPFiGGjbhta_"
      },
      "source": [
        "LEARNING_RATE = 0.001 # 0.0001\n",
        "MAX_EPOCHS = 25\n",
        "TARGET_FOLDER = \"weights\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvAJ8eTnTRkP"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW1RF4LvUUsG"
      },
      "source": [
        "Resize Images to 244, 244. By using to_tensor the images are already normalized between 0 and 1. \"The image object is an array of (244, 244, 3) should be flattened to be list (178, 608).\" What? wie?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z_GY07pSzQT"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((244, 244))\n",
        "                                , transforms.ToTensor()]\n",
        "                               #, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # find mean and std of dataset\n",
        "                              )\n",
        "\n",
        "test_set = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/test', transform=transform)\n",
        "\n",
        "train_set = dataset = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/train', transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z9lep1yVWY3"
      },
      "source": [
        "def label_preparation(labels):\n",
        "    labels = np.array(labels)\n",
        "    labels[labels > 0] = 1\n",
        "    return list(labels)\n",
        "\n",
        "def label_preparation_tensor(labels):\n",
        "    labels[labels > 0] = 1\n",
        "    return labels\n",
        "\n",
        "train_set.targets = label_preparation(train_set.targets)\n",
        "\n",
        "test_set.targets = label_preparation(test_set.targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAi62OkRTZfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061dab4c-98c3-4399-b23e-fa628bdd2176"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiu0JHYVWY5"
      },
      "source": [
        "train_loader = None\n",
        "test_loader = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVxct9thFVz2"
      },
      "source": [
        "In Paper aus References haben sie unter anderen ResNet18 benutzt: https://arxiv.org/pdf/2007.05592.pdf\n",
        "\n",
        "Deshalb würde ich das probieren.\n",
        "\n",
        "TODO: test different ones, e.g. VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWhXD92ZRzNP",
        "outputId": "474bd18f-946e-43cf-f59c-a358b6a839eb"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Current device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icqqTTRN-ht6"
      },
      "source": [
        "def calc_accuracy(result, labels):\n",
        "    result = torch.sigmoid(result).round()\n",
        "    \n",
        "    correct_results_sum = (result == labels).sum().float()\n",
        "    acc = correct_results_sum/labels.shape[0]\n",
        "    acc *= 100\n",
        "    \n",
        "    return acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Me-0EHR_md"
      },
      "source": [
        "def train(model, data_loader, optimizer, loss):\n",
        "    \"\"\"\n",
        "    model -- neural net\n",
        "    data_loader -- dataloader for train images\n",
        "    optimizer -- optimizer\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    # use pos weights because of unbalanced data set\n",
        "    accuracy = 0\n",
        "    for step, [images, labels] in enumerate(data_loader, 1):\n",
        "        images = images.to(device)\n",
        "        labels = label_preparation_tensor(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        result = model(images)\n",
        "        targets = labels.unsqueeze(1).float()\n",
        "\n",
        "        loss_value = loss(result.float(), targets)\n",
        "\n",
        "        # backpropagation\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "                                    \n",
        "        if step % 10 == 0:\n",
        "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
        "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk2CR_whWTxw"
      },
      "source": [
        "def test(model, test_loader, loss):\n",
        "    \"\"\"    \n",
        "    model -- neural net \n",
        "    test_loader -- dataloader of test images\n",
        "    epoch -- current epoch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        loss_value = 0\n",
        "        accuracy = 0\n",
        "        for step, [images, labels] in enumerate(test_loader, 1):\n",
        "            images = images.to(device)\n",
        "            labels = label_preparation_tensor(labels.to(device))\n",
        "\n",
        "            result = model(images)\n",
        "            targets = labels.detach().unsqueeze(1).float()\n",
        "\n",
        "            loss_value += loss(result.detach(), targets)\n",
        "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
        "\n",
        "        loss_value /= step\n",
        "        accuracy /=  step\n",
        "  \n",
        "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
        "    return accuracy > 93."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzfyMxdBVoB1"
      },
      "source": [
        "def run_local_training():\n",
        "    model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
        "    model.to(device)\n",
        "\n",
        "    # initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # use pos weights because of unbalanced data set\n",
        "    loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
        "\n",
        "    # start training\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        print(f\"+++ EPOCH: {epoch+1} +++++++++\")\n",
        "        torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy (non-federated)\n",
        "        train(model, train_loader, optimizer, loss)\n",
        "        break\n",
        "        # save interim weights\n",
        "        torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
        "\n",
        "        if test(model, test_loader, loss) and epoch > 4:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uC8i53EVWZA"
      },
      "source": [
        "#run_local_training()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc9Z1kAKniiG"
      },
      "source": [
        "# Federated\n",
        "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
        "\n",
        "i don't think we need syft if we just simulate the federation on one machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mllCcvx5VWZA"
      },
      "source": [
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJKZarBFVWZB"
      },
      "source": [
        "number_of_clients = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV6DREbPVWZB"
      },
      "source": [
        "federated_model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
        "# federated_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64fUsJCDVWZB"
      },
      "source": [
        "client_models = [torchvision.models.resnet18(pretrained=False, num_classes=1) for _ in range(number_of_clients)]\n",
        "client_training_loader = [torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2) for _ in range(number_of_clients)]\n",
        "\n",
        "federated_test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDAjHtuyVWZB"
      },
      "source": [
        "def update_client(federated_model, client):\n",
        "    client.load_state_dict(federated_model.state_dict(), True)\n",
        "    return client\n",
        "    \n",
        "def federated_average(federated_model, client_models):\n",
        "    average_weights = OrderedDict()\n",
        "\n",
        "    for client_model in client_models:\n",
        "        for key, value in client_model.state_dict().items():\n",
        "            if key in average_weights:\n",
        "                average_weights[key] += (1./number_of_clients) * value.clone()\n",
        "            else:\n",
        "                average_weights[key] = (1./number_of_clients) * value.clone()\n",
        "                \n",
        "                \n",
        "    federated_model.load_state_dict(average_weights, True)\n",
        "    return federated_model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNNFC2o-VWZC"
      },
      "source": [
        "def train_federated(model, data_loader, optimizer, loss):\n",
        "    \"\"\"\n",
        "    model -- neural net\n",
        "    data_loader -- dataloader for train images\n",
        "    optimizer -- optimizer\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    accuracy = 0\n",
        "    for step, [images, labels] in enumerate(data_loader, 1):\n",
        "        images = images.to(device)\n",
        "        labels = label_preparation_tensor(labels.to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        result = model(images)\n",
        "        targets = labels.unsqueeze(1).float()\n",
        "\n",
        "        loss_value = loss(result.float(), targets)\n",
        "\n",
        "        # backpropagation\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "                                    \n",
        "        if step % 10 == 0:\n",
        "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
        "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ_ek0ayVWZC"
      },
      "source": [
        "def test_federated(model, test_loader, loss):\n",
        "    \"\"\"    \n",
        "    model -- neural net \n",
        "    test_loader -- dataloader of test images\n",
        "    epoch -- current epoch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        loss_value = 0\n",
        "        accuracy = 0\n",
        "        for step, [images, labels] in enumerate(test_loader, 1):\n",
        "            images = images.to(device)\n",
        "            labels = label_preparation_tensor(labels.to(device))\n",
        "\n",
        "            result = model(images)\n",
        "            targets = labels.detach().unsqueeze(1).float()\n",
        "\n",
        "            loss_value += loss(result.detach(), targets)\n",
        "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
        "\n",
        "        loss_value /= step\n",
        "        accuracy /=  step\n",
        "        \n",
        "        if device.type == \"cuda\": \n",
        "            torch.cuda.empty_cache()\n",
        "      \n",
        "    model.to(\"cpu\")\n",
        "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR2YMmTLs49U"
      },
      "source": [
        "def run_federated_training(federated_model, client_models, client_training_loader):\n",
        "    # use pos weights because of unbalanced data set\n",
        "    federated_loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
        "    # federated_loss = torch.nn.CrossEntropyLoss(weight=torch.tensor([1./10])).to(device) # sparce categorical crossentropy (federated)\n",
        "\n",
        "    # start training\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        for client_idx in range (number_of_clients):\n",
        "            print(f\"+++ FEDERATED MODEL {client_idx}, EPOCH: {epoch+1} +++++++++\")\n",
        "\n",
        "            client_model = client_models[client_idx]\n",
        "            client_model.to(device)\n",
        "            client_model = update_client(federated_model, client_model)\n",
        "            client_optimizer = torch.optim.Adam(client_model.parameters())\n",
        "\n",
        "            train_federated(client_model, client_training_loader[client_idx], client_optimizer, federated_loss)\n",
        "\n",
        "            if device.type == \"cuda\": \n",
        "                torch.cuda.empty_cache()\n",
        "            \n",
        "            client_model.to(\"cpu\")\n",
        "            \n",
        "            # save interim weights\n",
        "            #torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
        "\n",
        "        federated_model = federated_average(federated_model, client_models)\n",
        "        \n",
        "        if test_federated(federated_model, federated_test_loader, federated_loss) > 97 and epoch > 4:\n",
        "          print(\"Early return: SUCCESS\")\n",
        "          break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLC89DQ2rKQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23759d6c-72f8-4fca-dbfc-4b0b512d8ba6"
      },
      "source": [
        "run_federated_training(federated_model, client_models, client_training_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+++ FEDERATED MODEL 0, EPOCH: 1 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0018761807586997747, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.06912363320589066, rolling accuracy: 87.5\n",
            "TRAINING - Step: 30, loss: 0.04626934230327606, rolling accuracy: 87.50000762939453\n",
            "TRAINING - Step: 40, loss: 0.001791737275198102, rolling accuracy: 90.625\n",
            "TRAINING - Step: 50, loss: 0.0636000856757164, rolling accuracy: 90.0\n",
            "TRAINING - Step: 60, loss: 0.06891538947820663, rolling accuracy: 89.58333587646484\n",
            "TRAINING - Step: 70, loss: 0.015322543680667877, rolling accuracy: 89.28571319580078\n",
            "TRAINING - Step: 80, loss: 0.04638412222266197, rolling accuracy: 87.5\n",
            "TRAINING - Step: 90, loss: 0.02101287990808487, rolling accuracy: 87.5\n",
            "TRAINING - Step: 100, loss: 0.06098581850528717, rolling accuracy: 85.0\n",
            "TRAINING - Step: 110, loss: 0.004822897724807262, rolling accuracy: 86.36363220214844\n",
            "TRAINING - Step: 120, loss: 0.06164140999317169, rolling accuracy: 85.41667175292969\n",
            "TRAINING - Step: 130, loss: 0.02038118988275528, rolling accuracy: 85.57691955566406\n",
            "TRAINING - Step: 140, loss: 0.00334617355838418, rolling accuracy: 86.60713958740234\n",
            "TRAINING - Step: 150, loss: 0.009301427751779556, rolling accuracy: 87.5\n",
            "TRAINING - Step: 160, loss: 0.0017810501158237457, rolling accuracy: 88.28125\n",
            "TRAINING - Step: 170, loss: 0.059160906821489334, rolling accuracy: 87.5\n",
            "TRAINING - Step: 180, loss: 0.0754932165145874, rolling accuracy: 88.19445037841797\n",
            "TRAINING - Step: 190, loss: 0.007732798345386982, rolling accuracy: 88.8157958984375\n",
            "TRAINING - Step: 200, loss: 0.02421938255429268, rolling accuracy: 88.75\n",
            "TRAINING - Step: 210, loss: 0.11857026815414429, rolling accuracy: 88.0952377319336\n",
            "TRAINING - Step: 220, loss: 0.009602890349924564, rolling accuracy: 88.63636016845703\n",
            "TRAINING - Step: 230, loss: 0.004061822779476643, rolling accuracy: 89.13043212890625\n",
            "TRAINING - Step: 240, loss: 0.17020028829574585, rolling accuracy: 89.06250762939453\n",
            "TRAINING - Step: 250, loss: 0.03304622322320938, rolling accuracy: 89.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.04668515920639038, rolling accuracy: 88.94230651855469\n",
            "TRAINING - Step: 270, loss: 0.10866160690784454, rolling accuracy: 88.88888549804688\n",
            "TRAINING - Step: 280, loss: 0.033972952514886856, rolling accuracy: 88.83928680419922\n",
            "TRAINING - Step: 290, loss: 0.0010297712869942188, rolling accuracy: 89.22413635253906\n",
            "TRAINING - Step: 300, loss: 0.013995369896292686, rolling accuracy: 89.16667175292969\n",
            "TRAINING - Step: 310, loss: 0.011368341743946075, rolling accuracy: 89.51612854003906\n",
            "TRAINING - Step: 320, loss: 0.0031896859873086214, rolling accuracy: 89.84375\n",
            "TRAINING - Step: 330, loss: 0.011579346843063831, rolling accuracy: 89.7727279663086\n",
            "TRAINING - Step: 340, loss: 0.0013455790467560291, rolling accuracy: 90.07353210449219\n",
            "TRAINING - Step: 350, loss: 0.0001923461095429957, rolling accuracy: 90.35713958740234\n",
            "TRAINING - Step: 360, loss: 0.05800911411643028, rolling accuracy: 90.27777862548828\n",
            "TRAINING - Step: 370, loss: 0.010703247040510178, rolling accuracy: 90.54054260253906\n",
            "TRAINING - Step: 380, loss: 0.07393063604831696, rolling accuracy: 89.47368621826172\n",
            "TRAINING - Step: 390, loss: 0.02704518660902977, rolling accuracy: 89.42308044433594\n",
            "TRAINING - Step: 400, loss: 0.01942070946097374, rolling accuracy: 89.375\n",
            "TRAINING - Step: 410, loss: 0.004485118202865124, rolling accuracy: 89.63414764404297\n",
            "TRAINING - Step: 420, loss: 0.01923437975347042, rolling accuracy: 89.8809585571289\n",
            "TRAINING - Step: 430, loss: 0.007100781425833702, rolling accuracy: 90.11627960205078\n",
            "TRAINING - Step: 440, loss: 0.050724610686302185, rolling accuracy: 89.7727279663086\n",
            "TRAINING - Step: 450, loss: 0.011471804231405258, rolling accuracy: 90.0\n",
            "TRAINING - Step: 460, loss: 0.004335122648626566, rolling accuracy: 90.2173843383789\n",
            "TRAINING - Step: 470, loss: 0.004503399599343538, rolling accuracy: 90.42552947998047\n",
            "TRAINING - Step: 480, loss: 0.006987432483583689, rolling accuracy: 90.62500762939453\n",
            "TRAINING - Step: 490, loss: 0.010432672686874866, rolling accuracy: 90.81632232666016\n",
            "TRAINING - Step: 500, loss: 0.011748882941901684, rolling accuracy: 90.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.005455656908452511, rolling accuracy: 90.93138122558594\n",
            "TRAINING - Step: 520, loss: 0.0017168568447232246, rolling accuracy: 91.10576629638672\n",
            "TRAINING - Step: 530, loss: 0.018995866179466248, rolling accuracy: 91.2735824584961\n",
            "TRAINING - Step: 540, loss: 0.007189532741904259, rolling accuracy: 91.4351806640625\n",
            "TRAINING - Step: 550, loss: 0.015956370159983635, rolling accuracy: 91.59091186523438\n",
            "TRAINING - Step: 560, loss: 0.004896628670394421, rolling accuracy: 91.74107360839844\n",
            "TRAINING - Step: 570, loss: 0.026913756504654884, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 580, loss: 0.0073447925969958305, rolling accuracy: 91.81034088134766\n",
            "TRAINING - Step: 590, loss: 0.031814102083444595, rolling accuracy: 91.73728942871094\n",
            "TRAINING - Step: 600, loss: 0.03589557111263275, rolling accuracy: 91.45833587646484\n",
            "TRAINING - Step: 610, loss: 0.0333126075565815, rolling accuracy: 91.39344024658203\n",
            "TRAINING - Step: 620, loss: 0.06483232229948044, rolling accuracy: 91.33064270019531\n",
            "TRAINING - Step: 630, loss: 0.002621015068143606, rolling accuracy: 91.46825408935547\n",
            "TRAINING - Step: 640, loss: 0.002308294642716646, rolling accuracy: 91.6015625\n",
            "+++ FEDERATED MODEL 1, EPOCH: 1 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.006719961296766996, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.02302750200033188, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.18056146800518036, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 40, loss: 0.005767316557466984, rolling accuracy: 93.75\n",
            "TRAINING - Step: 50, loss: 0.044896069914102554, rolling accuracy: 92.5\n",
            "TRAINING - Step: 60, loss: 0.023895272985100746, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.0581328347325325, rolling accuracy: 91.07142639160156\n",
            "TRAINING - Step: 80, loss: 0.01008545234799385, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.3008669316768646, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 100, loss: 0.012474628165364265, rolling accuracy: 92.5\n",
            "TRAINING - Step: 110, loss: 0.0326521135866642, rolling accuracy: 92.04544830322266\n",
            "TRAINING - Step: 120, loss: 0.01916585862636566, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 130, loss: 0.003240501508116722, rolling accuracy: 92.30769348144531\n",
            "TRAINING - Step: 140, loss: 0.04342258349061012, rolling accuracy: 91.96428680419922\n",
            "TRAINING - Step: 150, loss: 0.007835501804947853, rolling accuracy: 92.5\n",
            "TRAINING - Step: 160, loss: 0.02901129424571991, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 170, loss: 0.004005254711955786, rolling accuracy: 92.64706420898438\n",
            "TRAINING - Step: 180, loss: 0.04571753367781639, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 190, loss: 0.019178595393896103, rolling accuracy: 92.10527038574219\n",
            "TRAINING - Step: 200, loss: 0.010086352936923504, rolling accuracy: 92.5\n",
            "TRAINING - Step: 210, loss: 0.0010029756231233478, rolling accuracy: 92.85714721679688\n",
            "TRAINING - Step: 220, loss: 0.006267393007874489, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 230, loss: 0.0650377869606018, rolling accuracy: 92.93477630615234\n",
            "TRAINING - Step: 240, loss: 0.0035590953193604946, rolling accuracy: 93.22917175292969\n",
            "TRAINING - Step: 250, loss: 0.06843681633472443, rolling accuracy: 93.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.0752241313457489, rolling accuracy: 92.78845977783203\n",
            "TRAINING - Step: 270, loss: 0.005729627795517445, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 280, loss: 0.003561025485396385, rolling accuracy: 93.30357360839844\n",
            "TRAINING - Step: 290, loss: 0.05690686032176018, rolling accuracy: 92.67241668701172\n",
            "TRAINING - Step: 300, loss: 0.0023776337038725615, rolling accuracy: 92.91667175292969\n",
            "TRAINING - Step: 310, loss: 0.01152054313570261, rolling accuracy: 93.14515686035156\n",
            "TRAINING - Step: 320, loss: 0.022928260266780853, rolling accuracy: 92.96875\n",
            "TRAINING - Step: 330, loss: 0.0036195165012031794, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 340, loss: 0.019704192876815796, rolling accuracy: 93.38235473632812\n",
            "TRAINING - Step: 350, loss: 0.001546157174743712, rolling accuracy: 93.57142639160156\n",
            "TRAINING - Step: 360, loss: 0.009921328164637089, rolling accuracy: 93.75\n",
            "TRAINING - Step: 370, loss: 0.014969526790082455, rolling accuracy: 93.58108520507812\n",
            "TRAINING - Step: 380, loss: 0.0559610016644001, rolling accuracy: 93.42105865478516\n",
            "TRAINING - Step: 390, loss: 0.002550933277234435, rolling accuracy: 93.5897445678711\n",
            "TRAINING - Step: 400, loss: 0.004856621380895376, rolling accuracy: 93.75\n",
            "TRAINING - Step: 410, loss: 0.012625433504581451, rolling accuracy: 93.90243530273438\n",
            "TRAINING - Step: 420, loss: 0.020081739872694016, rolling accuracy: 93.75\n",
            "TRAINING - Step: 430, loss: 0.002180487383157015, rolling accuracy: 93.89534759521484\n",
            "TRAINING - Step: 440, loss: 0.006917929742485285, rolling accuracy: 94.03408813476562\n",
            "TRAINING - Step: 450, loss: 0.004894979298114777, rolling accuracy: 94.16667175292969\n",
            "TRAINING - Step: 460, loss: 0.012034849263727665, rolling accuracy: 94.02173614501953\n",
            "TRAINING - Step: 470, loss: 0.0045179203152656555, rolling accuracy: 94.14893341064453\n",
            "TRAINING - Step: 480, loss: 0.005175454542040825, rolling accuracy: 94.27083587646484\n",
            "TRAINING - Step: 490, loss: 0.11891207098960876, rolling accuracy: 94.13265228271484\n",
            "TRAINING - Step: 500, loss: 0.03652273491024971, rolling accuracy: 94.25000762939453\n",
            "TRAINING - Step: 510, loss: 0.006955993827432394, rolling accuracy: 94.36274719238281\n",
            "TRAINING - Step: 520, loss: 0.22788988053798676, rolling accuracy: 94.23076629638672\n",
            "TRAINING - Step: 530, loss: 0.04692158102989197, rolling accuracy: 94.10377502441406\n",
            "TRAINING - Step: 540, loss: 0.03258277848362923, rolling accuracy: 93.98148345947266\n",
            "TRAINING - Step: 550, loss: 0.001709622098132968, rolling accuracy: 94.09091186523438\n",
            "TRAINING - Step: 560, loss: 0.0066496930085122585, rolling accuracy: 94.19642639160156\n",
            "TRAINING - Step: 570, loss: 0.0025725308805704117, rolling accuracy: 94.29824829101562\n",
            "TRAINING - Step: 580, loss: 0.0013355053961277008, rolling accuracy: 94.39655303955078\n",
            "TRAINING - Step: 590, loss: 0.0013684756122529507, rolling accuracy: 94.49152374267578\n",
            "TRAINING - Step: 600, loss: 0.003915781620889902, rolling accuracy: 94.58333587646484\n",
            "TRAINING - Step: 610, loss: 0.0021944313775748014, rolling accuracy: 94.67213439941406\n",
            "TRAINING - Step: 620, loss: 0.10452161729335785, rolling accuracy: 94.55644989013672\n",
            "TRAINING - Step: 630, loss: 0.003587968647480011, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 640, loss: 0.006497842725366354, rolling accuracy: 94.7265625\n",
            "+++ FEDERATED MODEL 2, EPOCH: 1 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0004723944002762437, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.02627549320459366, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.028909187763929367, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 40, loss: 0.016097018495202065, rolling accuracy: 90.625\n",
            "TRAINING - Step: 50, loss: 0.04928815737366676, rolling accuracy: 92.5\n",
            "TRAINING - Step: 60, loss: 0.007526800036430359, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 70, loss: 0.003540451405569911, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 80, loss: 0.363280326128006, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.015474907122552395, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 100, loss: 0.06576009839773178, rolling accuracy: 92.5\n",
            "TRAINING - Step: 110, loss: 0.024833375588059425, rolling accuracy: 92.04544830322266\n",
            "TRAINING - Step: 120, loss: 0.06775099039077759, rolling accuracy: 90.62500762939453\n",
            "TRAINING - Step: 130, loss: 0.01127947960048914, rolling accuracy: 91.34615325927734\n",
            "TRAINING - Step: 140, loss: 0.0024244110099971294, rolling accuracy: 91.96428680419922\n",
            "TRAINING - Step: 150, loss: 0.004401759710162878, rolling accuracy: 92.5\n",
            "TRAINING - Step: 160, loss: 0.010227873921394348, rolling accuracy: 92.96875\n",
            "TRAINING - Step: 170, loss: 0.06749135255813599, rolling accuracy: 91.9117660522461\n",
            "TRAINING - Step: 180, loss: 0.042800743132829666, rolling accuracy: 90.97222137451172\n",
            "TRAINING - Step: 190, loss: 0.011790001764893532, rolling accuracy: 91.44737243652344\n",
            "TRAINING - Step: 200, loss: 0.01025751605629921, rolling accuracy: 91.875\n",
            "TRAINING - Step: 210, loss: 0.039312079548835754, rolling accuracy: 91.0714340209961\n",
            "TRAINING - Step: 220, loss: 0.01140713319182396, rolling accuracy: 91.4772720336914\n",
            "TRAINING - Step: 230, loss: 0.0030492462683469057, rolling accuracy: 91.84782409667969\n",
            "TRAINING - Step: 240, loss: 0.02594815008342266, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 250, loss: 0.001086971489712596, rolling accuracy: 92.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.007510142866522074, rolling accuracy: 92.30769348144531\n",
            "TRAINING - Step: 270, loss: 0.011218152940273285, rolling accuracy: 92.59259033203125\n",
            "TRAINING - Step: 280, loss: 0.025126483291387558, rolling accuracy: 92.41071319580078\n",
            "TRAINING - Step: 290, loss: 0.01182317454367876, rolling accuracy: 92.24137878417969\n",
            "TRAINING - Step: 300, loss: 0.009663103148341179, rolling accuracy: 92.5\n",
            "TRAINING - Step: 310, loss: 0.015329697169363499, rolling accuracy: 92.74193572998047\n",
            "TRAINING - Step: 320, loss: 0.05653570592403412, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 330, loss: 0.021305080503225327, rolling accuracy: 92.42424011230469\n",
            "TRAINING - Step: 340, loss: 0.2829670310020447, rolling accuracy: 92.27941131591797\n",
            "TRAINING - Step: 350, loss: 0.001592889428138733, rolling accuracy: 92.5\n",
            "TRAINING - Step: 360, loss: 0.024133209139108658, rolling accuracy: 92.36111450195312\n",
            "TRAINING - Step: 370, loss: 0.0316561721265316, rolling accuracy: 92.22972869873047\n",
            "TRAINING - Step: 380, loss: 0.001322382246144116, rolling accuracy: 92.43421173095703\n",
            "TRAINING - Step: 390, loss: 0.055739790201187134, rolling accuracy: 92.62820434570312\n",
            "TRAINING - Step: 400, loss: 0.0724186897277832, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 410, loss: 0.04220744967460632, rolling accuracy: 92.3780517578125\n",
            "TRAINING - Step: 420, loss: 0.008486639708280563, rolling accuracy: 92.55952453613281\n",
            "TRAINING - Step: 430, loss: 0.04065475985407829, rolling accuracy: 92.44185638427734\n",
            "TRAINING - Step: 440, loss: 0.09171177446842194, rolling accuracy: 92.61363220214844\n",
            "TRAINING - Step: 450, loss: 0.04624027758836746, rolling accuracy: 91.94445037841797\n",
            "TRAINING - Step: 460, loss: 0.00288243661634624, rolling accuracy: 92.11956024169922\n",
            "TRAINING - Step: 470, loss: 0.0438549742102623, rolling accuracy: 92.02127075195312\n",
            "TRAINING - Step: 480, loss: 0.03467965126037598, rolling accuracy: 91.92708587646484\n",
            "TRAINING - Step: 490, loss: 0.008775410242378712, rolling accuracy: 92.09183502197266\n",
            "TRAINING - Step: 500, loss: 0.0015446763718500733, rolling accuracy: 92.25000762939453\n",
            "TRAINING - Step: 510, loss: 0.0012448930647224188, rolling accuracy: 92.40196990966797\n",
            "TRAINING - Step: 520, loss: 0.38839247822761536, rolling accuracy: 92.30769348144531\n",
            "TRAINING - Step: 530, loss: 0.01149042509496212, rolling accuracy: 92.45283508300781\n",
            "TRAINING - Step: 540, loss: 0.00423544691875577, rolling accuracy: 92.59259033203125\n",
            "TRAINING - Step: 550, loss: 0.06831981986761093, rolling accuracy: 92.5\n",
            "TRAINING - Step: 560, loss: 0.002964334562420845, rolling accuracy: 92.63392639160156\n",
            "TRAINING - Step: 570, loss: 0.0077062491327524185, rolling accuracy: 92.7631607055664\n",
            "TRAINING - Step: 580, loss: 0.012012682855129242, rolling accuracy: 92.88793182373047\n",
            "TRAINING - Step: 590, loss: 0.006086451001465321, rolling accuracy: 93.00847625732422\n",
            "TRAINING - Step: 600, loss: 0.0030834584031254053, rolling accuracy: 93.125\n",
            "TRAINING - Step: 610, loss: 0.0031148826237767935, rolling accuracy: 93.23770141601562\n",
            "TRAINING - Step: 620, loss: 0.009626351296901703, rolling accuracy: 93.34677124023438\n",
            "TRAINING - Step: 630, loss: 0.013337908312678337, rolling accuracy: 93.25396728515625\n",
            "TRAINING - Step: 640, loss: 0.018085241317749023, rolling accuracy: 93.1640625\n",
            "+++ FEDERATED MODEL 3, EPOCH: 1 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.04706703498959541, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.005120386369526386, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.0005935544613748789, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.040459129959344864, rolling accuracy: 93.75\n",
            "TRAINING - Step: 50, loss: 0.009007376618683338, rolling accuracy: 95.0\n",
            "TRAINING - Step: 60, loss: 0.006666912697255611, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 70, loss: 0.02185596153140068, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 80, loss: 0.014274558052420616, rolling accuracy: 95.3125\n",
            "TRAINING - Step: 90, loss: 0.01289415080100298, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 100, loss: 0.03099052608013153, rolling accuracy: 93.75\n",
            "TRAINING - Step: 110, loss: 0.2445586770772934, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 120, loss: 0.01515334565192461, rolling accuracy: 92.70833587646484\n",
            "TRAINING - Step: 130, loss: 0.0024007027968764305, rolling accuracy: 93.26923370361328\n",
            "TRAINING - Step: 140, loss: 0.011632292531430721, rolling accuracy: 93.75\n",
            "TRAINING - Step: 150, loss: 0.013649214059114456, rolling accuracy: 94.16667175292969\n",
            "TRAINING - Step: 160, loss: 0.022872576490044594, rolling accuracy: 94.53125\n",
            "TRAINING - Step: 170, loss: 0.03228384256362915, rolling accuracy: 94.11764526367188\n",
            "TRAINING - Step: 180, loss: 0.03434505686163902, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 190, loss: 0.2539006769657135, rolling accuracy: 92.7631607055664\n",
            "TRAINING - Step: 200, loss: 0.008359786123037338, rolling accuracy: 93.125\n",
            "TRAINING - Step: 210, loss: 0.3679056465625763, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 220, loss: 0.014919016510248184, rolling accuracy: 92.04544830322266\n",
            "TRAINING - Step: 230, loss: 0.022450193762779236, rolling accuracy: 92.39129638671875\n",
            "TRAINING - Step: 240, loss: 0.045370787382125854, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 250, loss: 0.04996383562684059, rolling accuracy: 91.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.0018906770274043083, rolling accuracy: 91.82691955566406\n",
            "TRAINING - Step: 270, loss: 0.04126085340976715, rolling accuracy: 91.66666412353516\n",
            "TRAINING - Step: 280, loss: 0.031210636720061302, rolling accuracy: 91.51786041259766\n",
            "TRAINING - Step: 290, loss: 0.25743913650512695, rolling accuracy: 90.94827270507812\n",
            "TRAINING - Step: 300, loss: 0.012788212858140469, rolling accuracy: 90.83333587646484\n",
            "TRAINING - Step: 310, loss: 0.0026268933434039354, rolling accuracy: 91.1290283203125\n",
            "TRAINING - Step: 320, loss: 0.039692074060440063, rolling accuracy: 91.015625\n",
            "TRAINING - Step: 330, loss: 0.2259780466556549, rolling accuracy: 90.90908813476562\n",
            "TRAINING - Step: 340, loss: 0.02934967912733555, rolling accuracy: 90.80882263183594\n",
            "TRAINING - Step: 350, loss: 0.002910376526415348, rolling accuracy: 91.07142639160156\n",
            "TRAINING - Step: 360, loss: 0.05966911464929581, rolling accuracy: 90.97222137451172\n",
            "TRAINING - Step: 370, loss: 0.0038989230524748564, rolling accuracy: 91.21621704101562\n",
            "TRAINING - Step: 380, loss: 0.01047732587903738, rolling accuracy: 91.11842346191406\n",
            "TRAINING - Step: 390, loss: 0.01837923377752304, rolling accuracy: 91.02564239501953\n",
            "TRAINING - Step: 400, loss: 0.0012684566900134087, rolling accuracy: 91.25\n",
            "TRAINING - Step: 410, loss: 0.03483947366476059, rolling accuracy: 91.46341705322266\n",
            "TRAINING - Step: 420, loss: 0.0015367022715508938, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 430, loss: 0.024040695279836655, rolling accuracy: 91.56977081298828\n",
            "TRAINING - Step: 440, loss: 0.06333624571561813, rolling accuracy: 91.4772720336914\n",
            "TRAINING - Step: 450, loss: 0.06538090854883194, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 460, loss: 0.04161607474088669, rolling accuracy: 91.57608032226562\n",
            "TRAINING - Step: 470, loss: 0.006686917971819639, rolling accuracy: 91.75531768798828\n",
            "TRAINING - Step: 480, loss: 0.0054173460230231285, rolling accuracy: 91.92708587646484\n",
            "TRAINING - Step: 490, loss: 0.005470576696097851, rolling accuracy: 92.09183502197266\n",
            "TRAINING - Step: 500, loss: 0.005130813457071781, rolling accuracy: 92.25000762939453\n",
            "TRAINING - Step: 510, loss: 0.016217323020100594, rolling accuracy: 92.40196990966797\n",
            "TRAINING - Step: 520, loss: 0.0012821569107472897, rolling accuracy: 92.54808044433594\n",
            "TRAINING - Step: 530, loss: 0.011500325053930283, rolling accuracy: 92.45283508300781\n",
            "TRAINING - Step: 540, loss: 0.008403968065977097, rolling accuracy: 92.59259033203125\n",
            "TRAINING - Step: 550, loss: 0.0006174556910991669, rolling accuracy: 92.7272720336914\n",
            "TRAINING - Step: 560, loss: 0.33677423000335693, rolling accuracy: 92.63392639160156\n",
            "TRAINING - Step: 570, loss: 0.0797095000743866, rolling accuracy: 92.54386138916016\n",
            "TRAINING - Step: 580, loss: 0.07801011949777603, rolling accuracy: 92.24137878417969\n",
            "TRAINING - Step: 590, loss: 0.0025719604454934597, rolling accuracy: 92.37287902832031\n",
            "TRAINING - Step: 600, loss: 0.03516295552253723, rolling accuracy: 92.5\n",
            "TRAINING - Step: 610, loss: 0.00552565511316061, rolling accuracy: 92.6229476928711\n",
            "TRAINING - Step: 620, loss: 0.024335041642189026, rolling accuracy: 92.74193572998047\n",
            "TRAINING - Step: 630, loss: 0.021114708855748177, rolling accuracy: 92.65872955322266\n",
            "TRAINING - Step: 640, loss: 0.0347641184926033, rolling accuracy: 92.3828125\n",
            "TESTING - Loss: 0.024960346519947052, Accuracy: 96.35093688964844\n",
            "+++ FEDERATED MODEL 0, EPOCH: 2 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.010090544819831848, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.031170273199677467, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.09949998557567596, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 40, loss: 0.025730974972248077, rolling accuracy: 87.5\n",
            "TRAINING - Step: 50, loss: 0.0012073968537151814, rolling accuracy: 90.0\n",
            "TRAINING - Step: 60, loss: 0.06379392743110657, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.009080183692276478, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 80, loss: 0.041039254516363144, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.0008640367304906249, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 100, loss: 0.005292953923344612, rolling accuracy: 93.75\n",
            "TRAINING - Step: 110, loss: 0.009953919798135757, rolling accuracy: 94.31817626953125\n",
            "TRAINING - Step: 120, loss: 0.0018655919702723622, rolling accuracy: 94.79167175292969\n",
            "TRAINING - Step: 130, loss: 0.008348623290657997, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 140, loss: 0.031471289694309235, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 150, loss: 0.006816577166318893, rolling accuracy: 95.0\n",
            "TRAINING - Step: 160, loss: 0.04954819008708, rolling accuracy: 92.96875\n",
            "TRAINING - Step: 170, loss: 0.038275156170129776, rolling accuracy: 92.64706420898438\n",
            "TRAINING - Step: 180, loss: 0.01976754143834114, rolling accuracy: 92.36111450195312\n",
            "TRAINING - Step: 190, loss: 0.018973153084516525, rolling accuracy: 92.10527038574219\n",
            "TRAINING - Step: 200, loss: 0.015028642490506172, rolling accuracy: 92.5\n",
            "TRAINING - Step: 210, loss: 0.030280057340860367, rolling accuracy: 92.26190948486328\n",
            "TRAINING - Step: 220, loss: 0.0037204090040177107, rolling accuracy: 92.61363220214844\n",
            "TRAINING - Step: 230, loss: 0.02259501814842224, rolling accuracy: 92.39129638671875\n",
            "TRAINING - Step: 240, loss: 0.036779724061489105, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 250, loss: 0.005789194256067276, rolling accuracy: 92.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.018668439239263535, rolling accuracy: 91.82691955566406\n",
            "TRAINING - Step: 270, loss: 0.0017091105692088604, rolling accuracy: 92.12963104248047\n",
            "TRAINING - Step: 280, loss: 0.07512800395488739, rolling accuracy: 92.41071319580078\n",
            "TRAINING - Step: 290, loss: 0.009949865750968456, rolling accuracy: 92.67241668701172\n",
            "TRAINING - Step: 300, loss: 0.0012586525408551097, rolling accuracy: 92.91667175292969\n",
            "TRAINING - Step: 310, loss: 0.0009481600718572736, rolling accuracy: 93.14515686035156\n",
            "TRAINING - Step: 320, loss: 0.0003319020033814013, rolling accuracy: 93.359375\n",
            "TRAINING - Step: 330, loss: 0.010594218038022518, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 340, loss: 0.0035354949068278074, rolling accuracy: 93.38235473632812\n",
            "TRAINING - Step: 350, loss: 0.027692176401615143, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 360, loss: 0.017993783578276634, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 370, loss: 0.09028437733650208, rolling accuracy: 92.56757354736328\n",
            "TRAINING - Step: 380, loss: 0.008143257349729538, rolling accuracy: 92.7631607055664\n",
            "TRAINING - Step: 390, loss: 0.00953614991158247, rolling accuracy: 92.94872283935547\n",
            "TRAINING - Step: 400, loss: 0.0010362706379964948, rolling accuracy: 93.125\n",
            "TRAINING - Step: 410, loss: 0.005185634829103947, rolling accuracy: 93.29268646240234\n",
            "TRAINING - Step: 420, loss: 0.0015622768551111221, rolling accuracy: 93.45238494873047\n",
            "TRAINING - Step: 430, loss: 0.022474661469459534, rolling accuracy: 93.31394958496094\n",
            "TRAINING - Step: 440, loss: 0.013170668855309486, rolling accuracy: 93.46590423583984\n",
            "TRAINING - Step: 450, loss: 0.010263110511004925, rolling accuracy: 93.61111450195312\n",
            "TRAINING - Step: 460, loss: 0.046990767121315, rolling accuracy: 93.74999237060547\n",
            "TRAINING - Step: 470, loss: 0.006386075634509325, rolling accuracy: 93.88297271728516\n",
            "TRAINING - Step: 480, loss: 0.01738249696791172, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 490, loss: 0.0005157220875844359, rolling accuracy: 93.87754821777344\n",
            "TRAINING - Step: 500, loss: 0.02135542780160904, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.005846896208822727, rolling accuracy: 93.87255096435547\n",
            "TRAINING - Step: 520, loss: 0.0007969801081344485, rolling accuracy: 93.99038696289062\n",
            "TRAINING - Step: 530, loss: 0.0002235806023236364, rolling accuracy: 94.10377502441406\n",
            "TRAINING - Step: 540, loss: 0.0007573622278869152, rolling accuracy: 94.21295928955078\n",
            "TRAINING - Step: 550, loss: 0.05287814512848854, rolling accuracy: 93.86363983154297\n",
            "TRAINING - Step: 560, loss: 0.02049408107995987, rolling accuracy: 93.52678680419922\n",
            "TRAINING - Step: 570, loss: 0.005631001200526953, rolling accuracy: 93.64035034179688\n",
            "TRAINING - Step: 580, loss: 0.0053345574997365475, rolling accuracy: 93.75\n",
            "TRAINING - Step: 590, loss: 0.0025609517470002174, rolling accuracy: 93.8559341430664\n",
            "TRAINING - Step: 600, loss: 0.0406017005443573, rolling accuracy: 93.75\n",
            "TRAINING - Step: 610, loss: 0.02376001887023449, rolling accuracy: 93.64754486083984\n",
            "TRAINING - Step: 620, loss: 0.00444547226652503, rolling accuracy: 93.75\n",
            "TRAINING - Step: 630, loss: 0.01388854905962944, rolling accuracy: 93.65079498291016\n",
            "TRAINING - Step: 640, loss: 0.0036957443226128817, rolling accuracy: 93.75\n",
            "+++ FEDERATED MODEL 1, EPOCH: 2 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.012528744526207447, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.02257910929620266, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.004308309406042099, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.008597539737820625, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.001522046746686101, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.3376239836215973, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 70, loss: 0.005747941788285971, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 80, loss: 0.012156005948781967, rolling accuracy: 96.875\n",
            "TRAINING - Step: 90, loss: 0.006094663869589567, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 100, loss: 0.04760370030999184, rolling accuracy: 97.5\n",
            "TRAINING - Step: 110, loss: 0.05014510825276375, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 120, loss: 0.006904710084199905, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 130, loss: 0.0595337375998497, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 140, loss: 0.025256387889385223, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 150, loss: 0.04356735199689865, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 160, loss: 0.006824810057878494, rolling accuracy: 96.09375\n",
            "TRAINING - Step: 170, loss: 0.008001051843166351, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 180, loss: 0.002991167828440666, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 190, loss: 0.010365521535277367, rolling accuracy: 96.0526351928711\n",
            "TRAINING - Step: 200, loss: 0.0025931266136467457, rolling accuracy: 96.25\n",
            "TRAINING - Step: 210, loss: 0.07371357828378677, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 220, loss: 0.0037336673121899366, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 230, loss: 0.045660022646188736, rolling accuracy: 96.73912811279297\n",
            "TRAINING - Step: 240, loss: 0.0006746174767613411, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 250, loss: 0.009859787300229073, rolling accuracy: 96.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.03612303361296654, rolling accuracy: 96.15384674072266\n",
            "TRAINING - Step: 270, loss: 0.025924459099769592, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 280, loss: 0.07771003991365433, rolling accuracy: 95.08928680419922\n",
            "TRAINING - Step: 290, loss: 0.002691021654754877, rolling accuracy: 95.25862121582031\n",
            "TRAINING - Step: 300, loss: 0.00027629861142486334, rolling accuracy: 95.41667175292969\n",
            "TRAINING - Step: 310, loss: 0.0019807666540145874, rolling accuracy: 95.56451416015625\n",
            "TRAINING - Step: 320, loss: 0.0427156426012516, rolling accuracy: 94.921875\n",
            "TRAINING - Step: 330, loss: 0.041941557079553604, rolling accuracy: 94.31817626953125\n",
            "TRAINING - Step: 340, loss: 0.004758207127451897, rolling accuracy: 94.48529815673828\n",
            "TRAINING - Step: 350, loss: 0.0009180918568745255, rolling accuracy: 94.64285278320312\n",
            "TRAINING - Step: 360, loss: 0.0019698538817465305, rolling accuracy: 94.79167175292969\n",
            "TRAINING - Step: 370, loss: 0.0118277408182621, rolling accuracy: 94.59459686279297\n",
            "TRAINING - Step: 380, loss: 0.001344949472695589, rolling accuracy: 94.73684692382812\n",
            "TRAINING - Step: 390, loss: 0.0015753870829939842, rolling accuracy: 94.87179565429688\n",
            "TRAINING - Step: 400, loss: 0.008569685742259026, rolling accuracy: 95.0\n",
            "TRAINING - Step: 410, loss: 0.04032105952501297, rolling accuracy: 94.81707000732422\n",
            "TRAINING - Step: 420, loss: 0.0009782444685697556, rolling accuracy: 94.94048309326172\n",
            "TRAINING - Step: 430, loss: 0.018965987488627434, rolling accuracy: 94.76744079589844\n",
            "TRAINING - Step: 440, loss: 0.00554046081379056, rolling accuracy: 94.88636016845703\n",
            "TRAINING - Step: 450, loss: 0.01659773848950863, rolling accuracy: 94.72222137451172\n",
            "TRAINING - Step: 460, loss: 0.006655932869762182, rolling accuracy: 94.83695220947266\n",
            "TRAINING - Step: 470, loss: 0.004709063097834587, rolling accuracy: 94.94680786132812\n",
            "TRAINING - Step: 480, loss: 0.0030874533113092184, rolling accuracy: 95.05208587646484\n",
            "TRAINING - Step: 490, loss: 0.01294822245836258, rolling accuracy: 94.89795684814453\n",
            "TRAINING - Step: 500, loss: 0.0032333387061953545, rolling accuracy: 95.00000762939453\n",
            "TRAINING - Step: 510, loss: 0.2792510986328125, rolling accuracy: 94.60784912109375\n",
            "TRAINING - Step: 520, loss: 0.0141970319673419, rolling accuracy: 94.47115325927734\n",
            "TRAINING - Step: 530, loss: 0.012083228677511215, rolling accuracy: 94.57546997070312\n",
            "TRAINING - Step: 540, loss: 0.01738714426755905, rolling accuracy: 94.44444274902344\n",
            "TRAINING - Step: 550, loss: 0.5215672254562378, rolling accuracy: 94.31818389892578\n",
            "TRAINING - Step: 560, loss: 0.013650355860590935, rolling accuracy: 94.41963958740234\n",
            "TRAINING - Step: 570, loss: 0.009516056627035141, rolling accuracy: 94.51754760742188\n",
            "TRAINING - Step: 580, loss: 0.034318458288908005, rolling accuracy: 94.39655303955078\n",
            "TRAINING - Step: 590, loss: 0.01497381366789341, rolling accuracy: 94.2796630859375\n",
            "TRAINING - Step: 600, loss: 0.003456993494182825, rolling accuracy: 94.375\n",
            "TRAINING - Step: 610, loss: 0.0018839625408872962, rolling accuracy: 94.46721649169922\n",
            "TRAINING - Step: 620, loss: 0.038191333413124084, rolling accuracy: 94.3548355102539\n",
            "TRAINING - Step: 630, loss: 0.0023914147168397903, rolling accuracy: 94.44444274902344\n",
            "TRAINING - Step: 640, loss: 0.003698610933497548, rolling accuracy: 94.53125\n",
            "+++ FEDERATED MODEL 2, EPOCH: 2 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.00020522746490314603, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.051703356206417084, rolling accuracy: 87.5\n",
            "TRAINING - Step: 30, loss: 0.03264808654785156, rolling accuracy: 87.50000762939453\n",
            "TRAINING - Step: 40, loss: 0.019320975989103317, rolling accuracy: 87.5\n",
            "TRAINING - Step: 50, loss: 0.01301622949540615, rolling accuracy: 90.0\n",
            "TRAINING - Step: 60, loss: 0.008318128995597363, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.003047569654881954, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 80, loss: 0.016466673463582993, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.07845014333724976, rolling accuracy: 88.8888931274414\n",
            "TRAINING - Step: 100, loss: 0.0037587040569633245, rolling accuracy: 90.0\n",
            "TRAINING - Step: 110, loss: 0.17861609160900116, rolling accuracy: 89.7727279663086\n",
            "TRAINING - Step: 120, loss: 0.06429297477006912, rolling accuracy: 89.58333587646484\n",
            "TRAINING - Step: 130, loss: 0.0010892520658671856, rolling accuracy: 90.38461303710938\n",
            "TRAINING - Step: 140, loss: 0.03893071413040161, rolling accuracy: 90.17857360839844\n",
            "TRAINING - Step: 150, loss: 0.01909271627664566, rolling accuracy: 90.83333587646484\n",
            "TRAINING - Step: 160, loss: 0.015011169016361237, rolling accuracy: 90.625\n",
            "TRAINING - Step: 170, loss: 0.05040856450796127, rolling accuracy: 90.44117736816406\n",
            "TRAINING - Step: 180, loss: 0.011702198535203934, rolling accuracy: 90.97222137451172\n",
            "TRAINING - Step: 190, loss: 0.026645509526133537, rolling accuracy: 90.78947448730469\n",
            "TRAINING - Step: 200, loss: 0.02012339048087597, rolling accuracy: 90.625\n",
            "TRAINING - Step: 210, loss: 0.0505685918033123, rolling accuracy: 90.4761962890625\n",
            "TRAINING - Step: 220, loss: 0.005612886510789394, rolling accuracy: 90.90908813476562\n",
            "TRAINING - Step: 230, loss: 0.0075556933879852295, rolling accuracy: 91.3043441772461\n",
            "TRAINING - Step: 240, loss: 0.006182300858199596, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 250, loss: 0.002501800423488021, rolling accuracy: 92.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.028105102479457855, rolling accuracy: 91.82691955566406\n",
            "TRAINING - Step: 270, loss: 0.0075540002435445786, rolling accuracy: 92.12963104248047\n",
            "TRAINING - Step: 280, loss: 0.05109088495373726, rolling accuracy: 91.51786041259766\n",
            "TRAINING - Step: 290, loss: 0.009599337354302406, rolling accuracy: 91.81034088134766\n",
            "TRAINING - Step: 300, loss: 0.0009779686806723475, rolling accuracy: 92.08333587646484\n",
            "TRAINING - Step: 310, loss: 0.010707405395805836, rolling accuracy: 92.33870697021484\n",
            "TRAINING - Step: 320, loss: 0.011985084973275661, rolling accuracy: 92.578125\n",
            "TRAINING - Step: 330, loss: 0.007842831313610077, rolling accuracy: 92.80302429199219\n",
            "TRAINING - Step: 340, loss: 0.010228633880615234, rolling accuracy: 92.64706420898438\n",
            "TRAINING - Step: 350, loss: 0.1114339604973793, rolling accuracy: 92.5\n",
            "TRAINING - Step: 360, loss: 0.0008560357382521033, rolling accuracy: 92.70833587646484\n",
            "TRAINING - Step: 370, loss: 0.0044893529266119, rolling accuracy: 92.90541076660156\n",
            "TRAINING - Step: 380, loss: 0.032058242708444595, rolling accuracy: 92.7631607055664\n",
            "TRAINING - Step: 390, loss: 0.03784472122788429, rolling accuracy: 92.94872283935547\n",
            "TRAINING - Step: 400, loss: 0.004891994409263134, rolling accuracy: 93.125\n",
            "TRAINING - Step: 410, loss: 0.015576008707284927, rolling accuracy: 93.29268646240234\n",
            "TRAINING - Step: 420, loss: 0.025803280994296074, rolling accuracy: 93.1547622680664\n",
            "TRAINING - Step: 430, loss: 0.10246970504522324, rolling accuracy: 93.02325439453125\n",
            "TRAINING - Step: 440, loss: 0.028457222506403923, rolling accuracy: 92.8977279663086\n",
            "TRAINING - Step: 450, loss: 0.006306344643235207, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 460, loss: 0.021996676921844482, rolling accuracy: 93.2065200805664\n",
            "TRAINING - Step: 470, loss: 0.0009224629029631615, rolling accuracy: 93.35105895996094\n",
            "TRAINING - Step: 480, loss: 0.02532237581908703, rolling accuracy: 93.22917175292969\n",
            "TRAINING - Step: 490, loss: 0.002970281522721052, rolling accuracy: 93.36734008789062\n",
            "TRAINING - Step: 500, loss: 0.0008594963583163917, rolling accuracy: 93.50000762939453\n",
            "TRAINING - Step: 510, loss: 0.025236520916223526, rolling accuracy: 93.38235473632812\n",
            "TRAINING - Step: 520, loss: 0.01691834069788456, rolling accuracy: 93.26923370361328\n",
            "TRAINING - Step: 530, loss: 0.007720239460468292, rolling accuracy: 93.39622497558594\n",
            "TRAINING - Step: 540, loss: 0.003323764307424426, rolling accuracy: 93.51851654052734\n",
            "TRAINING - Step: 550, loss: 0.0008023615810088813, rolling accuracy: 93.63636779785156\n",
            "TRAINING - Step: 560, loss: 0.008920934051275253, rolling accuracy: 93.75\n",
            "TRAINING - Step: 570, loss: 0.02673107013106346, rolling accuracy: 93.85964965820312\n",
            "TRAINING - Step: 580, loss: 0.006455878261476755, rolling accuracy: 93.96551513671875\n",
            "TRAINING - Step: 590, loss: 0.15552634000778198, rolling accuracy: 93.8559341430664\n",
            "TRAINING - Step: 600, loss: 0.026305455714464188, rolling accuracy: 93.75\n",
            "TRAINING - Step: 610, loss: 0.0013290237402543426, rolling accuracy: 93.85246276855469\n",
            "TRAINING - Step: 620, loss: 0.004013475961983204, rolling accuracy: 93.95161437988281\n",
            "TRAINING - Step: 630, loss: 0.01569501683115959, rolling accuracy: 94.04762268066406\n",
            "TRAINING - Step: 640, loss: 0.040334492921829224, rolling accuracy: 93.9453125\n",
            "+++ FEDERATED MODEL 3, EPOCH: 2 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.003455499419942498, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.009947892278432846, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.0009655125322751701, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.0034208085853606462, rolling accuracy: 100.0\n",
            "TRAINING - Step: 50, loss: 0.011295038275420666, rolling accuracy: 100.0\n",
            "TRAINING - Step: 60, loss: 0.017072070389986038, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 70, loss: 0.017086561769247055, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 0.062317900359630585, rolling accuracy: 93.75\n",
            "TRAINING - Step: 90, loss: 0.005649327300488949, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 100, loss: 0.002060990547761321, rolling accuracy: 95.0\n",
            "TRAINING - Step: 110, loss: 0.0018490204820409417, rolling accuracy: 95.45454406738281\n",
            "TRAINING - Step: 120, loss: 0.00853307917714119, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 130, loss: 0.03814054653048515, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 140, loss: 0.028086597099900246, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 150, loss: 0.0004356599529273808, rolling accuracy: 95.0\n",
            "TRAINING - Step: 160, loss: 0.0050996472127735615, rolling accuracy: 95.3125\n",
            "TRAINING - Step: 170, loss: 0.001134358812123537, rolling accuracy: 95.5882339477539\n",
            "TRAINING - Step: 180, loss: 0.0008301232010126114, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 190, loss: 0.0023155196104198694, rolling accuracy: 96.0526351928711\n",
            "TRAINING - Step: 200, loss: 0.04955074563622475, rolling accuracy: 95.625\n",
            "TRAINING - Step: 210, loss: 0.03949544578790665, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 220, loss: 0.002510552294552326, rolling accuracy: 95.45454406738281\n",
            "TRAINING - Step: 230, loss: 0.006635420024394989, rolling accuracy: 95.65216827392578\n",
            "TRAINING - Step: 240, loss: 0.003653136547654867, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 250, loss: 0.001598516944795847, rolling accuracy: 96.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.026035262271761894, rolling accuracy: 96.15384674072266\n",
            "TRAINING - Step: 270, loss: 0.011769881471991539, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 280, loss: 0.0020768484100699425, rolling accuracy: 95.98213958740234\n",
            "TRAINING - Step: 290, loss: 0.0015793460188433528, rolling accuracy: 96.12068939208984\n",
            "TRAINING - Step: 300, loss: 0.008200728334486485, rolling accuracy: 96.25\n",
            "TRAINING - Step: 310, loss: 0.012518164701759815, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 320, loss: 0.005967609118670225, rolling accuracy: 96.484375\n",
            "TRAINING - Step: 330, loss: 0.012766650877892971, rolling accuracy: 96.21212005615234\n",
            "TRAINING - Step: 340, loss: 0.06569362431764603, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 350, loss: 0.004345787689089775, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 360, loss: 0.006484683603048325, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 370, loss: 0.06604466587305069, rolling accuracy: 96.62162017822266\n",
            "TRAINING - Step: 380, loss: 0.04634943604469299, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 390, loss: 0.03308388590812683, rolling accuracy: 96.79487609863281\n",
            "TRAINING - Step: 400, loss: 0.028706245124340057, rolling accuracy: 96.5625\n",
            "TRAINING - Step: 410, loss: 0.06329134106636047, rolling accuracy: 96.6463394165039\n",
            "TRAINING - Step: 420, loss: 0.018834291025996208, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 430, loss: 0.0005323896766640246, rolling accuracy: 96.51162719726562\n",
            "TRAINING - Step: 440, loss: 0.0025635026395320892, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 450, loss: 0.004290973301976919, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 460, loss: 0.0014882625546306372, rolling accuracy: 96.73912811279297\n",
            "TRAINING - Step: 470, loss: 0.06548994034528732, rolling accuracy: 96.80850982666016\n",
            "TRAINING - Step: 480, loss: 0.00856739655137062, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 490, loss: 0.0009096999419853091, rolling accuracy: 96.93877410888672\n",
            "TRAINING - Step: 500, loss: 0.004950390662997961, rolling accuracy: 97.00000762939453\n",
            "TRAINING - Step: 510, loss: 0.0014033259358257055, rolling accuracy: 97.05883026123047\n",
            "TRAINING - Step: 520, loss: 0.39631831645965576, rolling accuracy: 96.875\n",
            "TRAINING - Step: 530, loss: 0.0049579208716750145, rolling accuracy: 96.9339599609375\n",
            "TRAINING - Step: 540, loss: 0.010849378071725368, rolling accuracy: 96.99073791503906\n",
            "TRAINING - Step: 550, loss: 0.017669163644313812, rolling accuracy: 96.81818389892578\n",
            "TRAINING - Step: 560, loss: 0.2607874870300293, rolling accuracy: 96.65178680419922\n",
            "TRAINING - Step: 570, loss: 0.0027236463502049446, rolling accuracy: 96.71052551269531\n",
            "TRAINING - Step: 580, loss: 0.012485247105360031, rolling accuracy: 96.76724243164062\n",
            "TRAINING - Step: 590, loss: 0.0005098535912111402, rolling accuracy: 96.82202911376953\n",
            "TRAINING - Step: 600, loss: 0.014592115767300129, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 610, loss: 0.0031072774436324835, rolling accuracy: 96.7213134765625\n",
            "TRAINING - Step: 620, loss: 0.004380395170301199, rolling accuracy: 96.7741928100586\n",
            "TRAINING - Step: 630, loss: 0.0010509685380384326, rolling accuracy: 96.82540130615234\n",
            "TRAINING - Step: 640, loss: 0.0006216916954144835, rolling accuracy: 96.875\n",
            "TESTING - Loss: 0.012702093459665775, Accuracy: 95.72981262207031\n",
            "+++ FEDERATED MODEL 0, EPOCH: 3 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.008709201589226723, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.0038100602105259895, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.00108802889008075, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.01945423148572445, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.023893728852272034, rolling accuracy: 95.0\n",
            "TRAINING - Step: 60, loss: 0.008475874550640583, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 70, loss: 0.0031952294521033764, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 80, loss: 0.03148738667368889, rolling accuracy: 95.3125\n",
            "TRAINING - Step: 90, loss: 0.021128030493855476, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 100, loss: 0.0034847347997128963, rolling accuracy: 95.0\n",
            "TRAINING - Step: 110, loss: 0.030146736651659012, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 120, loss: 0.004913579672574997, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 130, loss: 0.01719578541815281, rolling accuracy: 93.26923370361328\n",
            "TRAINING - Step: 140, loss: 0.005352621898055077, rolling accuracy: 93.75\n",
            "TRAINING - Step: 150, loss: 0.017877694219350815, rolling accuracy: 93.33333587646484\n",
            "TRAINING - Step: 160, loss: 0.001510661095380783, rolling accuracy: 93.75\n",
            "TRAINING - Step: 170, loss: 0.011581595987081528, rolling accuracy: 93.38235473632812\n",
            "TRAINING - Step: 180, loss: 0.0033144382759928703, rolling accuracy: 93.75\n",
            "TRAINING - Step: 190, loss: 0.008725321851670742, rolling accuracy: 94.07894897460938\n",
            "TRAINING - Step: 200, loss: 0.0023307863157242537, rolling accuracy: 94.375\n",
            "TRAINING - Step: 210, loss: 0.0032525367569178343, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 220, loss: 0.0023580393753945827, rolling accuracy: 94.88636016845703\n",
            "TRAINING - Step: 230, loss: 0.011818325147032738, rolling accuracy: 95.10868835449219\n",
            "TRAINING - Step: 240, loss: 0.01585409790277481, rolling accuracy: 95.31250762939453\n",
            "TRAINING - Step: 250, loss: 0.00011399874347262084, rolling accuracy: 95.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.0018251945730298758, rolling accuracy: 95.67308044433594\n",
            "TRAINING - Step: 270, loss: 0.0050138989463448524, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 280, loss: 0.000439952127635479, rolling accuracy: 95.98213958740234\n",
            "TRAINING - Step: 290, loss: 0.0005688867531716824, rolling accuracy: 96.12068939208984\n",
            "TRAINING - Step: 300, loss: 0.005252944305539131, rolling accuracy: 96.25\n",
            "TRAINING - Step: 310, loss: 0.0002647127548698336, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 320, loss: 0.0006233046296983957, rolling accuracy: 96.484375\n",
            "TRAINING - Step: 330, loss: 0.0007857850869186223, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 340, loss: 0.001108347438275814, rolling accuracy: 96.69117736816406\n",
            "TRAINING - Step: 350, loss: 0.0015018206322565675, rolling accuracy: 96.78571319580078\n",
            "TRAINING - Step: 360, loss: 0.021586909890174866, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 370, loss: 0.013065214268863201, rolling accuracy: 96.28378295898438\n",
            "TRAINING - Step: 380, loss: 0.0031713582575321198, rolling accuracy: 96.38158416748047\n",
            "TRAINING - Step: 390, loss: 0.0746338814496994, rolling accuracy: 96.474365234375\n",
            "TRAINING - Step: 400, loss: 0.003183435183018446, rolling accuracy: 96.5625\n",
            "TRAINING - Step: 410, loss: 0.000606310204602778, rolling accuracy: 96.6463394165039\n",
            "TRAINING - Step: 420, loss: 0.000905107066500932, rolling accuracy: 96.7261962890625\n",
            "TRAINING - Step: 430, loss: 0.004098655190318823, rolling accuracy: 96.80232238769531\n",
            "TRAINING - Step: 440, loss: 0.04883338510990143, rolling accuracy: 96.0227279663086\n",
            "TRAINING - Step: 450, loss: 0.04254221171140671, rolling accuracy: 95.27777862548828\n",
            "TRAINING - Step: 460, loss: 0.0019232637714594603, rolling accuracy: 95.38043212890625\n",
            "TRAINING - Step: 470, loss: 0.0004906952381134033, rolling accuracy: 95.47872161865234\n",
            "TRAINING - Step: 480, loss: 0.0007841185433790088, rolling accuracy: 95.57292175292969\n",
            "TRAINING - Step: 490, loss: 0.0038074476178735495, rolling accuracy: 95.66326141357422\n",
            "TRAINING - Step: 500, loss: 0.02299989014863968, rolling accuracy: 95.50000762939453\n",
            "TRAINING - Step: 510, loss: 0.0010507149854674935, rolling accuracy: 95.58824157714844\n",
            "TRAINING - Step: 520, loss: 0.003087348071858287, rolling accuracy: 95.67308044433594\n",
            "TRAINING - Step: 530, loss: 0.3362656831741333, rolling accuracy: 95.28302001953125\n",
            "TRAINING - Step: 540, loss: 0.010219478979706764, rolling accuracy: 95.37036895751953\n",
            "TRAINING - Step: 550, loss: 0.014069248922169209, rolling accuracy: 95.2272720336914\n",
            "TRAINING - Step: 560, loss: 0.0069464510306715965, rolling accuracy: 95.3125\n",
            "TRAINING - Step: 570, loss: 0.01421020645648241, rolling accuracy: 95.1754379272461\n",
            "TRAINING - Step: 580, loss: 0.0036675073206424713, rolling accuracy: 95.25862121582031\n",
            "TRAINING - Step: 590, loss: 0.007079463917762041, rolling accuracy: 95.33898162841797\n",
            "TRAINING - Step: 600, loss: 0.0037263380363583565, rolling accuracy: 95.41667175292969\n",
            "TRAINING - Step: 610, loss: 0.0025251649785786867, rolling accuracy: 95.49180603027344\n",
            "TRAINING - Step: 620, loss: 0.006674676202237606, rolling accuracy: 95.56451416015625\n",
            "TRAINING - Step: 630, loss: 0.0029050614684820175, rolling accuracy: 95.63491821289062\n",
            "TRAINING - Step: 640, loss: 0.0006059953011572361, rolling accuracy: 95.703125\n",
            "+++ FEDERATED MODEL 1, EPOCH: 3 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.02572040632367134, rolling accuracy: 75.0\n",
            "TRAINING - Step: 20, loss: 0.00045098361442796886, rolling accuracy: 87.5\n",
            "TRAINING - Step: 30, loss: 0.47378507256507874, rolling accuracy: 87.50000762939453\n",
            "TRAINING - Step: 40, loss: 0.00031514494912698865, rolling accuracy: 90.625\n",
            "TRAINING - Step: 50, loss: 0.0035345375072211027, rolling accuracy: 92.5\n",
            "TRAINING - Step: 60, loss: 0.05642535910010338, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.00038599103572778404, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 80, loss: 0.0004891618154942989, rolling accuracy: 93.75\n",
            "TRAINING - Step: 90, loss: 0.04076993837952614, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 100, loss: 0.008352164179086685, rolling accuracy: 93.75\n",
            "TRAINING - Step: 110, loss: 0.01094178669154644, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 120, loss: 0.0025082228239625692, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 130, loss: 0.003897356102243066, rolling accuracy: 94.23076629638672\n",
            "TRAINING - Step: 140, loss: 0.00038527618744410574, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 150, loss: 0.01040291041135788, rolling accuracy: 95.0\n",
            "TRAINING - Step: 160, loss: 0.08152738958597183, rolling accuracy: 94.53125\n",
            "TRAINING - Step: 170, loss: 0.0005026540020480752, rolling accuracy: 94.85294342041016\n",
            "TRAINING - Step: 180, loss: 0.02064855769276619, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 190, loss: 0.06032167002558708, rolling accuracy: 94.07894897460938\n",
            "TRAINING - Step: 200, loss: 0.0006455430411733687, rolling accuracy: 94.375\n",
            "TRAINING - Step: 210, loss: 0.027791103348135948, rolling accuracy: 94.04762268066406\n",
            "TRAINING - Step: 220, loss: 0.004635558929294348, rolling accuracy: 94.31817626953125\n",
            "TRAINING - Step: 230, loss: 0.01860314980149269, rolling accuracy: 94.02173614501953\n",
            "TRAINING - Step: 240, loss: 0.0066932188346982, rolling accuracy: 94.27083587646484\n",
            "TRAINING - Step: 250, loss: 0.0019011707045137882, rolling accuracy: 94.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.003389125457033515, rolling accuracy: 94.71154022216797\n",
            "TRAINING - Step: 270, loss: 0.04277116805315018, rolling accuracy: 94.44444274902344\n",
            "TRAINING - Step: 280, loss: 0.07250141352415085, rolling accuracy: 94.19642639160156\n",
            "TRAINING - Step: 290, loss: 0.001834859955124557, rolling accuracy: 94.39655303955078\n",
            "TRAINING - Step: 300, loss: 0.024615274742245674, rolling accuracy: 94.16667175292969\n",
            "TRAINING - Step: 310, loss: 0.010108753107488155, rolling accuracy: 94.3548355102539\n",
            "TRAINING - Step: 320, loss: 0.027420654892921448, rolling accuracy: 94.53125\n",
            "TRAINING - Step: 330, loss: 0.00256316433660686, rolling accuracy: 94.69696807861328\n",
            "TRAINING - Step: 340, loss: 0.05319264531135559, rolling accuracy: 94.48529815673828\n",
            "TRAINING - Step: 350, loss: 0.0158481877297163, rolling accuracy: 94.28571319580078\n",
            "TRAINING - Step: 360, loss: 0.002753592561930418, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 370, loss: 0.03960108384490013, rolling accuracy: 94.59459686279297\n",
            "TRAINING - Step: 380, loss: 0.010458426550030708, rolling accuracy: 94.73684692382812\n",
            "TRAINING - Step: 390, loss: 0.041623812168836594, rolling accuracy: 94.23077392578125\n",
            "TRAINING - Step: 400, loss: 0.026782073080539703, rolling accuracy: 94.0625\n",
            "TRAINING - Step: 410, loss: 0.004774905275553465, rolling accuracy: 94.20731353759766\n",
            "TRAINING - Step: 420, loss: 0.005167918745428324, rolling accuracy: 94.34524536132812\n",
            "TRAINING - Step: 430, loss: 0.00268375757150352, rolling accuracy: 94.47674560546875\n",
            "TRAINING - Step: 440, loss: 0.006113000679761171, rolling accuracy: 94.6022720336914\n",
            "TRAINING - Step: 450, loss: 0.10291555523872375, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 460, loss: 0.0034781775902956724, rolling accuracy: 94.56521606445312\n",
            "TRAINING - Step: 470, loss: 0.037760473787784576, rolling accuracy: 94.4148941040039\n",
            "TRAINING - Step: 480, loss: 0.0027358909137547016, rolling accuracy: 94.53125762939453\n",
            "TRAINING - Step: 490, loss: 0.44517606496810913, rolling accuracy: 94.38774871826172\n",
            "TRAINING - Step: 500, loss: 0.010300945490598679, rolling accuracy: 94.50000762939453\n",
            "TRAINING - Step: 510, loss: 0.000684407539665699, rolling accuracy: 94.60784912109375\n",
            "TRAINING - Step: 520, loss: 0.0036336288321763277, rolling accuracy: 94.71154022216797\n",
            "TRAINING - Step: 530, loss: 0.0023757913149893284, rolling accuracy: 94.81132507324219\n",
            "TRAINING - Step: 540, loss: 0.0013667020248249173, rolling accuracy: 94.90740203857422\n",
            "TRAINING - Step: 550, loss: 0.005330306477844715, rolling accuracy: 95.0\n",
            "TRAINING - Step: 560, loss: 0.016072385013103485, rolling accuracy: 94.86607360839844\n",
            "TRAINING - Step: 570, loss: 0.006063208449631929, rolling accuracy: 94.95613861083984\n",
            "TRAINING - Step: 580, loss: 0.008018833585083485, rolling accuracy: 95.04310607910156\n",
            "TRAINING - Step: 590, loss: 0.006669501308351755, rolling accuracy: 95.12712097167969\n",
            "TRAINING - Step: 600, loss: 0.03615204989910126, rolling accuracy: 95.0\n",
            "TRAINING - Step: 610, loss: 0.006367672234773636, rolling accuracy: 95.08197021484375\n",
            "TRAINING - Step: 620, loss: 0.0012231185100972652, rolling accuracy: 95.16128540039062\n",
            "TRAINING - Step: 630, loss: 0.010311737656593323, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 640, loss: 0.028672806918621063, rolling accuracy: 95.1171875\n",
            "+++ FEDERATED MODEL 2, EPOCH: 3 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0009010982466861606, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.03429379314184189, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.00814464595168829, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.025105342268943787, rolling accuracy: 93.75\n",
            "TRAINING - Step: 50, loss: 0.035980165004730225, rolling accuracy: 92.5\n",
            "TRAINING - Step: 60, loss: 0.019975150004029274, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.017983464524149895, rolling accuracy: 91.07142639160156\n",
            "TRAINING - Step: 80, loss: 0.0028035794384777546, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.023934785276651382, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 100, loss: 0.05448673292994499, rolling accuracy: 91.25\n",
            "TRAINING - Step: 110, loss: 0.004652101546525955, rolling accuracy: 92.04544830322266\n",
            "TRAINING - Step: 120, loss: 0.023461751639842987, rolling accuracy: 90.62500762939453\n",
            "TRAINING - Step: 130, loss: 0.004015713464468718, rolling accuracy: 91.34615325927734\n",
            "TRAINING - Step: 140, loss: 0.0028416963759809732, rolling accuracy: 91.96428680419922\n",
            "TRAINING - Step: 150, loss: 0.022314289584755898, rolling accuracy: 90.83333587646484\n",
            "TRAINING - Step: 160, loss: 0.00421567540615797, rolling accuracy: 91.40625\n",
            "TRAINING - Step: 170, loss: 0.0007351042004302144, rolling accuracy: 91.9117660522461\n",
            "TRAINING - Step: 180, loss: 0.0015959725715219975, rolling accuracy: 92.36111450195312\n",
            "TRAINING - Step: 190, loss: 0.002206528326496482, rolling accuracy: 92.7631607055664\n",
            "TRAINING - Step: 200, loss: 0.0032744379714131355, rolling accuracy: 93.125\n",
            "TRAINING - Step: 210, loss: 0.01360042579472065, rolling accuracy: 92.85714721679688\n",
            "TRAINING - Step: 220, loss: 0.009130503050982952, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 230, loss: 0.0003957507433369756, rolling accuracy: 93.47825622558594\n",
            "TRAINING - Step: 240, loss: 0.015758635476231575, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 250, loss: 0.0009888054337352514, rolling accuracy: 94.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.0010094576282426715, rolling accuracy: 94.23076629638672\n",
            "TRAINING - Step: 270, loss: 0.0025657066144049168, rolling accuracy: 94.44444274902344\n",
            "TRAINING - Step: 280, loss: 0.04774455726146698, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 290, loss: 0.024706512689590454, rolling accuracy: 94.39655303955078\n",
            "TRAINING - Step: 300, loss: 0.012071994133293629, rolling accuracy: 94.16667175292969\n",
            "TRAINING - Step: 310, loss: 0.0006941095925867558, rolling accuracy: 94.3548355102539\n",
            "TRAINING - Step: 320, loss: 0.0016172805335372686, rolling accuracy: 94.53125\n",
            "TRAINING - Step: 330, loss: 0.20562131702899933, rolling accuracy: 94.31817626953125\n",
            "TRAINING - Step: 340, loss: 0.021392054855823517, rolling accuracy: 94.11764526367188\n",
            "TRAINING - Step: 350, loss: 0.019850024953484535, rolling accuracy: 93.92857360839844\n",
            "TRAINING - Step: 360, loss: 0.015621081925928593, rolling accuracy: 94.09722137451172\n",
            "TRAINING - Step: 370, loss: 0.04787078872323036, rolling accuracy: 93.9189224243164\n",
            "TRAINING - Step: 380, loss: 0.01639111153781414, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 390, loss: 0.0007049732957966626, rolling accuracy: 93.91026306152344\n",
            "TRAINING - Step: 400, loss: 0.006271463818848133, rolling accuracy: 94.0625\n",
            "TRAINING - Step: 410, loss: 0.02560843899846077, rolling accuracy: 93.59756469726562\n",
            "TRAINING - Step: 420, loss: 0.02049908973276615, rolling accuracy: 93.45238494873047\n",
            "TRAINING - Step: 430, loss: 0.03210175782442093, rolling accuracy: 93.02325439453125\n",
            "TRAINING - Step: 440, loss: 0.022302493453025818, rolling accuracy: 92.8977279663086\n",
            "TRAINING - Step: 450, loss: 0.004811977967619896, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 460, loss: 0.033441655337810516, rolling accuracy: 92.93477630615234\n",
            "TRAINING - Step: 470, loss: 0.012679141946136951, rolling accuracy: 92.81914520263672\n",
            "TRAINING - Step: 480, loss: 0.006549785379320383, rolling accuracy: 92.96875762939453\n",
            "TRAINING - Step: 490, loss: 0.0018119090236723423, rolling accuracy: 93.11224365234375\n",
            "TRAINING - Step: 500, loss: 0.24777273833751678, rolling accuracy: 92.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.025302089750766754, rolling accuracy: 92.64706420898438\n",
            "TRAINING - Step: 520, loss: 0.002650258131325245, rolling accuracy: 92.78845977783203\n",
            "TRAINING - Step: 530, loss: 0.06493180990219116, rolling accuracy: 92.68868255615234\n",
            "TRAINING - Step: 540, loss: 0.004363709595054388, rolling accuracy: 92.8240737915039\n",
            "TRAINING - Step: 550, loss: 0.007108920719474554, rolling accuracy: 92.95454406738281\n",
            "TRAINING - Step: 560, loss: 0.005873839370906353, rolling accuracy: 93.08036041259766\n",
            "TRAINING - Step: 570, loss: 0.03264664486050606, rolling accuracy: 93.2017593383789\n",
            "TRAINING - Step: 580, loss: 0.003178695682436228, rolling accuracy: 93.31896209716797\n",
            "TRAINING - Step: 590, loss: 0.036481183022260666, rolling accuracy: 93.2203369140625\n",
            "TRAINING - Step: 600, loss: 0.04572683572769165, rolling accuracy: 92.91667175292969\n",
            "TRAINING - Step: 610, loss: 0.009575381875038147, rolling accuracy: 93.03278350830078\n",
            "TRAINING - Step: 620, loss: 0.010621123015880585, rolling accuracy: 93.14515686035156\n",
            "TRAINING - Step: 630, loss: 0.0069215078838169575, rolling accuracy: 93.25396728515625\n",
            "TRAINING - Step: 640, loss: 0.0070825121365487576, rolling accuracy: 93.359375\n",
            "+++ FEDERATED MODEL 3, EPOCH: 3 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.06285754591226578, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.02081797830760479, rolling accuracy: 87.5\n",
            "TRAINING - Step: 30, loss: 0.0005163606256246567, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 40, loss: 0.17787939310073853, rolling accuracy: 90.625\n",
            "TRAINING - Step: 50, loss: 0.021094704046845436, rolling accuracy: 90.0\n",
            "TRAINING - Step: 60, loss: 0.0010935838799923658, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.05387919396162033, rolling accuracy: 91.07142639160156\n",
            "TRAINING - Step: 80, loss: 0.002832407131791115, rolling accuracy: 92.1875\n",
            "TRAINING - Step: 90, loss: 0.006750439293682575, rolling accuracy: 93.05555725097656\n",
            "TRAINING - Step: 100, loss: 0.032606158405542374, rolling accuracy: 92.5\n",
            "TRAINING - Step: 110, loss: 0.010379190556704998, rolling accuracy: 93.18181610107422\n",
            "TRAINING - Step: 120, loss: 0.0032898196950554848, rolling accuracy: 93.75000762939453\n",
            "TRAINING - Step: 130, loss: 0.01570431888103485, rolling accuracy: 93.26923370361328\n",
            "TRAINING - Step: 140, loss: 0.015583383850753307, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 150, loss: 0.01133840810507536, rolling accuracy: 93.33333587646484\n",
            "TRAINING - Step: 160, loss: 0.0034947486128658056, rolling accuracy: 93.75\n",
            "TRAINING - Step: 170, loss: 0.00299056526273489, rolling accuracy: 94.11764526367188\n",
            "TRAINING - Step: 180, loss: 0.0036634993739426136, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 190, loss: 0.001342601841315627, rolling accuracy: 94.73684692382812\n",
            "TRAINING - Step: 200, loss: 0.0031451943796128035, rolling accuracy: 95.0\n",
            "TRAINING - Step: 210, loss: 0.0014871234307065606, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 220, loss: 0.03534136340022087, rolling accuracy: 94.88636016845703\n",
            "TRAINING - Step: 230, loss: 0.0005037395749241114, rolling accuracy: 95.10868835449219\n",
            "TRAINING - Step: 240, loss: 0.00031630785088054836, rolling accuracy: 95.31250762939453\n",
            "TRAINING - Step: 250, loss: 0.003822385799139738, rolling accuracy: 95.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.0009102419717237353, rolling accuracy: 95.67308044433594\n",
            "TRAINING - Step: 270, loss: 0.0021283146925270557, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 280, loss: 0.006860011722892523, rolling accuracy: 95.98213958740234\n",
            "TRAINING - Step: 290, loss: 0.004612300544977188, rolling accuracy: 96.12068939208984\n",
            "TRAINING - Step: 300, loss: 0.0005179434665478766, rolling accuracy: 96.25\n",
            "TRAINING - Step: 310, loss: 0.016840340569615364, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 320, loss: 0.05818043649196625, rolling accuracy: 95.703125\n",
            "TRAINING - Step: 330, loss: 0.03125162050127983, rolling accuracy: 95.45454406738281\n",
            "TRAINING - Step: 340, loss: 0.010681040585041046, rolling accuracy: 95.5882339477539\n",
            "TRAINING - Step: 350, loss: 0.030428709462285042, rolling accuracy: 95.35713958740234\n",
            "TRAINING - Step: 360, loss: 0.007457421161234379, rolling accuracy: 95.48611450195312\n",
            "TRAINING - Step: 370, loss: 0.004700997844338417, rolling accuracy: 95.60810852050781\n",
            "TRAINING - Step: 380, loss: 0.014393247663974762, rolling accuracy: 95.72368621826172\n",
            "TRAINING - Step: 390, loss: 0.014367735013365746, rolling accuracy: 95.51282501220703\n",
            "TRAINING - Step: 400, loss: 0.000998612609691918, rolling accuracy: 95.625\n",
            "TRAINING - Step: 410, loss: 0.15624341368675232, rolling accuracy: 95.42682647705078\n",
            "TRAINING - Step: 420, loss: 0.05558799207210541, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 430, loss: 0.0030403942801058292, rolling accuracy: 95.34883880615234\n",
            "TRAINING - Step: 440, loss: 0.015445447526872158, rolling accuracy: 95.17044830322266\n",
            "TRAINING - Step: 450, loss: 0.026234101504087448, rolling accuracy: 95.27777862548828\n",
            "TRAINING - Step: 460, loss: 0.08116096258163452, rolling accuracy: 95.38043212890625\n",
            "TRAINING - Step: 470, loss: 0.07544847577810287, rolling accuracy: 94.94680786132812\n",
            "TRAINING - Step: 480, loss: 0.003548210021108389, rolling accuracy: 95.05208587646484\n",
            "TRAINING - Step: 490, loss: 0.0023111419286578894, rolling accuracy: 95.15306091308594\n",
            "TRAINING - Step: 500, loss: 0.002220597118139267, rolling accuracy: 95.25000762939453\n",
            "TRAINING - Step: 510, loss: 0.0009512222604826093, rolling accuracy: 95.3431396484375\n",
            "TRAINING - Step: 520, loss: 0.00860531721264124, rolling accuracy: 95.43269348144531\n",
            "TRAINING - Step: 530, loss: 0.004105963744223118, rolling accuracy: 95.51886749267578\n",
            "TRAINING - Step: 540, loss: 0.0025584849063307047, rolling accuracy: 95.60185241699219\n",
            "TRAINING - Step: 550, loss: 0.015187055803835392, rolling accuracy: 95.45454406738281\n",
            "TRAINING - Step: 560, loss: 0.001155114732682705, rolling accuracy: 95.53571319580078\n",
            "TRAINING - Step: 570, loss: 0.008708606474101543, rolling accuracy: 95.6140365600586\n",
            "TRAINING - Step: 580, loss: 0.003506394801661372, rolling accuracy: 95.68965148925781\n",
            "TRAINING - Step: 590, loss: 0.0015048737404868007, rolling accuracy: 95.76271057128906\n",
            "TRAINING - Step: 600, loss: 0.018060794100165367, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 610, loss: 0.013802523724734783, rolling accuracy: 95.69672393798828\n",
            "TRAINING - Step: 620, loss: 0.013907907530665398, rolling accuracy: 95.56451416015625\n",
            "TRAINING - Step: 630, loss: 0.0026087434962391853, rolling accuracy: 95.63491821289062\n",
            "TRAINING - Step: 640, loss: 0.007371499203145504, rolling accuracy: 95.703125\n",
            "TESTING - Loss: 0.011655495502054691, Accuracy: 96.8944091796875\n",
            "+++ FEDERATED MODEL 0, EPOCH: 4 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.09770691394805908, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.001570200896821916, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.0007823562482371926, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.0012023977469652891, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.0036867817398160696, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.008012575097382069, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 70, loss: 0.0003753283526748419, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 0.00017965356528293341, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 90, loss: 0.0033560977317392826, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 100, loss: 0.0036508124321699142, rolling accuracy: 98.75\n",
            "TRAINING - Step: 110, loss: 0.003317608730867505, rolling accuracy: 98.86363220214844\n",
            "TRAINING - Step: 120, loss: 0.0011431332677602768, rolling accuracy: 98.95833587646484\n",
            "TRAINING - Step: 130, loss: 0.01031281054019928, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 140, loss: 0.021713638678193092, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 150, loss: 0.0021499507129192352, rolling accuracy: 97.5\n",
            "TRAINING - Step: 160, loss: 0.003141125664114952, rolling accuracy: 97.65625\n",
            "TRAINING - Step: 170, loss: 0.0011098405811935663, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 180, loss: 0.01122953463345766, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 190, loss: 0.01480838656425476, rolling accuracy: 97.36842346191406\n",
            "TRAINING - Step: 200, loss: 0.0037664962001144886, rolling accuracy: 97.5\n",
            "TRAINING - Step: 210, loss: 0.0012733584735542536, rolling accuracy: 97.61904907226562\n",
            "TRAINING - Step: 220, loss: 0.0429866760969162, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 230, loss: 0.030545786023139954, rolling accuracy: 97.28260803222656\n",
            "TRAINING - Step: 240, loss: 0.02382158301770687, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 250, loss: 0.005081229843199253, rolling accuracy: 97.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.0010544912656769156, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 270, loss: 0.0408337377011776, rolling accuracy: 96.29629516601562\n",
            "TRAINING - Step: 280, loss: 0.0009903236059471965, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 290, loss: 0.0005928195314481854, rolling accuracy: 96.55172729492188\n",
            "TRAINING - Step: 300, loss: 0.009657914750277996, rolling accuracy: 96.25\n",
            "TRAINING - Step: 310, loss: 0.0015326340217143297, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 320, loss: 0.02452651411294937, rolling accuracy: 96.09375\n",
            "TRAINING - Step: 330, loss: 0.030428308993577957, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 340, loss: 0.004049594048410654, rolling accuracy: 95.95588684082031\n",
            "TRAINING - Step: 350, loss: 0.017916498705744743, rolling accuracy: 96.07142639160156\n",
            "TRAINING - Step: 360, loss: 0.0012027483899146318, rolling accuracy: 96.18055725097656\n",
            "TRAINING - Step: 370, loss: 0.05646262690424919, rolling accuracy: 95.60810852050781\n",
            "TRAINING - Step: 380, loss: 0.06621836125850677, rolling accuracy: 95.39473724365234\n",
            "TRAINING - Step: 390, loss: 0.040661077946424484, rolling accuracy: 95.19231414794922\n",
            "TRAINING - Step: 400, loss: 0.032964177429676056, rolling accuracy: 95.0\n",
            "TRAINING - Step: 410, loss: 0.0032075901981443167, rolling accuracy: 95.1219482421875\n",
            "TRAINING - Step: 420, loss: 0.0013107133563607931, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 430, loss: 0.008318368345499039, rolling accuracy: 95.34883880615234\n",
            "TRAINING - Step: 440, loss: 0.0019975260365754366, rolling accuracy: 95.45454406738281\n",
            "TRAINING - Step: 450, loss: 0.00872418750077486, rolling accuracy: 95.55555725097656\n",
            "TRAINING - Step: 460, loss: 0.0007398069137707353, rolling accuracy: 95.65216827392578\n",
            "TRAINING - Step: 470, loss: 0.004493660293519497, rolling accuracy: 95.74467468261719\n",
            "TRAINING - Step: 480, loss: 0.0031472451519221067, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 490, loss: 0.004220896866172552, rolling accuracy: 95.91836547851562\n",
            "TRAINING - Step: 500, loss: 0.01043962687253952, rolling accuracy: 95.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.00920887291431427, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 520, loss: 0.044346511363983154, rolling accuracy: 95.67308044433594\n",
            "TRAINING - Step: 530, loss: 0.00847103726118803, rolling accuracy: 95.75471496582031\n",
            "TRAINING - Step: 540, loss: 0.002672286704182625, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 550, loss: 0.015625720843672752, rolling accuracy: 95.68181610107422\n",
            "TRAINING - Step: 560, loss: 0.0017673780675977468, rolling accuracy: 95.75892639160156\n",
            "TRAINING - Step: 570, loss: 0.012373872101306915, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 580, loss: 0.11054769158363342, rolling accuracy: 95.68965148925781\n",
            "TRAINING - Step: 590, loss: 0.012599064968526363, rolling accuracy: 95.55084991455078\n",
            "TRAINING - Step: 600, loss: 0.004470777697861195, rolling accuracy: 95.625\n",
            "TRAINING - Step: 610, loss: 0.010683165863156319, rolling accuracy: 95.69672393798828\n",
            "TRAINING - Step: 620, loss: 0.0010246944148093462, rolling accuracy: 95.76612854003906\n",
            "TRAINING - Step: 630, loss: 0.012624422088265419, rolling accuracy: 95.63491821289062\n",
            "TRAINING - Step: 640, loss: 0.011566365137696266, rolling accuracy: 95.5078125\n",
            "+++ FEDERATED MODEL 1, EPOCH: 4 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0668518990278244, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.01619301177561283, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.05738535523414612, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.00021548029326368123, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.0033795349299907684, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.0018277564086019993, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 70, loss: 4.608334711520001e-05, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 9.465857147006318e-05, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 90, loss: 0.0030588172376155853, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 100, loss: 0.0006600752240046859, rolling accuracy: 98.75\n",
            "TRAINING - Step: 110, loss: 0.00457953754812479, rolling accuracy: 98.86363220214844\n",
            "TRAINING - Step: 120, loss: 0.0007490472053177655, rolling accuracy: 98.95833587646484\n",
            "TRAINING - Step: 130, loss: 0.03804127499461174, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 140, loss: 0.03547804430127144, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 150, loss: 0.006640222389250994, rolling accuracy: 97.5\n",
            "TRAINING - Step: 160, loss: 0.027013935148715973, rolling accuracy: 96.875\n",
            "TRAINING - Step: 170, loss: 0.0031064259819686413, rolling accuracy: 97.05882263183594\n",
            "TRAINING - Step: 180, loss: 0.05654322728514671, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 190, loss: 0.35755428671836853, rolling accuracy: 94.73684692382812\n",
            "TRAINING - Step: 200, loss: 0.0050416551530361176, rolling accuracy: 95.0\n",
            "TRAINING - Step: 210, loss: 0.0156259648501873, rolling accuracy: 95.23809814453125\n",
            "TRAINING - Step: 220, loss: 0.032005637884140015, rolling accuracy: 94.88636016845703\n",
            "TRAINING - Step: 230, loss: 0.010759999975562096, rolling accuracy: 94.56521606445312\n",
            "TRAINING - Step: 240, loss: 0.012996051460504532, rolling accuracy: 94.79167175292969\n",
            "TRAINING - Step: 250, loss: 0.0020692634861916304, rolling accuracy: 95.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.0018595114815980196, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 270, loss: 0.001722883665934205, rolling accuracy: 95.37036895751953\n",
            "TRAINING - Step: 280, loss: 0.0007121258531697094, rolling accuracy: 95.53571319580078\n",
            "TRAINING - Step: 290, loss: 0.0015041774604469538, rolling accuracy: 95.68965148925781\n",
            "TRAINING - Step: 300, loss: 0.00013135344488546252, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 310, loss: 0.015459607355296612, rolling accuracy: 95.96774291992188\n",
            "TRAINING - Step: 320, loss: 0.0009987402008846402, rolling accuracy: 96.09375\n",
            "TRAINING - Step: 330, loss: 0.0004902022192254663, rolling accuracy: 96.21212005615234\n",
            "TRAINING - Step: 340, loss: 0.0016747595509514213, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 350, loss: 0.0019184804987162352, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 360, loss: 0.003481656312942505, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 370, loss: 9.987028897739947e-05, rolling accuracy: 96.62162017822266\n",
            "TRAINING - Step: 380, loss: 0.003833367023617029, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 390, loss: 0.04562438279390335, rolling accuracy: 96.474365234375\n",
            "TRAINING - Step: 400, loss: 0.010016372427344322, rolling accuracy: 96.25\n",
            "TRAINING - Step: 410, loss: 0.0396796353161335, rolling accuracy: 96.03658294677734\n",
            "TRAINING - Step: 420, loss: 0.030123749747872353, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 430, loss: 0.004843638278543949, rolling accuracy: 95.93022918701172\n",
            "TRAINING - Step: 440, loss: 0.0014290295075625181, rolling accuracy: 96.0227279663086\n",
            "TRAINING - Step: 450, loss: 0.004748721141368151, rolling accuracy: 96.11111450195312\n",
            "TRAINING - Step: 460, loss: 0.02425013668835163, rolling accuracy: 96.19564819335938\n",
            "TRAINING - Step: 470, loss: 0.0465879812836647, rolling accuracy: 96.27659606933594\n",
            "TRAINING - Step: 480, loss: 0.03289975970983505, rolling accuracy: 96.09375762939453\n",
            "TRAINING - Step: 490, loss: 0.05306437984108925, rolling accuracy: 95.66326141357422\n",
            "TRAINING - Step: 500, loss: 0.019517721608281136, rolling accuracy: 95.50000762939453\n",
            "TRAINING - Step: 510, loss: 0.008501415140926838, rolling accuracy: 95.58824157714844\n",
            "TRAINING - Step: 520, loss: 0.0010286212200298905, rolling accuracy: 95.67308044433594\n",
            "TRAINING - Step: 530, loss: 0.0011576958931982517, rolling accuracy: 95.75471496582031\n",
            "TRAINING - Step: 540, loss: 0.00046051060780882835, rolling accuracy: 95.83332824707031\n",
            "TRAINING - Step: 550, loss: 0.009728828445076942, rolling accuracy: 95.90908813476562\n",
            "TRAINING - Step: 560, loss: 0.000316398567520082, rolling accuracy: 95.98213958740234\n",
            "TRAINING - Step: 570, loss: 0.00480533204972744, rolling accuracy: 96.0526351928711\n",
            "TRAINING - Step: 580, loss: 0.05605076998472214, rolling accuracy: 95.9051742553711\n",
            "TRAINING - Step: 590, loss: 0.0070126974023878574, rolling accuracy: 95.97457885742188\n",
            "TRAINING - Step: 600, loss: 0.0005204618792049587, rolling accuracy: 96.04167175292969\n",
            "TRAINING - Step: 610, loss: 0.0003122911730315536, rolling accuracy: 96.10655975341797\n",
            "TRAINING - Step: 620, loss: 0.001110331853851676, rolling accuracy: 96.16934967041016\n",
            "TRAINING - Step: 630, loss: 0.0016519413329660892, rolling accuracy: 96.23016357421875\n",
            "TRAINING - Step: 640, loss: 0.012368248775601387, rolling accuracy: 96.2890625\n",
            "+++ FEDERATED MODEL 2, EPOCH: 4 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0014479280216619372, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.03933583199977875, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 0.0009096877765841782, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.011414257809519768, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.001334519824013114, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.004694577772170305, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 70, loss: 0.004709302913397551, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 0.0033065220341086388, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 90, loss: 0.014228573068976402, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 100, loss: 0.005360549781471491, rolling accuracy: 98.75\n",
            "TRAINING - Step: 110, loss: 0.04138250648975372, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 120, loss: 0.0014154915697872639, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 130, loss: 0.027511198073625565, rolling accuracy: 96.15384674072266\n",
            "TRAINING - Step: 140, loss: 0.003812607377767563, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 150, loss: 0.0011921862605959177, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 160, loss: 0.009513943456113338, rolling accuracy: 96.875\n",
            "TRAINING - Step: 170, loss: 0.02875961735844612, rolling accuracy: 95.5882339477539\n",
            "TRAINING - Step: 180, loss: 0.0008117143297567964, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 190, loss: 0.0029366393573582172, rolling accuracy: 96.0526351928711\n",
            "TRAINING - Step: 200, loss: 0.0023172220680862665, rolling accuracy: 96.25\n",
            "TRAINING - Step: 210, loss: 0.00039346778066828847, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 220, loss: 0.006656048819422722, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 230, loss: 0.0005445796414278448, rolling accuracy: 96.73912811279297\n",
            "TRAINING - Step: 240, loss: 0.002612851792946458, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 250, loss: 0.1702989786863327, rolling accuracy: 96.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.11333976686000824, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 270, loss: 0.05711245909333229, rolling accuracy: 94.90740203857422\n",
            "TRAINING - Step: 280, loss: 0.03068433701992035, rolling accuracy: 95.08928680419922\n",
            "TRAINING - Step: 290, loss: 0.027112845331430435, rolling accuracy: 94.82758331298828\n",
            "TRAINING - Step: 300, loss: 0.0157664455473423, rolling accuracy: 94.58333587646484\n",
            "TRAINING - Step: 310, loss: 0.004188709892332554, rolling accuracy: 94.75806427001953\n",
            "TRAINING - Step: 320, loss: 0.022942496463656425, rolling accuracy: 94.53125\n",
            "TRAINING - Step: 330, loss: 0.0010924356756731868, rolling accuracy: 94.69696807861328\n",
            "TRAINING - Step: 340, loss: 0.04410476237535477, rolling accuracy: 94.48529815673828\n",
            "TRAINING - Step: 350, loss: 0.00836141873151064, rolling accuracy: 94.64285278320312\n",
            "TRAINING - Step: 360, loss: 0.0009967322694137692, rolling accuracy: 94.79167175292969\n",
            "TRAINING - Step: 370, loss: 0.001882702112197876, rolling accuracy: 94.93243408203125\n",
            "TRAINING - Step: 380, loss: 0.04268013685941696, rolling accuracy: 94.73684692382812\n",
            "TRAINING - Step: 390, loss: 0.04150618985295296, rolling accuracy: 94.23077392578125\n",
            "TRAINING - Step: 400, loss: 0.002107434906065464, rolling accuracy: 94.375\n",
            "TRAINING - Step: 410, loss: 0.0025690814945846796, rolling accuracy: 94.51219177246094\n",
            "TRAINING - Step: 420, loss: 0.0008855469641275704, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 430, loss: 0.00447065569460392, rolling accuracy: 94.76744079589844\n",
            "TRAINING - Step: 440, loss: 0.00015733619511593133, rolling accuracy: 94.88636016845703\n",
            "TRAINING - Step: 450, loss: 0.006263229064643383, rolling accuracy: 95.0\n",
            "TRAINING - Step: 460, loss: 0.01570637710392475, rolling accuracy: 94.83695220947266\n",
            "TRAINING - Step: 470, loss: 0.011972763575613499, rolling accuracy: 94.94680786132812\n",
            "TRAINING - Step: 480, loss: 0.0003866627812385559, rolling accuracy: 95.05208587646484\n",
            "TRAINING - Step: 490, loss: 0.0009460565051995218, rolling accuracy: 95.15306091308594\n",
            "TRAINING - Step: 500, loss: 0.0001638452522456646, rolling accuracy: 95.25000762939453\n",
            "TRAINING - Step: 510, loss: 0.04788827896118164, rolling accuracy: 95.0980453491211\n",
            "TRAINING - Step: 520, loss: 0.003094728570431471, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 530, loss: 0.0008694187272340059, rolling accuracy: 95.28302001953125\n",
            "TRAINING - Step: 540, loss: 0.013838866725564003, rolling accuracy: 95.13888549804688\n",
            "TRAINING - Step: 550, loss: 0.0053625041618943214, rolling accuracy: 95.2272720336914\n",
            "TRAINING - Step: 560, loss: 0.02101602405309677, rolling accuracy: 95.08928680419922\n",
            "TRAINING - Step: 570, loss: 0.014444680884480476, rolling accuracy: 94.95613861083984\n",
            "TRAINING - Step: 580, loss: 0.029734067618846893, rolling accuracy: 94.61206817626953\n",
            "TRAINING - Step: 590, loss: 0.03388630598783493, rolling accuracy: 94.49152374267578\n",
            "TRAINING - Step: 600, loss: 0.013689701445400715, rolling accuracy: 94.58333587646484\n",
            "TRAINING - Step: 610, loss: 0.0013363135512918234, rolling accuracy: 94.67213439941406\n",
            "TRAINING - Step: 620, loss: 0.17431047558784485, rolling accuracy: 94.55644989013672\n",
            "TRAINING - Step: 630, loss: 0.001606678357347846, rolling accuracy: 94.64286041259766\n",
            "TRAINING - Step: 640, loss: 0.000558802392333746, rolling accuracy: 94.7265625\n",
            "+++ FEDERATED MODEL 3, EPOCH: 4 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.003050759667530656, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.0038670601788908243, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.00020223314641043544, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.0010888923425227404, rolling accuracy: 100.0\n",
            "TRAINING - Step: 50, loss: 0.007167762145400047, rolling accuracy: 100.0\n",
            "TRAINING - Step: 60, loss: 0.0025117574259638786, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 70, loss: 0.0001018481416394934, rolling accuracy: 100.0\n",
            "TRAINING - Step: 80, loss: 0.00020078066154383123, rolling accuracy: 100.0\n",
            "TRAINING - Step: 90, loss: 0.0015084652695804834, rolling accuracy: 100.0\n",
            "TRAINING - Step: 100, loss: 0.0003710349556058645, rolling accuracy: 100.0\n",
            "TRAINING - Step: 110, loss: 0.05553949624300003, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 120, loss: 0.0015563875203952193, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 130, loss: 0.0030804146081209183, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 140, loss: 0.0014318586327135563, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 150, loss: 0.0043676006607711315, rolling accuracy: 98.33333587646484\n",
            "TRAINING - Step: 160, loss: 0.0040898071601986885, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 170, loss: 0.000808045850135386, rolling accuracy: 98.52941131591797\n",
            "TRAINING - Step: 180, loss: 0.003968392964452505, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 190, loss: 0.00674479641020298, rolling accuracy: 98.68421173095703\n",
            "TRAINING - Step: 200, loss: 0.0014152623480185866, rolling accuracy: 98.75\n",
            "TRAINING - Step: 210, loss: 0.0004888078547082841, rolling accuracy: 98.80952453613281\n",
            "TRAINING - Step: 220, loss: 0.004475332796573639, rolling accuracy: 98.86363220214844\n",
            "TRAINING - Step: 230, loss: 0.016755035147070885, rolling accuracy: 98.36956024169922\n",
            "TRAINING - Step: 240, loss: 0.0038586019072681665, rolling accuracy: 98.43750762939453\n",
            "TRAINING - Step: 250, loss: 0.0005608038627542555, rolling accuracy: 98.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.01718718372285366, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 270, loss: 0.015698278322815895, rolling accuracy: 98.14814758300781\n",
            "TRAINING - Step: 280, loss: 0.0021638991311192513, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 290, loss: 0.0009185169474221766, rolling accuracy: 98.27586364746094\n",
            "TRAINING - Step: 300, loss: 0.001677325926721096, rolling accuracy: 98.33333587646484\n",
            "TRAINING - Step: 310, loss: 0.0005616412381641567, rolling accuracy: 98.38709259033203\n",
            "TRAINING - Step: 320, loss: 0.009504174813628197, rolling accuracy: 98.046875\n",
            "TRAINING - Step: 330, loss: 0.00899583101272583, rolling accuracy: 98.1060562133789\n",
            "TRAINING - Step: 340, loss: 0.09864426404237747, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 350, loss: 0.04028104990720749, rolling accuracy: 97.5\n",
            "TRAINING - Step: 360, loss: 0.004985199775546789, rolling accuracy: 97.56945037841797\n",
            "TRAINING - Step: 370, loss: 0.010157343000173569, rolling accuracy: 97.63513946533203\n",
            "TRAINING - Step: 380, loss: 0.034564435482025146, rolling accuracy: 97.36842346191406\n",
            "TRAINING - Step: 390, loss: 0.0017466364661231637, rolling accuracy: 97.43589782714844\n",
            "TRAINING - Step: 400, loss: 0.008914661593735218, rolling accuracy: 97.5\n",
            "TRAINING - Step: 410, loss: 0.0033117199782282114, rolling accuracy: 97.56097412109375\n",
            "TRAINING - Step: 420, loss: 0.0009436308173462749, rolling accuracy: 97.61904907226562\n",
            "TRAINING - Step: 430, loss: 0.0023357458412647247, rolling accuracy: 97.6744155883789\n",
            "TRAINING - Step: 440, loss: 0.003174241166561842, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 450, loss: 0.0050219157710671425, rolling accuracy: 97.77777862548828\n",
            "TRAINING - Step: 460, loss: 0.0011326000094413757, rolling accuracy: 97.82608032226562\n",
            "TRAINING - Step: 470, loss: 0.008034788072109222, rolling accuracy: 97.8723373413086\n",
            "TRAINING - Step: 480, loss: 0.04164300113916397, rolling accuracy: 97.65625762939453\n",
            "TRAINING - Step: 490, loss: 0.003501760307699442, rolling accuracy: 97.7040786743164\n",
            "TRAINING - Step: 500, loss: 0.0031241055112332106, rolling accuracy: 97.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.005116383079439402, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 520, loss: 0.007061936426907778, rolling accuracy: 97.83654022216797\n",
            "TRAINING - Step: 530, loss: 0.012925910763442516, rolling accuracy: 97.64151000976562\n",
            "TRAINING - Step: 540, loss: 0.001617299858480692, rolling accuracy: 97.6851806640625\n",
            "TRAINING - Step: 550, loss: 0.008251674473285675, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 560, loss: 0.022784274071455002, rolling accuracy: 97.76786041259766\n",
            "TRAINING - Step: 570, loss: 0.0013154814951121807, rolling accuracy: 97.80702209472656\n",
            "TRAINING - Step: 580, loss: 0.002431778935715556, rolling accuracy: 97.8448257446289\n",
            "TRAINING - Step: 590, loss: 0.0052725449204444885, rolling accuracy: 97.88135528564453\n",
            "TRAINING - Step: 600, loss: 0.014074143022298813, rolling accuracy: 97.70833587646484\n",
            "TRAINING - Step: 610, loss: 0.07970768213272095, rolling accuracy: 97.33606719970703\n",
            "TRAINING - Step: 620, loss: 0.047459885478019714, rolling accuracy: 97.3790283203125\n",
            "TRAINING - Step: 630, loss: 0.007659131661057472, rolling accuracy: 97.42063903808594\n",
            "TRAINING - Step: 640, loss: 0.001613677479326725, rolling accuracy: 97.4609375\n",
            "TESTING - Loss: 0.010875420644879341, Accuracy: 96.04037475585938\n",
            "+++ FEDERATED MODEL 0, EPOCH: 5 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.0005238607409410179, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.8173657655715942, rolling accuracy: 75.0\n",
            "TRAINING - Step: 30, loss: 8.966676978161559e-05, rolling accuracy: 83.33333587646484\n",
            "TRAINING - Step: 40, loss: 8.436964708380401e-05, rolling accuracy: 87.5\n",
            "TRAINING - Step: 50, loss: 0.0005484529538080096, rolling accuracy: 90.0\n",
            "TRAINING - Step: 60, loss: 0.00030537522980012, rolling accuracy: 91.66667175292969\n",
            "TRAINING - Step: 70, loss: 0.008971450850367546, rolling accuracy: 92.85713958740234\n",
            "TRAINING - Step: 80, loss: 0.001651412108913064, rolling accuracy: 93.75\n",
            "TRAINING - Step: 90, loss: 0.0002050496987067163, rolling accuracy: 94.44445037841797\n",
            "TRAINING - Step: 100, loss: 0.01727757416665554, rolling accuracy: 93.75\n",
            "TRAINING - Step: 110, loss: 0.00029099249513819814, rolling accuracy: 94.31817626953125\n",
            "TRAINING - Step: 120, loss: 0.0010051358258351684, rolling accuracy: 94.79167175292969\n",
            "TRAINING - Step: 130, loss: 0.00020665052579715848, rolling accuracy: 95.19230651855469\n",
            "TRAINING - Step: 140, loss: 0.00044422101927921176, rolling accuracy: 95.53571319580078\n",
            "TRAINING - Step: 150, loss: 0.003273328300565481, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 160, loss: 0.0015721683157607913, rolling accuracy: 96.09375\n",
            "TRAINING - Step: 170, loss: 0.0013356974814087152, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 180, loss: 0.00011017991346307099, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 190, loss: 0.0006631542928516865, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 200, loss: 0.0021843058057129383, rolling accuracy: 96.875\n",
            "TRAINING - Step: 210, loss: 5.4874173656571656e-05, rolling accuracy: 97.02381134033203\n",
            "TRAINING - Step: 220, loss: 0.006978318095207214, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 230, loss: 0.001518480945378542, rolling accuracy: 97.28260803222656\n",
            "TRAINING - Step: 240, loss: 0.00023588500334881246, rolling accuracy: 97.39583587646484\n",
            "TRAINING - Step: 250, loss: 4.584101770888083e-05, rolling accuracy: 97.50000762939453\n",
            "TRAINING - Step: 260, loss: 0.005779062397778034, rolling accuracy: 97.59615325927734\n",
            "TRAINING - Step: 270, loss: 0.001228787237778306, rolling accuracy: 97.6851806640625\n",
            "TRAINING - Step: 280, loss: 0.00028315186500549316, rolling accuracy: 97.76786041259766\n",
            "TRAINING - Step: 290, loss: 0.0011325563536956906, rolling accuracy: 97.8448257446289\n",
            "TRAINING - Step: 300, loss: 0.0011274607386440039, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 310, loss: 0.00035650908830575645, rolling accuracy: 97.98387145996094\n",
            "TRAINING - Step: 320, loss: 0.008314920589327812, rolling accuracy: 98.046875\n",
            "TRAINING - Step: 330, loss: 7.033156725810841e-05, rolling accuracy: 98.1060562133789\n",
            "TRAINING - Step: 340, loss: 0.03429427370429039, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 350, loss: 0.0003910084196832031, rolling accuracy: 97.85713958740234\n",
            "TRAINING - Step: 360, loss: 0.005280510988086462, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 370, loss: 0.0012825899757444859, rolling accuracy: 97.97297668457031\n",
            "TRAINING - Step: 380, loss: 0.002929484471678734, rolling accuracy: 98.02632141113281\n",
            "TRAINING - Step: 390, loss: 0.016164585947990417, rolling accuracy: 97.75641632080078\n",
            "TRAINING - Step: 400, loss: 0.00029975431971251965, rolling accuracy: 97.8125\n",
            "TRAINING - Step: 410, loss: 0.42683470249176025, rolling accuracy: 97.56097412109375\n",
            "TRAINING - Step: 420, loss: 0.03593418747186661, rolling accuracy: 97.61904907226562\n",
            "TRAINING - Step: 430, loss: 0.046601552516222, rolling accuracy: 97.38372039794922\n",
            "TRAINING - Step: 440, loss: 0.01552781742066145, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 450, loss: 0.00014486312284134328, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 460, loss: 0.07161825150251389, rolling accuracy: 96.73912811279297\n",
            "TRAINING - Step: 470, loss: 0.0025124680250883102, rolling accuracy: 96.80850982666016\n",
            "TRAINING - Step: 480, loss: 0.005497291684150696, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 490, loss: 0.0009857628028839827, rolling accuracy: 96.93877410888672\n",
            "TRAINING - Step: 500, loss: 0.0025885861832648516, rolling accuracy: 97.00000762939453\n",
            "TRAINING - Step: 510, loss: 0.015166820026934147, rolling accuracy: 96.81372833251953\n",
            "TRAINING - Step: 520, loss: 0.10877219587564468, rolling accuracy: 96.63461303710938\n",
            "TRAINING - Step: 530, loss: 0.003471900010481477, rolling accuracy: 96.69811248779297\n",
            "TRAINING - Step: 540, loss: 0.009535450488328934, rolling accuracy: 96.7592544555664\n",
            "TRAINING - Step: 550, loss: 0.0015021872241050005, rolling accuracy: 96.81818389892578\n",
            "TRAINING - Step: 560, loss: 0.0015724013792350888, rolling accuracy: 96.875\n",
            "TRAINING - Step: 570, loss: 0.07358422875404358, rolling accuracy: 96.71052551269531\n",
            "TRAINING - Step: 580, loss: 0.04088699817657471, rolling accuracy: 96.55172729492188\n",
            "TRAINING - Step: 590, loss: 0.0034905553329735994, rolling accuracy: 96.61016845703125\n",
            "TRAINING - Step: 600, loss: 0.004084127489477396, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 610, loss: 0.021128296852111816, rolling accuracy: 96.51639556884766\n",
            "TRAINING - Step: 620, loss: 0.04173830524086952, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 630, loss: 0.006633394863456488, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 640, loss: 0.0006745019927620888, rolling accuracy: 96.484375\n",
            "+++ FEDERATED MODEL 1, EPOCH: 5 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.019177444279193878, rolling accuracy: 87.5\n",
            "TRAINING - Step: 20, loss: 0.0004612463526427746, rolling accuracy: 93.75\n",
            "TRAINING - Step: 30, loss: 2.540946297813207e-05, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 40, loss: 0.006543976720422506, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.0001210486443596892, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.006741723045706749, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 70, loss: 0.0006317972438409925, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 0.0029362302739173174, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 90, loss: 0.010561483912169933, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 100, loss: 0.00036620188620872796, rolling accuracy: 98.75\n",
            "TRAINING - Step: 110, loss: 0.01844829134643078, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 120, loss: 0.011056183837354183, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 130, loss: 0.007047759834676981, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 140, loss: 0.0012740340316668153, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 150, loss: 0.02600647695362568, rolling accuracy: 97.5\n",
            "TRAINING - Step: 160, loss: 0.01592664048075676, rolling accuracy: 96.875\n",
            "TRAINING - Step: 170, loss: 0.0018637834582477808, rolling accuracy: 97.05882263183594\n",
            "TRAINING - Step: 180, loss: 0.0675077959895134, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 190, loss: 0.017653901129961014, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 200, loss: 0.0014077372616156936, rolling accuracy: 96.875\n",
            "TRAINING - Step: 210, loss: 0.00011102033749921247, rolling accuracy: 97.02381134033203\n",
            "TRAINING - Step: 220, loss: 0.0010613086633384228, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 230, loss: 0.0034263187553733587, rolling accuracy: 97.28260803222656\n",
            "TRAINING - Step: 240, loss: 0.008244278840720654, rolling accuracy: 97.39583587646484\n",
            "TRAINING - Step: 250, loss: 0.4032551050186157, rolling accuracy: 97.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.04764917492866516, rolling accuracy: 96.15384674072266\n",
            "TRAINING - Step: 270, loss: 0.00864440482109785, rolling accuracy: 96.29629516601562\n",
            "TRAINING - Step: 280, loss: 0.003624666016548872, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 290, loss: 0.0017088443273678422, rolling accuracy: 96.55172729492188\n",
            "TRAINING - Step: 300, loss: 0.0045480309054255486, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 310, loss: 0.021356970071792603, rolling accuracy: 96.37096405029297\n",
            "TRAINING - Step: 320, loss: 0.0001837966265156865, rolling accuracy: 96.484375\n",
            "TRAINING - Step: 330, loss: 0.0007402462651953101, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 340, loss: 0.024998778477311134, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 350, loss: 0.000578447594307363, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 360, loss: 0.0011028400622308254, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 370, loss: 0.0010497710900381207, rolling accuracy: 96.62162017822266\n",
            "TRAINING - Step: 380, loss: 0.00033308425918221474, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 390, loss: 0.027003612369298935, rolling accuracy: 96.474365234375\n",
            "TRAINING - Step: 400, loss: 0.002254093298688531, rolling accuracy: 96.5625\n",
            "TRAINING - Step: 410, loss: 0.06860724836587906, rolling accuracy: 96.34146118164062\n",
            "TRAINING - Step: 420, loss: 0.00025996327167376876, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 430, loss: 0.0007708632037974894, rolling accuracy: 96.51162719726562\n",
            "TRAINING - Step: 440, loss: 0.014999291859567165, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 450, loss: 0.010105153545737267, rolling accuracy: 96.3888931274414\n",
            "TRAINING - Step: 460, loss: 0.0001988487201742828, rolling accuracy: 96.4673843383789\n",
            "TRAINING - Step: 470, loss: 0.002585636917501688, rolling accuracy: 96.54254913330078\n",
            "TRAINING - Step: 480, loss: 0.0017229922814294696, rolling accuracy: 96.61458587646484\n",
            "TRAINING - Step: 490, loss: 0.005959765054285526, rolling accuracy: 96.68367004394531\n",
            "TRAINING - Step: 500, loss: 0.002209143713116646, rolling accuracy: 96.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.5331692099571228, rolling accuracy: 96.32353210449219\n",
            "TRAINING - Step: 520, loss: 0.0028234776109457016, rolling accuracy: 96.39423370361328\n",
            "TRAINING - Step: 530, loss: 0.005126507952809334, rolling accuracy: 96.46226501464844\n",
            "TRAINING - Step: 540, loss: 0.0014443608233705163, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 550, loss: 0.00219904538244009, rolling accuracy: 96.59091186523438\n",
            "TRAINING - Step: 560, loss: 0.0011343188816681504, rolling accuracy: 96.65178680419922\n",
            "TRAINING - Step: 570, loss: 0.02149752900004387, rolling accuracy: 96.4912338256836\n",
            "TRAINING - Step: 580, loss: 0.001987921539694071, rolling accuracy: 96.55172729492188\n",
            "TRAINING - Step: 590, loss: 0.00755518302321434, rolling accuracy: 96.61016845703125\n",
            "TRAINING - Step: 600, loss: 0.0006384089938364923, rolling accuracy: 96.66667175292969\n",
            "TRAINING - Step: 610, loss: 0.0008047338342294097, rolling accuracy: 96.7213134765625\n",
            "TRAINING - Step: 620, loss: 0.0010638547828420997, rolling accuracy: 96.7741928100586\n",
            "TRAINING - Step: 630, loss: 0.00291691767051816, rolling accuracy: 96.82540130615234\n",
            "TRAINING - Step: 640, loss: 0.0017648490611463785, rolling accuracy: 96.875\n",
            "+++ FEDERATED MODEL 2, EPOCH: 5 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.002846959512680769, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 9.914300608215854e-05, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.00036006627487950027, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.0006989478133618832, rolling accuracy: 100.0\n",
            "TRAINING - Step: 50, loss: 0.019734743982553482, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.01972641423344612, rolling accuracy: 95.83333587646484\n",
            "TRAINING - Step: 70, loss: 0.0014270558021962643, rolling accuracy: 96.42857360839844\n",
            "TRAINING - Step: 80, loss: 0.10310163348913193, rolling accuracy: 96.875\n",
            "TRAINING - Step: 90, loss: 0.002954032737761736, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 100, loss: 0.0014616897096857429, rolling accuracy: 97.5\n",
            "TRAINING - Step: 110, loss: 0.11174140125513077, rolling accuracy: 96.59090423583984\n",
            "TRAINING - Step: 120, loss: 0.003446517512202263, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 130, loss: 0.00898626446723938, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 140, loss: 0.01106969453394413, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 150, loss: 0.0011712646810337901, rolling accuracy: 97.5\n",
            "TRAINING - Step: 160, loss: 0.000807701435405761, rolling accuracy: 97.65625\n",
            "TRAINING - Step: 170, loss: 0.00026293686823919415, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 180, loss: 0.00017365606618113816, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 190, loss: 0.00045094345114193857, rolling accuracy: 98.02632141113281\n",
            "TRAINING - Step: 200, loss: 0.0014802295481786132, rolling accuracy: 98.125\n",
            "TRAINING - Step: 210, loss: 0.004261675290763378, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 220, loss: 0.0001442457432858646, rolling accuracy: 98.29544830322266\n",
            "TRAINING - Step: 230, loss: 0.0009131478145718575, rolling accuracy: 98.36956024169922\n",
            "TRAINING - Step: 240, loss: 0.0014265625504776835, rolling accuracy: 98.43750762939453\n",
            "TRAINING - Step: 250, loss: 0.019091423600912094, rolling accuracy: 98.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.12841860949993134, rolling accuracy: 97.59615325927734\n",
            "TRAINING - Step: 270, loss: 0.027442730963230133, rolling accuracy: 96.7592544555664\n",
            "TRAINING - Step: 280, loss: 0.001953827915713191, rolling accuracy: 96.875\n",
            "TRAINING - Step: 290, loss: 0.009672271087765694, rolling accuracy: 96.98275756835938\n",
            "TRAINING - Step: 300, loss: 0.0007306562038138509, rolling accuracy: 97.08333587646484\n",
            "TRAINING - Step: 310, loss: 0.0004820544854737818, rolling accuracy: 97.17742156982422\n",
            "TRAINING - Step: 320, loss: 0.043658021837472916, rolling accuracy: 96.875\n",
            "TRAINING - Step: 330, loss: 0.004623583983629942, rolling accuracy: 96.96969604492188\n",
            "TRAINING - Step: 340, loss: 0.0012878177221864462, rolling accuracy: 97.05882263183594\n",
            "TRAINING - Step: 350, loss: 0.0003847129119094461, rolling accuracy: 97.14285278320312\n",
            "TRAINING - Step: 360, loss: 0.0005988497869111598, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 370, loss: 0.005788308568298817, rolling accuracy: 97.29730224609375\n",
            "TRAINING - Step: 380, loss: 0.0003897686838172376, rolling accuracy: 97.36842346191406\n",
            "TRAINING - Step: 390, loss: 0.024613667279481888, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 400, loss: 0.000129860476590693, rolling accuracy: 97.1875\n",
            "TRAINING - Step: 410, loss: 0.0015748043078929186, rolling accuracy: 97.25609588623047\n",
            "TRAINING - Step: 420, loss: 0.02280234917998314, rolling accuracy: 97.02381134033203\n",
            "TRAINING - Step: 430, loss: 0.0004663086438085884, rolling accuracy: 97.09302520751953\n",
            "TRAINING - Step: 440, loss: 0.014736967161297798, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 450, loss: 0.0007742563611827791, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 460, loss: 0.020023241639137268, rolling accuracy: 97.0108642578125\n",
            "TRAINING - Step: 470, loss: 0.06957122683525085, rolling accuracy: 96.80850982666016\n",
            "TRAINING - Step: 480, loss: 0.001069728285074234, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 490, loss: 0.0015732706524431705, rolling accuracy: 96.93877410888672\n",
            "TRAINING - Step: 500, loss: 0.028524719178676605, rolling accuracy: 96.75000762939453\n",
            "TRAINING - Step: 510, loss: 0.0020296175498515368, rolling accuracy: 96.81372833251953\n",
            "TRAINING - Step: 520, loss: 0.0033564302138984203, rolling accuracy: 96.875\n",
            "TRAINING - Step: 530, loss: 0.0007042026845738292, rolling accuracy: 96.9339599609375\n",
            "TRAINING - Step: 540, loss: 0.0006740582175552845, rolling accuracy: 96.99073791503906\n",
            "TRAINING - Step: 550, loss: 0.00744493817910552, rolling accuracy: 97.04545593261719\n",
            "TRAINING - Step: 560, loss: 0.0008268739911727607, rolling accuracy: 97.09821319580078\n",
            "TRAINING - Step: 570, loss: 0.27448633313179016, rolling accuracy: 96.92982482910156\n",
            "TRAINING - Step: 580, loss: 0.008810670115053654, rolling accuracy: 96.98275756835938\n",
            "TRAINING - Step: 590, loss: 0.0009296232019551098, rolling accuracy: 97.03389739990234\n",
            "TRAINING - Step: 600, loss: 0.0022383739706128836, rolling accuracy: 97.08333587646484\n",
            "TRAINING - Step: 610, loss: 0.001331895007751882, rolling accuracy: 97.13114929199219\n",
            "TRAINING - Step: 620, loss: 0.00028461628244258463, rolling accuracy: 97.17742156982422\n",
            "TRAINING - Step: 630, loss: 0.013916125521063805, rolling accuracy: 97.02381134033203\n",
            "TRAINING - Step: 640, loss: 0.009900613687932491, rolling accuracy: 97.0703125\n",
            "+++ FEDERATED MODEL 3, EPOCH: 5 +++++++++\n",
            "TRAINING - Step: 10, loss: 0.022319192066788673, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 0.0324622243642807, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.0012275524204596877, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.00040696520591154695, rolling accuracy: 100.0\n",
            "TRAINING - Step: 50, loss: 0.0010899137705564499, rolling accuracy: 100.0\n",
            "TRAINING - Step: 60, loss: 5.620316369459033e-05, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 70, loss: 0.01156278233975172, rolling accuracy: 100.0\n",
            "TRAINING - Step: 80, loss: 3.2327974622603506e-05, rolling accuracy: 100.0\n",
            "TRAINING - Step: 90, loss: 0.00029564296710304916, rolling accuracy: 100.0\n",
            "TRAINING - Step: 100, loss: 0.001646700082346797, rolling accuracy: 100.0\n",
            "TRAINING - Step: 110, loss: 0.00045034007052890956, rolling accuracy: 100.0\n",
            "TRAINING - Step: 120, loss: 0.0003748502931557596, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 130, loss: 0.00013711223436985165, rolling accuracy: 100.0\n",
            "TRAINING - Step: 140, loss: 4.212113708490506e-05, rolling accuracy: 100.0\n",
            "TRAINING - Step: 150, loss: 0.07253438979387283, rolling accuracy: 99.16667175292969\n",
            "TRAINING - Step: 160, loss: 0.0035590799525380135, rolling accuracy: 99.21875\n",
            "TRAINING - Step: 170, loss: 0.06050945073366165, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 180, loss: 0.06294752657413483, rolling accuracy: 96.52777862548828\n",
            "TRAINING - Step: 190, loss: 0.0024142141919583082, rolling accuracy: 96.71053314208984\n",
            "TRAINING - Step: 200, loss: 0.0018635774031281471, rolling accuracy: 96.875\n",
            "TRAINING - Step: 210, loss: 0.004727265797555447, rolling accuracy: 97.02381134033203\n",
            "TRAINING - Step: 220, loss: 0.006229055114090443, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 230, loss: 0.0012974918354302645, rolling accuracy: 97.28260803222656\n",
            "TRAINING - Step: 240, loss: 0.021588310599327087, rolling accuracy: 96.87500762939453\n",
            "TRAINING - Step: 250, loss: 0.005699684843420982, rolling accuracy: 97.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.02505374327301979, rolling accuracy: 96.63461303710938\n",
            "TRAINING - Step: 270, loss: 0.0012680827639997005, rolling accuracy: 96.7592544555664\n",
            "TRAINING - Step: 280, loss: 0.00020103351562283933, rolling accuracy: 96.875\n",
            "TRAINING - Step: 290, loss: 0.0032138917595148087, rolling accuracy: 96.98275756835938\n",
            "TRAINING - Step: 300, loss: 0.0021583461202681065, rolling accuracy: 97.08333587646484\n",
            "TRAINING - Step: 310, loss: 0.0012170588597655296, rolling accuracy: 97.17742156982422\n",
            "TRAINING - Step: 320, loss: 0.00032084796112030745, rolling accuracy: 97.265625\n",
            "TRAINING - Step: 330, loss: 0.01150805689394474, rolling accuracy: 97.34848022460938\n",
            "TRAINING - Step: 340, loss: 0.0040659247897565365, rolling accuracy: 97.42647552490234\n",
            "TRAINING - Step: 350, loss: 0.04223516955971718, rolling accuracy: 97.14285278320312\n",
            "TRAINING - Step: 360, loss: 0.02114085480570793, rolling accuracy: 96.875\n",
            "TRAINING - Step: 370, loss: 6.477215356426314e-05, rolling accuracy: 96.95946502685547\n",
            "TRAINING - Step: 380, loss: 8.449172310065478e-05, rolling accuracy: 97.03947448730469\n",
            "TRAINING - Step: 390, loss: 0.003121044719591737, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 400, loss: 0.003452535020187497, rolling accuracy: 97.1875\n",
            "TRAINING - Step: 410, loss: 0.0010870123514905572, rolling accuracy: 97.25609588623047\n",
            "TRAINING - Step: 420, loss: 0.09589342027902603, rolling accuracy: 97.3214340209961\n",
            "TRAINING - Step: 430, loss: 0.006482134573161602, rolling accuracy: 97.38372039794922\n",
            "TRAINING - Step: 440, loss: 0.0078597916290164, rolling accuracy: 97.44317626953125\n",
            "TRAINING - Step: 450, loss: 0.017292415723204613, rolling accuracy: 97.5\n",
            "TRAINING - Step: 460, loss: 0.0003588077670428902, rolling accuracy: 97.5543441772461\n",
            "TRAINING - Step: 470, loss: 0.0005583548336289823, rolling accuracy: 97.60637664794922\n",
            "TRAINING - Step: 480, loss: 0.005163914058357477, rolling accuracy: 97.65625762939453\n",
            "TRAINING - Step: 490, loss: 0.07356575131416321, rolling accuracy: 97.448974609375\n",
            "TRAINING - Step: 500, loss: 0.0012821387499570847, rolling accuracy: 97.50000762939453\n",
            "TRAINING - Step: 510, loss: 0.0014646422350779176, rolling accuracy: 97.54902648925781\n",
            "TRAINING - Step: 520, loss: 0.006918754894286394, rolling accuracy: 97.59615325927734\n",
            "TRAINING - Step: 530, loss: 0.016623664647340775, rolling accuracy: 97.4056625366211\n",
            "TRAINING - Step: 540, loss: 0.019518226385116577, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 550, loss: 0.006243489217013121, rolling accuracy: 97.2727279663086\n",
            "TRAINING - Step: 560, loss: 0.005653007887303829, rolling accuracy: 97.32142639160156\n",
            "TRAINING - Step: 570, loss: 0.0016618712106719613, rolling accuracy: 97.36842346191406\n",
            "TRAINING - Step: 580, loss: 0.036134738475084305, rolling accuracy: 97.19827270507812\n",
            "TRAINING - Step: 590, loss: 0.011893262155354023, rolling accuracy: 97.03389739990234\n",
            "TRAINING - Step: 600, loss: 0.000597469275817275, rolling accuracy: 97.08333587646484\n",
            "TRAINING - Step: 610, loss: 0.008877872489392757, rolling accuracy: 97.13114929199219\n",
            "TRAINING - Step: 620, loss: 0.08401036262512207, rolling accuracy: 96.9758071899414\n",
            "TRAINING - Step: 630, loss: 0.21272321045398712, rolling accuracy: 96.82540130615234\n",
            "TRAINING - Step: 640, loss: 0.004253980703651905, rolling accuracy: 96.875\n",
            "TESTING - Loss: 0.006190321408212185, Accuracy: 98.13665008544922\n",
            "+++ FEDERATED MODEL 0, EPOCH: 6 +++++++++\n",
            "TRAINING - Step: 10, loss: 4.093855750397779e-05, rolling accuracy: 100.0\n",
            "TRAINING - Step: 20, loss: 8.427196007687598e-06, rolling accuracy: 100.0\n",
            "TRAINING - Step: 30, loss: 0.0013384325429797173, rolling accuracy: 100.00000762939453\n",
            "TRAINING - Step: 40, loss: 0.00982434581965208, rolling accuracy: 96.875\n",
            "TRAINING - Step: 50, loss: 0.00035325484350323677, rolling accuracy: 97.5\n",
            "TRAINING - Step: 60, loss: 0.03543977811932564, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 70, loss: 0.00010191904584644362, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 80, loss: 0.06934637576341629, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 90, loss: 0.0071096098981797695, rolling accuracy: 98.61111450195312\n",
            "TRAINING - Step: 100, loss: 0.01575217768549919, rolling accuracy: 97.5\n",
            "TRAINING - Step: 110, loss: 0.010314283892512321, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 120, loss: 0.00012173288268968463, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 130, loss: 9.089474042411894e-05, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 140, loss: 0.0009769359603524208, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 150, loss: 0.008959480561316013, rolling accuracy: 97.5\n",
            "TRAINING - Step: 160, loss: 0.04088955372571945, rolling accuracy: 97.65625\n",
            "TRAINING - Step: 170, loss: 0.00011468784214230254, rolling accuracy: 97.79412078857422\n",
            "TRAINING - Step: 180, loss: 0.0002283897192683071, rolling accuracy: 97.91667175292969\n",
            "TRAINING - Step: 190, loss: 3.667073178803548e-05, rolling accuracy: 98.02632141113281\n",
            "TRAINING - Step: 200, loss: 0.00018509426445234567, rolling accuracy: 98.125\n",
            "TRAINING - Step: 210, loss: 0.00035426943213678896, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 220, loss: 0.0006461193552240729, rolling accuracy: 98.29544830322266\n",
            "TRAINING - Step: 230, loss: 0.0010158055229112506, rolling accuracy: 98.36956024169922\n",
            "TRAINING - Step: 240, loss: 0.000333390140440315, rolling accuracy: 98.43750762939453\n",
            "TRAINING - Step: 250, loss: 0.013636252842843533, rolling accuracy: 98.00000762939453\n",
            "TRAINING - Step: 260, loss: 0.00039389615994878113, rolling accuracy: 98.07691955566406\n",
            "TRAINING - Step: 270, loss: 3.18725396937225e-05, rolling accuracy: 98.14814758300781\n",
            "TRAINING - Step: 280, loss: 0.0016853292472660542, rolling accuracy: 98.21428680419922\n",
            "TRAINING - Step: 290, loss: 0.0004898279439657927, rolling accuracy: 98.27586364746094\n",
            "TRAINING - Step: 300, loss: 0.00040308659663423896, rolling accuracy: 98.33333587646484\n",
            "TRAINING - Step: 310, loss: 0.0002128774212906137, rolling accuracy: 98.38709259033203\n",
            "TRAINING - Step: 320, loss: 0.0012759041273966432, rolling accuracy: 98.4375\n",
            "TRAINING - Step: 330, loss: 0.9487278461456299, rolling accuracy: 97.7272720336914\n",
            "TRAINING - Step: 340, loss: 0.01210716087371111, rolling accuracy: 97.42647552490234\n",
            "TRAINING - Step: 350, loss: 0.003205891465768218, rolling accuracy: 97.5\n",
            "TRAINING - Step: 360, loss: 0.011712094768881798, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 370, loss: 0.003384967800229788, rolling accuracy: 97.29730224609375\n",
            "TRAINING - Step: 380, loss: 0.0036613005213439465, rolling accuracy: 97.36842346191406\n",
            "TRAINING - Step: 390, loss: 0.01863396354019642, rolling accuracy: 97.11538696289062\n",
            "TRAINING - Step: 400, loss: 0.020729171112179756, rolling accuracy: 97.1875\n",
            "TRAINING - Step: 410, loss: 0.0008865209529176354, rolling accuracy: 97.25609588623047\n",
            "TRAINING - Step: 420, loss: 0.00032588429166935384, rolling accuracy: 97.3214340209961\n",
            "TRAINING - Step: 430, loss: 0.015312900766730309, rolling accuracy: 97.09302520751953\n",
            "TRAINING - Step: 440, loss: 9.919960575643927e-05, rolling accuracy: 97.15908813476562\n",
            "TRAINING - Step: 450, loss: 0.0036858064122498035, rolling accuracy: 97.22222137451172\n",
            "TRAINING - Step: 460, loss: 0.0007817659061402082, rolling accuracy: 97.28260803222656\n",
            "TRAINING - Step: 470, loss: 9.147549280896783e-05, rolling accuracy: 97.34042358398438\n",
            "TRAINING - Step: 480, loss: 0.0001199482794618234, rolling accuracy: 97.39583587646484\n",
            "TRAINING - Step: 490, loss: 0.0003500131133478135, rolling accuracy: 97.448974609375\n",
            "TRAINING - Step: 500, loss: 0.39609527587890625, rolling accuracy: 97.25000762939453\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f192beeb9ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_federated_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_training_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-2ea3916e78b8>\u001b[0m in \u001b[0;36mrun_federated_training\u001b[0;34m(federated_model, client_models, client_training_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mclient_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_federated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_training_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-21789c22f0b6>\u001b[0m in \u001b[0;36mtrain_federated\u001b[0;34m(model, data_loader, optimizer, loss)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_preparation_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    213\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2q5KpTXVWZD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}