{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zwZJFe5-pCLV",
    "outputId": "5b270b81-a9e7-43d9-cc42-378c0fed3740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click in c:\\users\\donat\\anaconda3\\lib\\site-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\donat\\anaconda3\\lib\\site-packages (from opendatasets) (4.59.0)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\donat\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\donat\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\donat\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.25.1)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.26.4)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=13dd37300a5c46c5ceb7d1ea514e60f8759ebd65463ea887d0f4523f518f3f4f\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
      "Successfully installed kaggle-1.5.12 opendatasets-0.1.20 python-slugify-5.0.2 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.18.0-cp38-cp38-win_amd64.whl (912 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.40.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=e89aa74750e1c100124ec380fca3ec922eefe1b444907467b6050a3969ab1e0f\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=b1dbfd20f0298628d3acb611bbb8783bdd48f7f130d4ff9336e44f0455dee328\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_federated\n",
      "  Downloading tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
      "Collecting tensorflow-model-optimization~=0.5.0\n",
      "  Downloading tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172 kB)\n",
      "Collecting tensorflow_federated\n",
      "  Downloading tensorflow_federated-0.18.0-py2.py3-none-any.whl (578 kB)\n",
      "  Downloading tensorflow_federated-0.17.0-py2.py3-none-any.whl (517 kB)\n",
      "Collecting tensorflow-model-optimization~=0.4.0\n",
      "  Downloading tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172 kB)\n",
      "Collecting grpcio~=1.29.0\n",
      "  Downloading grpcio-1.29.0-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "Collecting semantic-version~=2.8.5\n",
      "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy~=1.18.4\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Collecting cachetools~=3.1.1\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting attrs~=19.3.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting absl-py~=0.9.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting retrying~=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting tensorflow~=2.3.0\n",
      "  Downloading tensorflow-2.3.4-cp38-cp38-win_amd64.whl (342.7 MB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "Collecting tensorflow-addons~=0.11.1\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-win_amd64.whl (911 kB)\n",
      "Collecting portpicker~=1.3.1\n",
      "  Downloading portpicker-1.3.9-py3-none-any.whl (13 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting tensorflow-privacy~=0.5.0\n",
      "  Downloading tensorflow_privacy-0.5.2-py3-none-any.whl (192 kB)\n",
      "Requirement already satisfied: six in c:\\users\\donat\\anaconda3\\lib\\site-packages (from absl-py~=0.9.0->tensorflow_federated) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.18.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.6.3)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.1.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow-privacy~=0.5.0->tensorflow_federated) (1.6.2)\n",
      "Requirement already satisfied: mpmath in c:\\users\\donat\\anaconda3\\lib\\site-packages (from tensorflow-privacy~=0.5.0->tensorflow_federated) (1.2.1)\n",
      "Building wheels for collected packages: absl-py, retrying\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121930 sha256=dc16daa7c4e7aedf58e7fac736dd3c327c65cc57c4f0e7d892565f5127646575\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\1d\\10\\8e\\2f79b924179ff1e6510933d63eb851bea01054fff262343b7a\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=165cffd8d70b855439cae8b753d1b5431cf1f23e6003b655bc794ccdf66c4d74\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\c4\\a7\\48\\0a434133f6d56e878ca511c0e6c38326907c0792f67b476e56\n",
      "Successfully built absl-py retrying\n",
      "Installing collected packages: cachetools, numpy, grpcio, absl-py, typeguard, tensorflow-estimator, h5py, gast, dm-tree, tensorflow-privacy, tensorflow-model-optimization, tensorflow-addons, tensorflow, semantic-version, retrying, portpicker, attrs, tensorflow-federated\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.40.0\n",
      "    Uninstalling grpcio-1.40.0:\n",
      "      Successfully uninstalled grpcio-1.40.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "Successfully installed absl-py-0.9.0 attrs-19.3.0 cachetools-3.1.1 dm-tree-0.1.6 gast-0.3.3 grpcio-1.29.0 h5py-2.10.0 numpy-1.18.5 portpicker-1.3.9 retrying-1.3.3 semantic-version-2.8.5 tensorflow-2.3.4 tensorflow-addons-0.11.2 tensorflow-estimator-2.3.0 tensorflow-federated-0.17.0 tensorflow-model-optimization-0.4.1 tensorflow-privacy-0.5.2 typeguard-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\donat\\anaconda3\\lib\\site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\donat\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\donat\\anaconda3\\lib\\site-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: numpy in c:\\users\\donat\\anaconda3\\lib\\site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: torch==1.9.0 in c:\\users\\donat\\anaconda3\\lib\\site-packages (from torchvision) (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\donat\\anaconda3\\lib\\site-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jpd3iU1El1ZT"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TraicDeCpHii",
    "outputId": "ec940e7b-0a35-4c28-9a85-98f7d7b3f6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\chest-xray-covid19-pneumonia\" (use force=True to force download)\n",
      "Skipping, found downloaded files in \".\\novel-corona-virus-2019-dataset\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\")\n",
    "od.download(\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JPFiGGjbhta_"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001 # 0.0001\n",
    "MAX_EPOCHS = 25\n",
    "TARGET_FOLDER = \"weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WvAJ8eTnTRkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW1RF4LvUUsG"
   },
   "source": [
    "Resize Images to 244, 244. By using to_tensor the images are already normalized between 0 and 1. \"The image object is an array of (244, 244, 3) should be flattened to be list (178, 608).\" What? wie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Z_GY07pSzQT"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((244, 244))\n",
    "                                , transforms.ToTensor()]\n",
    "                               #, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # find mean and std of dataset\n",
    "                              )\n",
    "\n",
    "test_set = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/test', transform=transform)\n",
    "\n",
    "train_set = dataset = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_preparation(labels):\n",
    "    labels = np.array(labels)\n",
    "    labels[labels > 0] = 1\n",
    "    return list(labels)\n",
    "\n",
    "def label_preparation_tensor(labels):\n",
    "    labels[labels > 0] = 1\n",
    "    return labels\n",
    "\n",
    "train_set.targets = label_preparation(train_set.targets)\n",
    "\n",
    "test_set.targets = label_preparation(test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VAi62OkRTZfV"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVxct9thFVz2"
   },
   "source": [
    "In Paper aus References haben sie unter anderen ResNet18 benutzt: https://arxiv.org/pdf/2007.05592.pdf\n",
    "\n",
    "Deshalb würde ich das probieren.\n",
    "\n",
    "TODO: test different ones, e.g. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWhXD92ZRzNP",
    "outputId": "b473ca2e-2e15-4480-dab7-4fd210df225a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "icqqTTRN-ht6"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(result, labels):\n",
    "    result = torch.sigmoid(result).round()\n",
    "    \n",
    "    correct_results_sum = (result == labels).sum().float()\n",
    "    acc = correct_results_sum/labels.shape[0]\n",
    "    acc *= 100\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A5Me-0EHR_md"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, loss):\n",
    "    \"\"\"\n",
    "    model -- neural net\n",
    "    data_loader -- dataloader for train images\n",
    "    optimizer -- optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(data_loader, 1):\n",
    "        images = images.to(device)\n",
    "        labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result = model(images)\n",
    "        targets = labels.unsqueeze(1).float()\n",
    "\n",
    "        loss_value = loss(result.float(), targets)\n",
    "\n",
    "        # backpropagation\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        if step % 10 == 0:\n",
    "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
    "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Xk2CR_whWTxw"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss):\n",
    "    \"\"\"    \n",
    "    model -- neural net \n",
    "    test_loader -- dataloader of test images\n",
    "    epoch -- current epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value = 0\n",
    "        accuracy = 0\n",
    "        for step, [images, labels] in enumerate(test_loader, 1):\n",
    "            images = images.to(device)\n",
    "            labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "            result = model(images)\n",
    "            targets = labels.detach().unsqueeze(1).float()\n",
    "\n",
    "            loss_value += loss(result.detach(), targets)\n",
    "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
    "\n",
    "        loss_value /= step\n",
    "        accuracy /=  step\n",
    "  \n",
    "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
    "    return accuracy > 93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "jzfyMxdBVoB1",
    "outputId": "c3a6aff3-d0a6-4a92-fd67-8d72310e227d"
   },
   "outputs": [],
   "source": [
    "def run_local_training():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # use pos weights because of unbalanced data set\n",
    "    loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        print(f\"+++ EPOCH: {epoch+1} +++++++++\")\n",
    "        torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy (non-federated)\n",
    "        train(model, train_loader, optimizer, loss)\n",
    "        break\n",
    "        # save interim weights\n",
    "        torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "        if test(model, test_loader, loss) and epoch > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_local_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc9Z1kAKniiG"
   },
   "source": [
    "# Federated\n",
    "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
    "\n",
    "i don't think we need syft if we just simulate the federation on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clients = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "federated_model.to(device)\n",
    "\n",
    "def reset_client_param_list():\n",
    "    return [None] * number_of_clients\n",
    "    \n",
    "client_params = reset_client_param_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_models = [torchvision.models.resnet18(pretrained=False, num_classes=1).to(device) for _ in range(number_of_clients)]\n",
    "client_optimizers = [torch.optim.Adam(client_models[idx].parameters()) for idx in range(number_of_clients)]\n",
    "client_training_loader = [torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True, num_workers=4) for _ in range(number_of_clients)]\n",
    "\n",
    "federated_test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_params_to_global_model(client_params, client_idx, params):\n",
    "    client_params[client_idx] = params\n",
    "    for idx in range (number_of_clients):\n",
    "        if client_params[idx] is None:\n",
    "            return client_params, False\n",
    "        \n",
    "    return client_params, True\n",
    "\n",
    "def send_params_to_client(federated_model, client):\n",
    "    client.load_state_dict(federated_model.state_dict(), True)\n",
    "    return client\n",
    "    \n",
    "def federated_average(federated_model, client_params):\n",
    "    with torch.no_grad():\n",
    "        average_weights = OrderedDict()\n",
    "\n",
    "        # check if client_params has no Nones?\n",
    "        for client_param in client_params:\n",
    "            for key, value in client_param.items():\n",
    "                if key in average_weights:\n",
    "                    average_weights[key] += value.clone()\n",
    "                else:\n",
    "                    average_weights[key] = value.clone()\n",
    "\n",
    "        for key, value in average_weights.items():\n",
    "            average_weights[key] = torch.div(average_weights[key], len(client_params), rounding_mode='floor')\n",
    "                \n",
    "                \n",
    "    federated_model.load_state_dict(average_weights, True)\n",
    "    return federated_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_federated(model, data_loader, optimizer, loss):\n",
    "    \"\"\"\n",
    "    model -- neural net\n",
    "    data_loader -- dataloader for train images\n",
    "    optimizer -- optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(data_loader, 1):\n",
    "        images = images.to(device)\n",
    "        labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result = model(images)\n",
    "        targets = labels.unsqueeze(1).float()\n",
    "\n",
    "        loss_value = loss(result.float(), targets)\n",
    "\n",
    "        # backpropagation\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        if step % 10 == 0:\n",
    "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
    "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_federated(model, test_loader, loss):\n",
    "    \"\"\"    \n",
    "    model -- neural net \n",
    "    test_loader -- dataloader of test images\n",
    "    epoch -- current epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy (non-federated)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value = 0\n",
    "        accuracy = 0\n",
    "        for step, [images, labels] in enumerate(test_loader, 1):\n",
    "            images = images.to(device)\n",
    "            labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "            result = model(images)\n",
    "            targets = labels.detach().unsqueeze(1).float()\n",
    "\n",
    "            loss_value += loss(result.detach(), targets)\n",
    "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
    "\n",
    "        loss_value /= step\n",
    "        accuracy /=  step\n",
    "  \n",
    "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
    "    return accuracy > 93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "hR2YMmTLs49U",
    "outputId": "f2c51ce1-3579-46de-9458-7b03e73c8cec"
   },
   "outputs": [],
   "source": [
    "def run_federated_training(federated_model, client_models, client_optimizers, client_params, client_training_loader):\n",
    "    # use pos weights because of unbalanced data set\n",
    "    federated_loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
    "    # federated_loss = torch.nn.CrossEntropyLoss(weight=torch.tensor([1./10])).to(device) # sparce categorical crossentropy (federated)\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        for client_idx in range (number_of_clients):\n",
    "            print(f\"+++ FEDERATED MODEL {client_idx}, EPOCH: {epoch+1} +++++++++\")\n",
    "\n",
    "            client_model = client_models[client_idx]\n",
    "            client_model = send_params_to_client(federated_model, client_model)\n",
    "\n",
    "            client_optimizer = client_optimizers[client_idx]\n",
    "\n",
    "            train_federated(client_model, client_training_loader[client_idx], client_optimizer, federated_loss)\n",
    "\n",
    "            client_params, _ = send_params_to_global_model(client_params, client_idx, client_model.state_dict())\n",
    "\n",
    "            # save interim weights\n",
    "            #torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "        federated_model = federated_average(federated_model, client_params)\n",
    "        client_params = reset_client_param_list()\n",
    "        \n",
    "        test_federated(federated_model, federated_test_loader, federated_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YLC89DQ2rKQe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ FEDERATED MODEL 0, EPOCH: 1 +++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donat\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 10, loss: 0.1328851580619812, rolling accuracy: 75.0\n",
      "TRAINING - Step: 20, loss: 0.043614745140075684, rolling accuracy: 81.25\n",
      "TRAINING - Step: 30, loss: 0.1180800199508667, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 40, loss: 0.020529018715023994, rolling accuracy: 84.375\n",
      "TRAINING - Step: 50, loss: 0.5681607127189636, rolling accuracy: 85.0\n",
      "TRAINING - Step: 60, loss: 0.1679622232913971, rolling accuracy: 77.08333587646484\n",
      "TRAINING - Step: 70, loss: 0.06908659636974335, rolling accuracy: 75.0\n",
      "TRAINING - Step: 80, loss: 0.020371772348880768, rolling accuracy: 78.125\n",
      "TRAINING - Step: 90, loss: 0.061381686478853226, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 100, loss: 0.4305272102355957, rolling accuracy: 80.0\n",
      "TRAINING - Step: 110, loss: 0.4173275828361511, rolling accuracy: 78.40908813476562\n",
      "TRAINING - Step: 120, loss: 0.01620090752840042, rolling accuracy: 80.20833587646484\n",
      "TRAINING - Step: 130, loss: 0.015924273058772087, rolling accuracy: 81.73076629638672\n",
      "TRAINING - Step: 140, loss: 0.032410673797130585, rolling accuracy: 82.14286041259766\n",
      "TRAINING - Step: 150, loss: 0.04164170101284981, rolling accuracy: 81.66667175292969\n",
      "TRAINING - Step: 160, loss: 0.011347312480211258, rolling accuracy: 82.8125\n",
      "TRAINING - Step: 170, loss: 0.03807659074664116, rolling accuracy: 83.0882339477539\n",
      "TRAINING - Step: 180, loss: 0.04459359496831894, rolling accuracy: 84.02777862548828\n",
      "TRAINING - Step: 190, loss: 0.1527654081583023, rolling accuracy: 83.5526351928711\n",
      "TRAINING - Step: 200, loss: 0.036498889327049255, rolling accuracy: 83.75\n",
      "TRAINING - Step: 210, loss: 0.018491191789507866, rolling accuracy: 84.52381134033203\n",
      "TRAINING - Step: 220, loss: 0.014745725318789482, rolling accuracy: 85.2272720336914\n",
      "TRAINING - Step: 230, loss: 0.03046218305826187, rolling accuracy: 85.32608032226562\n",
      "TRAINING - Step: 240, loss: 0.01693766564130783, rolling accuracy: 85.93750762939453\n",
      "TRAINING - Step: 250, loss: 0.024880582466721535, rolling accuracy: 86.00000762939453\n",
      "TRAINING - Step: 260, loss: 0.02290276810526848, rolling accuracy: 86.05769348144531\n",
      "TRAINING - Step: 270, loss: 0.05418398976325989, rolling accuracy: 85.1851806640625\n",
      "TRAINING - Step: 280, loss: 0.04752675071358681, rolling accuracy: 84.82142639160156\n",
      "TRAINING - Step: 290, loss: 0.02943863719701767, rolling accuracy: 85.3448257446289\n",
      "TRAINING - Step: 300, loss: 0.0032980230171233416, rolling accuracy: 85.83333587646484\n",
      "TRAINING - Step: 310, loss: 0.04858653247356415, rolling accuracy: 85.88709259033203\n",
      "TRAINING - Step: 320, loss: 0.03354210779070854, rolling accuracy: 85.546875\n",
      "TRAINING - Step: 330, loss: 0.033329032361507416, rolling accuracy: 85.6060562133789\n",
      "TRAINING - Step: 340, loss: 0.005541935563087463, rolling accuracy: 86.02941131591797\n",
      "TRAINING - Step: 350, loss: 0.02078903466463089, rolling accuracy: 86.07142639160156\n",
      "TRAINING - Step: 360, loss: 0.08508967608213425, rolling accuracy: 85.41667175292969\n",
      "TRAINING - Step: 370, loss: 0.02818296290934086, rolling accuracy: 85.47297668457031\n",
      "TRAINING - Step: 380, loss: 0.015639876946806908, rolling accuracy: 85.85527038574219\n",
      "TRAINING - Step: 390, loss: 0.006585015915334225, rolling accuracy: 86.21794891357422\n",
      "TRAINING - Step: 400, loss: 0.0812026634812355, rolling accuracy: 86.5625\n",
      "TRAINING - Step: 410, loss: 0.009950820356607437, rolling accuracy: 86.89024353027344\n",
      "TRAINING - Step: 420, loss: 0.0027172539848834276, rolling accuracy: 87.20238494873047\n",
      "TRAINING - Step: 430, loss: 0.008597617037594318, rolling accuracy: 87.5\n",
      "TRAINING - Step: 440, loss: 0.07937133312225342, rolling accuracy: 87.21590423583984\n",
      "TRAINING - Step: 450, loss: 0.0473928228020668, rolling accuracy: 86.66667175292969\n",
      "TRAINING - Step: 460, loss: 0.011423636227846146, rolling accuracy: 86.9565200805664\n",
      "TRAINING - Step: 470, loss: 0.1404767781496048, rolling accuracy: 86.96807861328125\n",
      "TRAINING - Step: 480, loss: 0.058373235166072845, rolling accuracy: 86.71875762939453\n",
      "TRAINING - Step: 490, loss: 0.014448332600295544, rolling accuracy: 86.98979187011719\n",
      "TRAINING - Step: 500, loss: 0.037071987986564636, rolling accuracy: 87.00000762939453\n",
      "TRAINING - Step: 510, loss: 0.003394010476768017, rolling accuracy: 87.2549057006836\n",
      "TRAINING - Step: 520, loss: 0.014030100777745247, rolling accuracy: 87.5\n",
      "TRAINING - Step: 530, loss: 0.012744313105940819, rolling accuracy: 87.73584747314453\n",
      "TRAINING - Step: 540, loss: 0.004647690802812576, rolling accuracy: 87.96295928955078\n",
      "TRAINING - Step: 550, loss: 0.02202526479959488, rolling accuracy: 87.95454406738281\n",
      "TRAINING - Step: 560, loss: 0.007097143679857254, rolling accuracy: 88.16963958740234\n",
      "TRAINING - Step: 570, loss: 0.018807295709848404, rolling accuracy: 88.15789794921875\n",
      "TRAINING - Step: 580, loss: 0.03159699961543083, rolling accuracy: 88.14655303955078\n",
      "TRAINING - Step: 590, loss: 0.002271477598696947, rolling accuracy: 88.34745788574219\n",
      "TRAINING - Step: 600, loss: 0.010066335089504719, rolling accuracy: 88.54167175292969\n",
      "TRAINING - Step: 610, loss: 0.01195461768656969, rolling accuracy: 88.52458953857422\n",
      "TRAINING - Step: 620, loss: 0.004906667396426201, rolling accuracy: 88.70967864990234\n",
      "TRAINING - Step: 630, loss: 0.004819572903215885, rolling accuracy: 88.8888931274414\n",
      "TRAINING - Step: 640, loss: 0.024698201566934586, rolling accuracy: 89.0625\n",
      "+++ FEDERATED MODEL 1, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.4482787251472473, rolling accuracy: 62.5\n",
      "TRAINING - Step: 20, loss: 0.05457928776741028, rolling accuracy: 68.75\n",
      "TRAINING - Step: 30, loss: 0.03818250447511673, rolling accuracy: 75.00000762939453\n",
      "TRAINING - Step: 40, loss: 0.009421881288290024, rolling accuracy: 81.25\n",
      "TRAINING - Step: 50, loss: 0.08141689747571945, rolling accuracy: 80.0\n",
      "TRAINING - Step: 60, loss: 0.008153204806149006, rolling accuracy: 83.33333587646484\n",
      "TRAINING - Step: 70, loss: 0.1007196307182312, rolling accuracy: 83.92857360839844\n",
      "TRAINING - Step: 80, loss: 0.12033472955226898, rolling accuracy: 82.8125\n",
      "TRAINING - Step: 90, loss: 0.04284004867076874, rolling accuracy: 83.33333587646484\n",
      "TRAINING - Step: 100, loss: 0.027022968977689743, rolling accuracy: 83.75\n",
      "TRAINING - Step: 110, loss: 0.019213244318962097, rolling accuracy: 85.2272720336914\n",
      "TRAINING - Step: 120, loss: 0.015447660349309444, rolling accuracy: 86.45833587646484\n",
      "TRAINING - Step: 130, loss: 0.006383843254297972, rolling accuracy: 87.5\n",
      "TRAINING - Step: 140, loss: 0.010402992367744446, rolling accuracy: 88.39286041259766\n",
      "TRAINING - Step: 150, loss: 0.05223250389099121, rolling accuracy: 87.5\n",
      "TRAINING - Step: 160, loss: 0.08262462913990021, rolling accuracy: 87.5\n",
      "TRAINING - Step: 170, loss: 0.033850621432065964, rolling accuracy: 86.76470947265625\n",
      "TRAINING - Step: 180, loss: 0.03508256748318672, rolling accuracy: 86.80555725097656\n",
      "TRAINING - Step: 190, loss: 0.19370818138122559, rolling accuracy: 86.18421173095703\n",
      "TRAINING - Step: 200, loss: 0.023032771423459053, rolling accuracy: 86.25\n",
      "TRAINING - Step: 210, loss: 0.00775330001488328, rolling accuracy: 86.9047622680664\n",
      "TRAINING - Step: 220, loss: 0.04106241092085838, rolling accuracy: 86.93181610107422\n",
      "TRAINING - Step: 230, loss: 0.04945610463619232, rolling accuracy: 86.41304016113281\n",
      "TRAINING - Step: 240, loss: 0.03656516224145889, rolling accuracy: 86.45833587646484\n",
      "TRAINING - Step: 250, loss: 0.014914589002728462, rolling accuracy: 86.50000762939453\n",
      "TRAINING - Step: 260, loss: 0.008972417563199997, rolling accuracy: 87.01923370361328\n",
      "TRAINING - Step: 270, loss: 0.01318217720836401, rolling accuracy: 87.5\n",
      "TRAINING - Step: 280, loss: 0.016829106956720352, rolling accuracy: 87.94642639160156\n",
      "TRAINING - Step: 290, loss: 0.005277520976960659, rolling accuracy: 88.36206817626953\n",
      "TRAINING - Step: 300, loss: 0.0020607621408998966, rolling accuracy: 88.75\n",
      "TRAINING - Step: 310, loss: 0.014824995771050453, rolling accuracy: 89.11289978027344\n",
      "TRAINING - Step: 320, loss: 0.018400533124804497, rolling accuracy: 89.0625\n",
      "TRAINING - Step: 330, loss: 0.016526181250810623, rolling accuracy: 89.39393615722656\n",
      "TRAINING - Step: 340, loss: 0.039992332458496094, rolling accuracy: 89.3382339477539\n",
      "TRAINING - Step: 350, loss: 0.05576114356517792, rolling accuracy: 89.64285278320312\n",
      "TRAINING - Step: 360, loss: 0.02901419810950756, rolling accuracy: 89.58333587646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 370, loss: 0.009386278688907623, rolling accuracy: 89.8648681640625\n",
      "TRAINING - Step: 380, loss: 0.02614017389714718, rolling accuracy: 89.8026351928711\n",
      "TRAINING - Step: 390, loss: 0.041117098182439804, rolling accuracy: 89.74359130859375\n",
      "TRAINING - Step: 400, loss: 0.05815167725086212, rolling accuracy: 89.6875\n",
      "TRAINING - Step: 410, loss: 0.07526841759681702, rolling accuracy: 89.63414764404297\n",
      "TRAINING - Step: 420, loss: 0.02711445838212967, rolling accuracy: 89.58333587646484\n",
      "TRAINING - Step: 430, loss: 0.018263189122080803, rolling accuracy: 89.53488159179688\n",
      "TRAINING - Step: 440, loss: 0.04973895102739334, rolling accuracy: 89.48863220214844\n",
      "TRAINING - Step: 450, loss: 0.01250607892870903, rolling accuracy: 89.72222137451172\n",
      "TRAINING - Step: 460, loss: 0.009420952759683132, rolling accuracy: 89.94564819335938\n",
      "TRAINING - Step: 470, loss: 0.018845807760953903, rolling accuracy: 90.1595687866211\n",
      "TRAINING - Step: 480, loss: 0.0030417917296290398, rolling accuracy: 90.36458587646484\n",
      "TRAINING - Step: 490, loss: 0.07325682789087296, rolling accuracy: 90.30612182617188\n",
      "TRAINING - Step: 500, loss: 0.040754638612270355, rolling accuracy: 90.25000762939453\n",
      "TRAINING - Step: 510, loss: 0.25340527296066284, rolling accuracy: 89.95098876953125\n",
      "TRAINING - Step: 520, loss: 0.00838263239711523, rolling accuracy: 90.14423370361328\n",
      "TRAINING - Step: 530, loss: 0.035890862345695496, rolling accuracy: 90.0943374633789\n",
      "TRAINING - Step: 540, loss: 0.05619187280535698, rolling accuracy: 89.58332824707031\n",
      "TRAINING - Step: 550, loss: 0.012376014143228531, rolling accuracy: 89.7727279663086\n",
      "TRAINING - Step: 560, loss: 0.00523137953132391, rolling accuracy: 89.95536041259766\n",
      "TRAINING - Step: 570, loss: 0.37522363662719727, rolling accuracy: 89.69298553466797\n",
      "TRAINING - Step: 580, loss: 0.03567083179950714, rolling accuracy: 89.87068939208984\n",
      "TRAINING - Step: 590, loss: 0.045023586601018906, rolling accuracy: 89.40677642822266\n",
      "TRAINING - Step: 600, loss: 0.02777674049139023, rolling accuracy: 89.375\n",
      "TRAINING - Step: 610, loss: 0.02369873598217964, rolling accuracy: 89.3442611694336\n",
      "TRAINING - Step: 620, loss: 0.024698296561837196, rolling accuracy: 89.31451416015625\n",
      "TRAINING - Step: 630, loss: 0.02790260687470436, rolling accuracy: 89.28571319580078\n",
      "TRAINING - Step: 640, loss: 0.2725764513015747, rolling accuracy: 89.2578125\n",
      "+++ FEDERATED MODEL 2, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.012717923149466515, rolling accuracy: 100.0\n",
      "TRAINING - Step: 20, loss: 0.06665994971990585, rolling accuracy: 87.5\n",
      "TRAINING - Step: 30, loss: 0.03667031228542328, rolling accuracy: 91.66667175292969\n",
      "TRAINING - Step: 40, loss: 0.21791264414787292, rolling accuracy: 84.375\n",
      "TRAINING - Step: 50, loss: 0.06606918573379517, rolling accuracy: 82.5\n",
      "TRAINING - Step: 60, loss: 0.06945902854204178, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 70, loss: 0.04971206188201904, rolling accuracy: 78.57142639160156\n",
      "TRAINING - Step: 80, loss: 0.059299953281879425, rolling accuracy: 79.6875\n",
      "TRAINING - Step: 90, loss: 0.012344959191977978, rolling accuracy: 81.94444274902344\n",
      "TRAINING - Step: 100, loss: 0.06008530780673027, rolling accuracy: 83.75\n",
      "TRAINING - Step: 110, loss: 0.0054255286231637, rolling accuracy: 85.2272720336914\n",
      "TRAINING - Step: 120, loss: 0.026980416849255562, rolling accuracy: 86.45833587646484\n",
      "TRAINING - Step: 130, loss: 0.059212688356637955, rolling accuracy: 87.5\n",
      "TRAINING - Step: 140, loss: 0.09850257635116577, rolling accuracy: 87.5\n",
      "TRAINING - Step: 150, loss: 0.026115473359823227, rolling accuracy: 87.5\n",
      "TRAINING - Step: 160, loss: 0.048639677464962006, rolling accuracy: 86.71875\n",
      "TRAINING - Step: 170, loss: 0.049748495221138, rolling accuracy: 86.76470947265625\n",
      "TRAINING - Step: 180, loss: 0.01823669672012329, rolling accuracy: 87.5\n",
      "TRAINING - Step: 190, loss: 0.04123936593532562, rolling accuracy: 87.5\n",
      "TRAINING - Step: 200, loss: 0.06357352435588837, rolling accuracy: 87.5\n",
      "TRAINING - Step: 210, loss: 0.032686151564121246, rolling accuracy: 87.5\n",
      "TRAINING - Step: 220, loss: 0.02933647856116295, rolling accuracy: 88.06817626953125\n",
      "TRAINING - Step: 230, loss: 0.04136081039905548, rolling accuracy: 88.04347229003906\n",
      "TRAINING - Step: 240, loss: 0.07744772732257843, rolling accuracy: 87.50000762939453\n",
      "TRAINING - Step: 250, loss: 0.00801277719438076, rolling accuracy: 88.00000762939453\n",
      "TRAINING - Step: 260, loss: 0.0033298516646027565, rolling accuracy: 88.46154022216797\n",
      "TRAINING - Step: 270, loss: 0.09277959913015366, rolling accuracy: 87.96295928955078\n",
      "TRAINING - Step: 280, loss: 0.027377553284168243, rolling accuracy: 87.94642639160156\n",
      "TRAINING - Step: 290, loss: 0.01332036592066288, rolling accuracy: 88.36206817626953\n",
      "TRAINING - Step: 300, loss: 0.0058644297532737255, rolling accuracy: 88.75\n",
      "TRAINING - Step: 310, loss: 0.027960240840911865, rolling accuracy: 89.11289978027344\n",
      "TRAINING - Step: 320, loss: 0.09969168156385422, rolling accuracy: 89.453125\n",
      "TRAINING - Step: 330, loss: 0.06192966550588608, rolling accuracy: 89.39393615722656\n",
      "TRAINING - Step: 340, loss: 0.03463241457939148, rolling accuracy: 89.70588684082031\n",
      "TRAINING - Step: 350, loss: 0.02354743331670761, rolling accuracy: 89.64285278320312\n",
      "TRAINING - Step: 360, loss: 0.12474875897169113, rolling accuracy: 89.23611450195312\n",
      "TRAINING - Step: 370, loss: 0.02514164336025715, rolling accuracy: 89.52703094482422\n",
      "TRAINING - Step: 380, loss: 0.10787764191627502, rolling accuracy: 89.47368621826172\n",
      "TRAINING - Step: 390, loss: 0.0201481431722641, rolling accuracy: 89.42308044433594\n",
      "TRAINING - Step: 400, loss: 0.059484079480171204, rolling accuracy: 89.6875\n",
      "TRAINING - Step: 410, loss: 0.040318094193935394, rolling accuracy: 89.32926940917969\n",
      "TRAINING - Step: 420, loss: 0.19285953044891357, rolling accuracy: 89.28572082519531\n",
      "TRAINING - Step: 430, loss: 0.01837337762117386, rolling accuracy: 89.53488159179688\n",
      "TRAINING - Step: 440, loss: 0.029352780431509018, rolling accuracy: 89.48863220214844\n",
      "TRAINING - Step: 450, loss: 0.021092627197504044, rolling accuracy: 89.44445037841797\n",
      "TRAINING - Step: 460, loss: 0.03848820924758911, rolling accuracy: 89.13043212890625\n",
      "TRAINING - Step: 470, loss: 0.011127561330795288, rolling accuracy: 89.36170196533203\n",
      "TRAINING - Step: 480, loss: 0.003234702628105879, rolling accuracy: 89.58333587646484\n",
      "TRAINING - Step: 490, loss: 0.018485549837350845, rolling accuracy: 89.79591369628906\n",
      "TRAINING - Step: 500, loss: 0.005047619342803955, rolling accuracy: 90.00000762939453\n",
      "TRAINING - Step: 510, loss: 0.00728994095697999, rolling accuracy: 90.19608306884766\n",
      "TRAINING - Step: 520, loss: 0.014573387801647186, rolling accuracy: 90.14423370361328\n",
      "TRAINING - Step: 530, loss: 0.00928017683327198, rolling accuracy: 90.33019256591797\n",
      "TRAINING - Step: 540, loss: 0.008687404915690422, rolling accuracy: 90.5092544555664\n",
      "TRAINING - Step: 550, loss: 0.03002355247735977, rolling accuracy: 90.45454406738281\n",
      "TRAINING - Step: 560, loss: 0.02973741665482521, rolling accuracy: 90.40178680419922\n",
      "TRAINING - Step: 570, loss: 0.005021931603550911, rolling accuracy: 90.57017517089844\n",
      "TRAINING - Step: 580, loss: 0.008414855226874352, rolling accuracy: 90.73275756835938\n",
      "TRAINING - Step: 590, loss: 0.0025907610543072224, rolling accuracy: 90.88983154296875\n",
      "TRAINING - Step: 600, loss: 0.006715141702443361, rolling accuracy: 91.04167175292969\n",
      "TRAINING - Step: 610, loss: 0.027079982683062553, rolling accuracy: 91.18852233886719\n",
      "TRAINING - Step: 620, loss: 0.032273147255182266, rolling accuracy: 91.1290283203125\n",
      "TRAINING - Step: 630, loss: 0.016754306852817535, rolling accuracy: 91.26984405517578\n",
      "TRAINING - Step: 640, loss: 0.009969277307391167, rolling accuracy: 91.40625\n",
      "+++ FEDERATED MODEL 3, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.08275605738162994, rolling accuracy: 50.0\n",
      "TRAINING - Step: 20, loss: 0.08771587908267975, rolling accuracy: 56.25\n",
      "TRAINING - Step: 30, loss: 0.12356099486351013, rolling accuracy: 62.500003814697266\n",
      "TRAINING - Step: 40, loss: 0.16851340234279633, rolling accuracy: 65.625\n",
      "TRAINING - Step: 50, loss: 0.011337041854858398, rolling accuracy: 72.5\n",
      "TRAINING - Step: 60, loss: 0.05035657063126564, rolling accuracy: 75.00000762939453\n",
      "TRAINING - Step: 70, loss: 0.02450542338192463, rolling accuracy: 76.78571319580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 80, loss: 0.028262455016374588, rolling accuracy: 76.5625\n",
      "TRAINING - Step: 90, loss: 0.0599149689078331, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 100, loss: 0.01970897614955902, rolling accuracy: 80.0\n",
      "TRAINING - Step: 110, loss: 0.101010762155056, rolling accuracy: 78.40908813476562\n",
      "TRAINING - Step: 120, loss: 0.03463415428996086, rolling accuracy: 80.20833587646484\n",
      "TRAINING - Step: 130, loss: 0.08371539413928986, rolling accuracy: 78.84615325927734\n",
      "TRAINING - Step: 140, loss: 0.03627185523509979, rolling accuracy: 79.46428680419922\n",
      "TRAINING - Step: 150, loss: 0.011906055733561516, rolling accuracy: 80.83333587646484\n",
      "TRAINING - Step: 160, loss: 0.03652975335717201, rolling accuracy: 81.25\n",
      "TRAINING - Step: 170, loss: 0.020044676959514618, rolling accuracy: 82.35294342041016\n",
      "TRAINING - Step: 180, loss: 0.1106523871421814, rolling accuracy: 82.6388931274414\n",
      "TRAINING - Step: 190, loss: 0.03141968697309494, rolling accuracy: 82.89473724365234\n",
      "TRAINING - Step: 200, loss: 0.016814392060041428, rolling accuracy: 83.75\n",
      "TRAINING - Step: 210, loss: 0.017704959958791733, rolling accuracy: 84.52381134033203\n",
      "TRAINING - Step: 220, loss: 0.018783848732709885, rolling accuracy: 84.65908813476562\n",
      "TRAINING - Step: 230, loss: 0.016004879027605057, rolling accuracy: 84.78260803222656\n",
      "TRAINING - Step: 240, loss: 0.04874248802661896, rolling accuracy: 84.89583587646484\n",
      "TRAINING - Step: 250, loss: 0.05738387256860733, rolling accuracy: 84.00000762939453\n",
      "TRAINING - Step: 260, loss: 0.06855806708335876, rolling accuracy: 82.69230651855469\n",
      "TRAINING - Step: 270, loss: 0.09335596859455109, rolling accuracy: 81.48148345947266\n",
      "TRAINING - Step: 280, loss: 0.03248148784041405, rolling accuracy: 81.69642639160156\n",
      "TRAINING - Step: 290, loss: 0.01670057140290737, rolling accuracy: 81.89655303955078\n",
      "TRAINING - Step: 300, loss: 0.014324380084872246, rolling accuracy: 82.5\n",
      "TRAINING - Step: 310, loss: 0.0060987831093370914, rolling accuracy: 83.06451416015625\n",
      "TRAINING - Step: 320, loss: 0.04900065436959267, rolling accuracy: 83.59375\n",
      "TRAINING - Step: 330, loss: 0.030054796487092972, rolling accuracy: 83.71212005615234\n",
      "TRAINING - Step: 340, loss: 0.007889136672019958, rolling accuracy: 84.19117736816406\n",
      "TRAINING - Step: 350, loss: 0.0743006244301796, rolling accuracy: 83.92857360839844\n",
      "TRAINING - Step: 360, loss: 0.011686012148857117, rolling accuracy: 84.375\n",
      "TRAINING - Step: 370, loss: 0.010212332010269165, rolling accuracy: 84.79730224609375\n",
      "TRAINING - Step: 380, loss: 0.057764265686273575, rolling accuracy: 85.19737243652344\n",
      "TRAINING - Step: 390, loss: 0.4316732585430145, rolling accuracy: 84.61538696289062\n",
      "TRAINING - Step: 400, loss: 0.10801386833190918, rolling accuracy: 84.375\n",
      "TRAINING - Step: 410, loss: 0.010232209227979183, rolling accuracy: 84.75609588623047\n",
      "TRAINING - Step: 420, loss: 0.05626293644309044, rolling accuracy: 84.52381134033203\n",
      "TRAINING - Step: 430, loss: 0.03833456337451935, rolling accuracy: 84.59302520751953\n",
      "TRAINING - Step: 440, loss: 0.07278543710708618, rolling accuracy: 84.65908813476562\n",
      "TRAINING - Step: 450, loss: 0.007074163760989904, rolling accuracy: 85.0\n",
      "TRAINING - Step: 460, loss: 0.009338819421827793, rolling accuracy: 85.32608032226562\n",
      "TRAINING - Step: 470, loss: 0.010452218353748322, rolling accuracy: 85.63829803466797\n",
      "TRAINING - Step: 480, loss: 0.015544468536973, rolling accuracy: 85.93750762939453\n",
      "TRAINING - Step: 490, loss: 0.09918950498104095, rolling accuracy: 85.9693832397461\n",
      "TRAINING - Step: 500, loss: 0.007360891904681921, rolling accuracy: 86.25000762939453\n",
      "TRAINING - Step: 510, loss: 0.014272796921432018, rolling accuracy: 86.51961517333984\n",
      "TRAINING - Step: 520, loss: 0.02213551290333271, rolling accuracy: 86.53845977783203\n",
      "TRAINING - Step: 530, loss: 0.027013670653104782, rolling accuracy: 86.55660247802734\n",
      "TRAINING - Step: 540, loss: 0.09681858122348785, rolling accuracy: 86.34259033203125\n",
      "TRAINING - Step: 550, loss: 0.020180493593215942, rolling accuracy: 86.36363983154297\n",
      "TRAINING - Step: 560, loss: 0.005332328844815493, rolling accuracy: 86.60713958740234\n",
      "TRAINING - Step: 570, loss: 0.017995530739426613, rolling accuracy: 86.62281036376953\n",
      "TRAINING - Step: 580, loss: 0.025730814784765244, rolling accuracy: 86.42241668701172\n",
      "TRAINING - Step: 590, loss: 0.013714544475078583, rolling accuracy: 86.65254211425781\n",
      "TRAINING - Step: 600, loss: 0.043668925762176514, rolling accuracy: 86.66667175292969\n",
      "TRAINING - Step: 610, loss: 0.009982449933886528, rolling accuracy: 86.88524627685547\n",
      "TRAINING - Step: 620, loss: 0.10248495638370514, rolling accuracy: 87.09677124023438\n",
      "TRAINING - Step: 630, loss: 0.06941182911396027, rolling accuracy: 86.70635223388672\n",
      "TRAINING - Step: 640, loss: 0.0172310508787632, rolling accuracy: 86.9140625\n",
      "TESTING - Loss: 0.12549860775470734, Accuracy: 9.006211280822754\n",
      "+++ FEDERATED MODEL 0, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.14727148413658142, rolling accuracy: 12.5\n",
      "TRAINING - Step: 20, loss: 0.14722296595573425, rolling accuracy: 12.5\n",
      "TRAINING - Step: 30, loss: 0.06940975785255432, rolling accuracy: 8.333333969116211\n",
      "TRAINING - Step: 40, loss: 0.14736640453338623, rolling accuracy: 28.125\n",
      "TRAINING - Step: 50, loss: 0.06901422142982483, rolling accuracy: 42.5\n",
      "TRAINING - Step: 60, loss: 0.06899856775999069, rolling accuracy: 52.083335876464844\n",
      "TRAINING - Step: 70, loss: 0.06887710094451904, rolling accuracy: 58.92857360839844\n",
      "TRAINING - Step: 80, loss: 0.14743490517139435, rolling accuracy: 62.5\n",
      "TRAINING - Step: 90, loss: 0.06907583773136139, rolling accuracy: 66.66667175292969\n",
      "TRAINING - Step: 100, loss: 0.1474282294511795, rolling accuracy: 68.75\n",
      "TRAINING - Step: 110, loss: 0.06893255561590195, rolling accuracy: 71.59090423583984\n",
      "TRAINING - Step: 120, loss: 0.22598697245121002, rolling accuracy: 71.875\n",
      "TRAINING - Step: 130, loss: 0.06896500289440155, rolling accuracy: 74.03845977783203\n",
      "TRAINING - Step: 140, loss: 0.22590969502925873, rolling accuracy: 74.10713958740234\n",
      "TRAINING - Step: 150, loss: 0.06901994347572327, rolling accuracy: 75.83333587646484\n",
      "TRAINING - Step: 160, loss: 0.22590278089046478, rolling accuracy: 75.78125\n",
      "TRAINING - Step: 170, loss: 0.06901923567056656, rolling accuracy: 77.20588684082031\n",
      "TRAINING - Step: 180, loss: 0.06911006569862366, rolling accuracy: 78.47222137451172\n",
      "TRAINING - Step: 190, loss: 0.1473916471004486, rolling accuracy: 78.94737243652344\n",
      "TRAINING - Step: 200, loss: 0.1473984718322754, rolling accuracy: 79.375\n",
      "TRAINING - Step: 210, loss: 0.0692405104637146, rolling accuracy: 80.35714721679688\n",
      "TRAINING - Step: 220, loss: 0.06931006908416748, rolling accuracy: 81.25\n",
      "TRAINING - Step: 230, loss: 0.0693882405757904, rolling accuracy: 77.7173843383789\n",
      "TRAINING - Step: 240, loss: 0.06930860131978989, rolling accuracy: 78.64583587646484\n",
      "TRAINING - Step: 250, loss: 0.1473136693239212, rolling accuracy: 79.0\n",
      "TRAINING - Step: 260, loss: 0.14735999703407288, rolling accuracy: 79.32691955566406\n",
      "TRAINING - Step: 270, loss: 0.06910113990306854, rolling accuracy: 80.09259033203125\n",
      "TRAINING - Step: 280, loss: 0.06924650073051453, rolling accuracy: 80.80357360839844\n",
      "TRAINING - Step: 290, loss: 0.14732609689235687, rolling accuracy: 81.03448486328125\n",
      "TRAINING - Step: 300, loss: 0.06925754994153976, rolling accuracy: 81.66667175292969\n",
      "TRAINING - Step: 310, loss: 0.06921368092298508, rolling accuracy: 82.25806427001953\n",
      "TRAINING - Step: 320, loss: 0.0690704882144928, rolling accuracy: 82.8125\n",
      "TRAINING - Step: 330, loss: 0.06913253664970398, rolling accuracy: 83.33332824707031\n",
      "TRAINING - Step: 340, loss: 0.06908594071865082, rolling accuracy: 83.82353210449219\n",
      "TRAINING - Step: 350, loss: 0.22578316926956177, rolling accuracy: 83.57142639160156\n",
      "TRAINING - Step: 360, loss: 0.14743413031101227, rolling accuracy: 83.68055725097656\n",
      "TRAINING - Step: 370, loss: 0.14745570719242096, rolling accuracy: 83.78378295898438\n",
      "TRAINING - Step: 380, loss: 0.0690351128578186, rolling accuracy: 84.21053314208984\n",
      "TRAINING - Step: 390, loss: 0.14733785390853882, rolling accuracy: 84.29487609863281\n",
      "TRAINING - Step: 400, loss: 0.0690009593963623, rolling accuracy: 84.6875\n",
      "TRAINING - Step: 410, loss: 0.06883223354816437, rolling accuracy: 85.06097412109375\n",
      "TRAINING - Step: 420, loss: 0.06859453022480011, rolling accuracy: 85.41667175292969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 430, loss: 0.06842362880706787, rolling accuracy: 85.75581359863281\n",
      "TRAINING - Step: 440, loss: 0.1477246880531311, rolling accuracy: 85.79544830322266\n",
      "TRAINING - Step: 450, loss: 0.06865536421537399, rolling accuracy: 86.11111450195312\n",
      "TRAINING - Step: 460, loss: 0.06866416335105896, rolling accuracy: 86.41304016113281\n",
      "TRAINING - Step: 470, loss: 0.06858185678720474, rolling accuracy: 86.7021255493164\n",
      "TRAINING - Step: 480, loss: 0.14744143187999725, rolling accuracy: 86.71875762939453\n",
      "TRAINING - Step: 490, loss: 0.14763256907463074, rolling accuracy: 86.73468780517578\n",
      "TRAINING - Step: 500, loss: 0.06874167174100876, rolling accuracy: 87.00000762939453\n",
      "TRAINING - Step: 510, loss: 0.14746400713920593, rolling accuracy: 87.00981140136719\n",
      "TRAINING - Step: 520, loss: 0.0687362551689148, rolling accuracy: 87.25961303710938\n",
      "TRAINING - Step: 530, loss: 0.22614580392837524, rolling accuracy: 87.02830505371094\n",
      "TRAINING - Step: 540, loss: 0.06884197145700455, rolling accuracy: 87.26851654052734\n",
      "TRAINING - Step: 550, loss: 0.14736536145210266, rolling accuracy: 87.2727279663086\n",
      "TRAINING - Step: 560, loss: 0.06863804161548615, rolling accuracy: 87.5\n",
      "TRAINING - Step: 570, loss: 0.14760035276412964, rolling accuracy: 87.5\n",
      "TRAINING - Step: 580, loss: 0.06821201741695404, rolling accuracy: 87.71551513671875\n",
      "TRAINING - Step: 590, loss: 0.14764811098575592, rolling accuracy: 87.71186065673828\n",
      "TRAINING - Step: 600, loss: 0.06800854206085205, rolling accuracy: 87.91667175292969\n",
      "TRAINING - Step: 610, loss: 0.06794051826000214, rolling accuracy: 88.11475372314453\n",
      "TRAINING - Step: 620, loss: 0.22666221857070923, rolling accuracy: 87.9032211303711\n",
      "TRAINING - Step: 630, loss: 0.22674739360809326, rolling accuracy: 87.69841766357422\n",
      "TRAINING - Step: 640, loss: 0.06852427124977112, rolling accuracy: 87.890625\n",
      "+++ FEDERATED MODEL 1, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.14728233218193054, rolling accuracy: 12.5\n",
      "TRAINING - Step: 20, loss: 0.06929952651262283, rolling accuracy: 56.25\n",
      "TRAINING - Step: 30, loss: 0.06926308572292328, rolling accuracy: 70.83333587646484\n",
      "TRAINING - Step: 40, loss: 0.0691925659775734, rolling accuracy: 78.125\n",
      "TRAINING - Step: 50, loss: 0.14733822643756866, rolling accuracy: 80.0\n",
      "TRAINING - Step: 60, loss: 0.06908975541591644, rolling accuracy: 83.33333587646484\n",
      "TRAINING - Step: 70, loss: 0.2260054051876068, rolling accuracy: 82.14286041259766\n",
      "TRAINING - Step: 80, loss: 0.06889645010232925, rolling accuracy: 84.375\n",
      "TRAINING - Step: 90, loss: 0.06881716847419739, rolling accuracy: 86.11111450195312\n",
      "TRAINING - Step: 100, loss: 0.1475110948085785, rolling accuracy: 86.25\n",
      "TRAINING - Step: 110, loss: 0.14752937853336334, rolling accuracy: 86.36363220214844\n",
      "TRAINING - Step: 120, loss: 0.14757582545280457, rolling accuracy: 86.45833587646484\n",
      "TRAINING - Step: 130, loss: 0.22622427344322205, rolling accuracy: 85.57691955566406\n",
      "TRAINING - Step: 140, loss: 0.06894321739673615, rolling accuracy: 86.60713958740234\n",
      "TRAINING - Step: 150, loss: 0.14740633964538574, rolling accuracy: 86.66667175292969\n",
      "TRAINING - Step: 160, loss: 0.06915844231843948, rolling accuracy: 87.5\n",
      "TRAINING - Step: 170, loss: 0.06913793087005615, rolling accuracy: 88.23529815673828\n",
      "TRAINING - Step: 180, loss: 0.06897109746932983, rolling accuracy: 88.8888931274414\n",
      "TRAINING - Step: 190, loss: 0.06890075653791428, rolling accuracy: 89.47368621826172\n",
      "TRAINING - Step: 200, loss: 0.1474308967590332, rolling accuracy: 89.375\n",
      "TRAINING - Step: 210, loss: 0.06895749270915985, rolling accuracy: 89.8809585571289\n",
      "TRAINING - Step: 220, loss: 0.14743754267692566, rolling accuracy: 89.7727279663086\n",
      "TRAINING - Step: 230, loss: 0.06880795210599899, rolling accuracy: 90.2173843383789\n",
      "TRAINING - Step: 240, loss: 0.06868092715740204, rolling accuracy: 90.62500762939453\n",
      "TRAINING - Step: 250, loss: 0.14753511548042297, rolling accuracy: 90.50000762939453\n",
      "TRAINING - Step: 260, loss: 0.06871818006038666, rolling accuracy: 90.86538696289062\n",
      "TRAINING - Step: 270, loss: 0.06863778829574585, rolling accuracy: 91.20370483398438\n",
      "TRAINING - Step: 280, loss: 0.14757132530212402, rolling accuracy: 91.07142639160156\n",
      "TRAINING - Step: 290, loss: 0.06862238049507141, rolling accuracy: 91.37931060791016\n",
      "TRAINING - Step: 300, loss: 0.06863590329885483, rolling accuracy: 91.66667175292969\n",
      "TRAINING - Step: 310, loss: 0.06848808377981186, rolling accuracy: 91.93548583984375\n",
      "TRAINING - Step: 320, loss: 0.06826868653297424, rolling accuracy: 92.1875\n",
      "TRAINING - Step: 330, loss: 0.06817187368869781, rolling accuracy: 92.42424011230469\n",
      "TRAINING - Step: 340, loss: 0.227558895945549, rolling accuracy: 91.9117660522461\n",
      "TRAINING - Step: 350, loss: 0.1477571427822113, rolling accuracy: 91.78571319580078\n",
      "TRAINING - Step: 360, loss: 0.22782078385353088, rolling accuracy: 91.31945037841797\n",
      "TRAINING - Step: 370, loss: 0.06808736175298691, rolling accuracy: 91.5540542602539\n",
      "TRAINING - Step: 380, loss: 0.06826581805944443, rolling accuracy: 91.77632141113281\n",
      "TRAINING - Step: 390, loss: 0.22679555416107178, rolling accuracy: 91.34615325927734\n",
      "TRAINING - Step: 400, loss: 0.06855997443199158, rolling accuracy: 91.5625\n",
      "TRAINING - Step: 410, loss: 0.22629769146442413, rolling accuracy: 91.15853881835938\n",
      "TRAINING - Step: 420, loss: 0.22623690962791443, rolling accuracy: 90.77381134033203\n",
      "TRAINING - Step: 430, loss: 0.304995059967041, rolling accuracy: 90.11627960205078\n",
      "TRAINING - Step: 440, loss: 0.1474168300628662, rolling accuracy: 90.05681610107422\n",
      "TRAINING - Step: 450, loss: 0.06919878721237183, rolling accuracy: 90.27777862548828\n",
      "TRAINING - Step: 460, loss: 0.06922006607055664, rolling accuracy: 90.48912811279297\n",
      "TRAINING - Step: 470, loss: 0.1473451852798462, rolling accuracy: 90.42552947998047\n",
      "TRAINING - Step: 480, loss: 0.22571709752082825, rolling accuracy: 90.10417175292969\n",
      "TRAINING - Step: 490, loss: 0.06913772225379944, rolling accuracy: 90.30612182617188\n",
      "TRAINING - Step: 500, loss: 0.069106325507164, rolling accuracy: 90.50000762939453\n",
      "TRAINING - Step: 510, loss: 0.06897339224815369, rolling accuracy: 90.686279296875\n",
      "TRAINING - Step: 520, loss: 0.14741221070289612, rolling accuracy: 90.625\n",
      "TRAINING - Step: 530, loss: 0.06881947815418243, rolling accuracy: 90.80188751220703\n",
      "TRAINING - Step: 540, loss: 0.06872404366731644, rolling accuracy: 90.97222137451172\n",
      "TRAINING - Step: 550, loss: 0.22622108459472656, rolling accuracy: 90.68181610107422\n",
      "TRAINING - Step: 560, loss: 0.06886762380599976, rolling accuracy: 90.84821319580078\n",
      "TRAINING - Step: 570, loss: 0.14761939644813538, rolling accuracy: 90.78947448730469\n",
      "TRAINING - Step: 580, loss: 0.22652438282966614, rolling accuracy: 90.51724243164062\n",
      "TRAINING - Step: 590, loss: 0.06868597865104675, rolling accuracy: 90.67796325683594\n",
      "TRAINING - Step: 600, loss: 0.14767415821552277, rolling accuracy: 90.625\n",
      "TRAINING - Step: 610, loss: 0.14766451716423035, rolling accuracy: 90.57376861572266\n",
      "TRAINING - Step: 620, loss: 0.14747291803359985, rolling accuracy: 90.5241928100586\n",
      "TRAINING - Step: 630, loss: 0.1475035548210144, rolling accuracy: 90.47618865966797\n",
      "TRAINING - Step: 640, loss: 0.1474321484565735, rolling accuracy: 90.4296875\n",
      "+++ FEDERATED MODEL 2, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.06941064447164536, rolling accuracy: 0.0\n",
      "TRAINING - Step: 20, loss: 0.06947614252567291, rolling accuracy: 0.0\n",
      "TRAINING - Step: 30, loss: 0.14719226956367493, rolling accuracy: 4.1666669845581055\n",
      "TRAINING - Step: 40, loss: 0.06976787000894547, rolling accuracy: 3.125\n",
      "TRAINING - Step: 50, loss: 0.06999817490577698, rolling accuracy: 2.5\n",
      "TRAINING - Step: 60, loss: 0.0699971467256546, rolling accuracy: 2.0833334922790527\n",
      "TRAINING - Step: 70, loss: 0.06984009593725204, rolling accuracy: 1.7857142686843872\n",
      "TRAINING - Step: 80, loss: 0.06976829469203949, rolling accuracy: 1.5625\n",
      "TRAINING - Step: 90, loss: 0.06975507736206055, rolling accuracy: 1.388888955116272\n",
      "TRAINING - Step: 100, loss: 0.14720208942890167, rolling accuracy: 2.5\n",
      "TRAINING - Step: 110, loss: 0.0695800855755806, rolling accuracy: 2.2727272510528564\n",
      "TRAINING - Step: 120, loss: 0.14714783430099487, rolling accuracy: 3.125000238418579\n",
      "TRAINING - Step: 130, loss: 0.06962980329990387, rolling accuracy: 2.884615421295166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 140, loss: 0.06955083459615707, rolling accuracy: 2.6785714626312256\n",
      "TRAINING - Step: 150, loss: 0.06946998089551926, rolling accuracy: 2.5\n",
      "TRAINING - Step: 160, loss: 0.0692499652504921, rolling accuracy: 8.59375\n",
      "TRAINING - Step: 170, loss: 0.14727430045604706, rolling accuracy: 13.235294342041016\n",
      "TRAINING - Step: 180, loss: 0.14731259644031525, rolling accuracy: 17.36111068725586\n",
      "TRAINING - Step: 190, loss: 0.147367924451828, rolling accuracy: 21.05263328552246\n",
      "TRAINING - Step: 200, loss: 0.14737215638160706, rolling accuracy: 24.375\n",
      "TRAINING - Step: 210, loss: 0.30348464846611023, rolling accuracy: 26.19047737121582\n",
      "TRAINING - Step: 220, loss: 0.14719349145889282, rolling accuracy: 25.568180084228516\n",
      "TRAINING - Step: 230, loss: 0.0695032924413681, rolling accuracy: 24.456520080566406\n",
      "TRAINING - Step: 240, loss: 0.14723321795463562, rolling accuracy: 23.95833396911621\n",
      "TRAINING - Step: 250, loss: 0.14725515246391296, rolling accuracy: 23.500001907348633\n",
      "TRAINING - Step: 260, loss: 0.14723652601242065, rolling accuracy: 23.076923370361328\n",
      "TRAINING - Step: 270, loss: 0.22484008967876434, rolling accuracy: 23.148147583007812\n",
      "TRAINING - Step: 280, loss: 0.14728417992591858, rolling accuracy: 22.76785659790039\n",
      "TRAINING - Step: 290, loss: 0.1471475511789322, rolling accuracy: 22.413793563842773\n",
      "TRAINING - Step: 300, loss: 0.2250385284423828, rolling accuracy: 22.5\n",
      "TRAINING - Step: 310, loss: 0.22513021528720856, rolling accuracy: 22.983871459960938\n",
      "TRAINING - Step: 320, loss: 0.06942719221115112, rolling accuracy: 22.265625\n",
      "TRAINING - Step: 330, loss: 0.14733991026878357, rolling accuracy: 21.969696044921875\n",
      "TRAINING - Step: 340, loss: 0.14726075530052185, rolling accuracy: 23.161766052246094\n",
      "TRAINING - Step: 350, loss: 0.22570273280143738, rolling accuracy: 24.64285659790039\n",
      "TRAINING - Step: 360, loss: 0.06892502307891846, rolling accuracy: 26.736112594604492\n",
      "TRAINING - Step: 370, loss: 0.1475583016872406, rolling accuracy: 28.378379821777344\n",
      "TRAINING - Step: 380, loss: 0.06889968365430832, rolling accuracy: 30.263158798217773\n",
      "TRAINING - Step: 390, loss: 0.06912481784820557, rolling accuracy: 32.05128479003906\n",
      "TRAINING - Step: 400, loss: 0.06939718127250671, rolling accuracy: 31.5625\n",
      "TRAINING - Step: 410, loss: 0.14723879098892212, rolling accuracy: 31.09756088256836\n",
      "TRAINING - Step: 420, loss: 0.06985654681921005, rolling accuracy: 30.35714340209961\n",
      "TRAINING - Step: 430, loss: 0.06990909576416016, rolling accuracy: 29.65116310119629\n",
      "TRAINING - Step: 440, loss: 0.14696042239665985, rolling accuracy: 29.261362075805664\n",
      "TRAINING - Step: 450, loss: 0.06995335966348648, rolling accuracy: 28.611112594604492\n",
      "TRAINING - Step: 460, loss: 0.22473733127117157, rolling accuracy: 28.532608032226562\n",
      "TRAINING - Step: 470, loss: 0.14700621366500854, rolling accuracy: 28.19148826599121\n",
      "TRAINING - Step: 480, loss: 0.06982392817735672, rolling accuracy: 27.604167938232422\n",
      "TRAINING - Step: 490, loss: 0.1469736248254776, rolling accuracy: 27.295917510986328\n",
      "TRAINING - Step: 500, loss: 0.0698327049612999, rolling accuracy: 26.750001907348633\n",
      "TRAINING - Step: 510, loss: 0.2240985929965973, rolling accuracy: 26.715688705444336\n",
      "TRAINING - Step: 520, loss: 0.22470581531524658, rolling accuracy: 26.68269157409668\n",
      "TRAINING - Step: 530, loss: 0.06950822472572327, rolling accuracy: 26.41509437561035\n",
      "TRAINING - Step: 540, loss: 0.06924930214881897, rolling accuracy: 26.851850509643555\n",
      "TRAINING - Step: 550, loss: 0.06900359690189362, rolling accuracy: 28.18181800842285\n",
      "TRAINING - Step: 560, loss: 0.06888279318809509, rolling accuracy: 29.46428680419922\n",
      "TRAINING - Step: 570, loss: 0.06885772943496704, rolling accuracy: 30.70175552368164\n",
      "TRAINING - Step: 580, loss: 0.06886903196573257, rolling accuracy: 31.89655113220215\n",
      "TRAINING - Step: 590, loss: 0.06878753751516342, rolling accuracy: 33.050846099853516\n",
      "TRAINING - Step: 600, loss: 0.06866839528083801, rolling accuracy: 34.16666793823242\n",
      "TRAINING - Step: 610, loss: 0.22661994397640228, rolling accuracy: 34.83606719970703\n",
      "TRAINING - Step: 620, loss: 0.06816188991069794, rolling accuracy: 35.8870964050293\n",
      "TRAINING - Step: 630, loss: 0.06779973208904266, rolling accuracy: 36.904762268066406\n",
      "TRAINING - Step: 640, loss: 0.1466914713382721, rolling accuracy: 37.6953125\n",
      "+++ FEDERATED MODEL 3, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.2252953052520752, rolling accuracy: 75.0\n",
      "TRAINING - Step: 20, loss: 0.14725899696350098, rolling accuracy: 43.75\n",
      "TRAINING - Step: 30, loss: 0.06959185004234314, rolling accuracy: 29.166667938232422\n",
      "TRAINING - Step: 40, loss: 0.06944823265075684, rolling accuracy: 21.875\n",
      "TRAINING - Step: 50, loss: 0.06920826435089111, rolling accuracy: 37.5\n",
      "TRAINING - Step: 60, loss: 0.06916110217571259, rolling accuracy: 47.91666793823242\n",
      "TRAINING - Step: 70, loss: 0.1472984254360199, rolling accuracy: 53.57143020629883\n",
      "TRAINING - Step: 80, loss: 0.06947724521160126, rolling accuracy: 46.875\n",
      "TRAINING - Step: 90, loss: 0.06959268450737, rolling accuracy: 41.66666793823242\n",
      "TRAINING - Step: 100, loss: 0.06975428760051727, rolling accuracy: 37.5\n",
      "TRAINING - Step: 110, loss: 0.06969974935054779, rolling accuracy: 34.09090805053711\n",
      "TRAINING - Step: 120, loss: 0.14723354578018188, rolling accuracy: 32.29166793823242\n",
      "TRAINING - Step: 130, loss: 0.06932838261127472, rolling accuracy: 31.730770111083984\n",
      "TRAINING - Step: 140, loss: 0.06913671642541885, rolling accuracy: 36.60714340209961\n",
      "TRAINING - Step: 150, loss: 0.2257300615310669, rolling accuracy: 39.16666793823242\n",
      "TRAINING - Step: 160, loss: 0.06890152394771576, rolling accuracy: 42.96875\n",
      "TRAINING - Step: 170, loss: 0.06876012682914734, rolling accuracy: 46.32353210449219\n",
      "TRAINING - Step: 180, loss: 0.06872523576021194, rolling accuracy: 49.30555725097656\n",
      "TRAINING - Step: 190, loss: 0.1475714147090912, rolling accuracy: 51.315792083740234\n",
      "TRAINING - Step: 200, loss: 0.30596303939819336, rolling accuracy: 51.875\n",
      "TRAINING - Step: 210, loss: 0.14763441681861877, rolling accuracy: 53.57143020629883\n",
      "TRAINING - Step: 220, loss: 0.06870340555906296, rolling accuracy: 55.68181610107422\n",
      "TRAINING - Step: 230, loss: 0.06886593997478485, rolling accuracy: 57.60869216918945\n",
      "TRAINING - Step: 240, loss: 0.06890655308961868, rolling accuracy: 59.375003814697266\n",
      "TRAINING - Step: 250, loss: 0.30482497811317444, rolling accuracy: 59.500003814697266\n",
      "TRAINING - Step: 260, loss: 0.06893104314804077, rolling accuracy: 61.05769348144531\n",
      "TRAINING - Step: 270, loss: 0.0690697580575943, rolling accuracy: 62.5\n",
      "TRAINING - Step: 280, loss: 0.06910276412963867, rolling accuracy: 63.83928680419922\n",
      "TRAINING - Step: 290, loss: 0.1474008560180664, rolling accuracy: 64.6551742553711\n",
      "TRAINING - Step: 300, loss: 0.06897597014904022, rolling accuracy: 65.83333587646484\n",
      "TRAINING - Step: 310, loss: 0.06890303641557693, rolling accuracy: 66.93548583984375\n",
      "TRAINING - Step: 320, loss: 0.14734971523284912, rolling accuracy: 67.578125\n",
      "TRAINING - Step: 330, loss: 0.22536146640777588, rolling accuracy: 67.80303192138672\n",
      "TRAINING - Step: 340, loss: 0.14724527299404144, rolling accuracy: 68.75\n",
      "TRAINING - Step: 350, loss: 0.06933523714542389, rolling accuracy: 67.5\n",
      "TRAINING - Step: 360, loss: 0.14723527431488037, rolling accuracy: 65.97222137451172\n",
      "TRAINING - Step: 370, loss: 0.22528840601444244, rolling accuracy: 66.21621704101562\n",
      "TRAINING - Step: 380, loss: 0.06916587054729462, rolling accuracy: 67.10526275634766\n",
      "TRAINING - Step: 390, loss: 0.06909788399934769, rolling accuracy: 67.94872283935547\n",
      "TRAINING - Step: 400, loss: 0.06910784542560577, rolling accuracy: 68.75\n",
      "TRAINING - Step: 410, loss: 0.0690566897392273, rolling accuracy: 69.51219177246094\n",
      "TRAINING - Step: 420, loss: 0.14736688137054443, rolling accuracy: 69.94047546386719\n",
      "TRAINING - Step: 430, loss: 0.14740091562271118, rolling accuracy: 70.34883880615234\n",
      "TRAINING - Step: 440, loss: 0.06874813139438629, rolling accuracy: 71.0227279663086\n",
      "TRAINING - Step: 450, loss: 0.1473541557788849, rolling accuracy: 71.3888931274414\n",
      "TRAINING - Step: 460, loss: 0.22631561756134033, rolling accuracy: 71.4673843383789\n",
      "TRAINING - Step: 470, loss: 0.06870678067207336, rolling accuracy: 72.074462890625\n",
      "TRAINING - Step: 480, loss: 0.06855528801679611, rolling accuracy: 72.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 490, loss: 0.14750966429710388, rolling accuracy: 72.95918273925781\n",
      "TRAINING - Step: 500, loss: 0.06837034225463867, rolling accuracy: 73.5\n",
      "TRAINING - Step: 510, loss: 0.14746274054050446, rolling accuracy: 73.7745132446289\n",
      "TRAINING - Step: 520, loss: 0.14759117364883423, rolling accuracy: 74.03845977783203\n",
      "TRAINING - Step: 530, loss: 0.14744383096694946, rolling accuracy: 74.2924575805664\n",
      "TRAINING - Step: 540, loss: 0.1475229263305664, rolling accuracy: 74.53703308105469\n",
      "TRAINING - Step: 550, loss: 0.14794638752937317, rolling accuracy: 74.7727279663086\n",
      "TRAINING - Step: 560, loss: 0.14751310646533966, rolling accuracy: 75.0\n",
      "TRAINING - Step: 570, loss: 0.14789634943008423, rolling accuracy: 75.21929931640625\n",
      "TRAINING - Step: 580, loss: 0.06831462681293488, rolling accuracy: 75.64655303955078\n",
      "TRAINING - Step: 590, loss: 0.14761953055858612, rolling accuracy: 75.84745788574219\n",
      "TRAINING - Step: 600, loss: 0.1473805010318756, rolling accuracy: 76.04167175292969\n",
      "TRAINING - Step: 610, loss: 0.14785823225975037, rolling accuracy: 76.22950744628906\n",
      "TRAINING - Step: 620, loss: 0.22679035365581512, rolling accuracy: 76.20967864990234\n",
      "TRAINING - Step: 630, loss: 0.22628557682037354, rolling accuracy: 76.19047546386719\n",
      "TRAINING - Step: 640, loss: 0.14745423197746277, rolling accuracy: 76.3671875\n",
      "TESTING - Loss: 440368533340160.0, Accuracy: 9.006211280822754\n",
      "+++ FEDERATED MODEL 0, EPOCH: 3 +++++++++\n",
      "TRAINING - Step: 10, loss: 6.804784297943115, rolling accuracy: 62.5\n",
      "TRAINING - Step: 20, loss: 6.338259220123291, rolling accuracy: 62.5\n",
      "TRAINING - Step: 30, loss: 5.133473873138428, rolling accuracy: 66.66667175292969\n",
      "TRAINING - Step: 40, loss: 4.743300437927246, rolling accuracy: 65.625\n",
      "TRAINING - Step: 50, loss: 4.11572265625, rolling accuracy: 67.5\n",
      "TRAINING - Step: 60, loss: 3.642683982849121, rolling accuracy: 68.75\n",
      "TRAINING - Step: 70, loss: 3.3112027645111084, rolling accuracy: 71.42857360839844\n",
      "TRAINING - Step: 80, loss: 2.902331829071045, rolling accuracy: 70.3125\n",
      "TRAINING - Step: 90, loss: 2.564910888671875, rolling accuracy: 72.22222137451172\n",
      "TRAINING - Step: 100, loss: 1.853691577911377, rolling accuracy: 72.5\n",
      "TRAINING - Step: 110, loss: 1.6521427631378174, rolling accuracy: 72.7272720336914\n",
      "TRAINING - Step: 120, loss: 1.533903956413269, rolling accuracy: 72.91667175292969\n",
      "TRAINING - Step: 130, loss: 1.3423792123794556, rolling accuracy: 73.07691955566406\n",
      "TRAINING - Step: 140, loss: 1.3674583435058594, rolling accuracy: 73.21428680419922\n",
      "TRAINING - Step: 150, loss: 0.8940597772598267, rolling accuracy: 73.33333587646484\n",
      "TRAINING - Step: 160, loss: 0.5789521932601929, rolling accuracy: 73.4375\n",
      "TRAINING - Step: 170, loss: 0.5248339772224426, rolling accuracy: 73.52941131591797\n",
      "TRAINING - Step: 180, loss: 0.582897424697876, rolling accuracy: 72.91667175292969\n",
      "TRAINING - Step: 190, loss: 0.28927168250083923, rolling accuracy: 73.68421173095703\n",
      "TRAINING - Step: 200, loss: 0.0826299637556076, rolling accuracy: 74.375\n",
      "TRAINING - Step: 210, loss: 0.4325169622898102, rolling accuracy: 73.80952453613281\n",
      "TRAINING - Step: 220, loss: 0.056491076946258545, rolling accuracy: 75.0\n",
      "TRAINING - Step: 230, loss: 0.04915598779916763, rolling accuracy: 76.08695220947266\n",
      "TRAINING - Step: 240, loss: 0.05693197622895241, rolling accuracy: 77.08333587646484\n",
      "TRAINING - Step: 250, loss: 0.1576915681362152, rolling accuracy: 77.0\n",
      "TRAINING - Step: 260, loss: 0.14109794795513153, rolling accuracy: 77.40384674072266\n",
      "TRAINING - Step: 270, loss: 0.05741499736905098, rolling accuracy: 78.24073791503906\n",
      "TRAINING - Step: 280, loss: 0.1436454951763153, rolling accuracy: 78.57142639160156\n",
      "TRAINING - Step: 290, loss: 0.22400540113449097, rolling accuracy: 78.44827270507812\n",
      "TRAINING - Step: 300, loss: 0.1481705605983734, rolling accuracy: 78.75\n",
      "TRAINING - Step: 310, loss: 0.26151880621910095, rolling accuracy: 78.2258071899414\n",
      "TRAINING - Step: 320, loss: 0.20319929718971252, rolling accuracy: 78.125\n",
      "TRAINING - Step: 330, loss: 0.3001561164855957, rolling accuracy: 77.2727279663086\n",
      "TRAINING - Step: 340, loss: 0.14010758697986603, rolling accuracy: 77.57353210449219\n",
      "TRAINING - Step: 350, loss: 0.05914107337594032, rolling accuracy: 78.21428680419922\n",
      "TRAINING - Step: 360, loss: 0.058852970600128174, rolling accuracy: 78.81944274902344\n",
      "TRAINING - Step: 370, loss: 0.1930452287197113, rolling accuracy: 78.37837982177734\n",
      "TRAINING - Step: 380, loss: 1.262550950050354, rolling accuracy: 78.61842346191406\n",
      "TRAINING - Step: 390, loss: 0.16893523931503296, rolling accuracy: 78.52564239501953\n",
      "TRAINING - Step: 400, loss: 0.059047453105449677, rolling accuracy: 79.0625\n",
      "TRAINING - Step: 410, loss: 0.1435079574584961, rolling accuracy: 78.96341705322266\n",
      "TRAINING - Step: 420, loss: 0.058812063187360764, rolling accuracy: 79.46428680419922\n",
      "TRAINING - Step: 430, loss: 0.05895419046282768, rolling accuracy: 79.65116119384766\n",
      "TRAINING - Step: 440, loss: 0.22170689702033997, rolling accuracy: 79.54544830322266\n",
      "TRAINING - Step: 450, loss: 0.1976945996284485, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 460, loss: 0.22382906079292297, rolling accuracy: 78.8043441772461\n",
      "TRAINING - Step: 470, loss: 0.244711771607399, rolling accuracy: 78.19149017333984\n",
      "TRAINING - Step: 480, loss: 0.17619198560714722, rolling accuracy: 77.86458587646484\n",
      "TRAINING - Step: 490, loss: 0.06117204576730728, rolling accuracy: 78.06121826171875\n",
      "TRAINING - Step: 500, loss: 0.06139941141009331, rolling accuracy: 77.75\n",
      "TRAINING - Step: 510, loss: 0.07257139682769775, rolling accuracy: 77.45098876953125\n",
      "TRAINING - Step: 520, loss: 0.06091960519552231, rolling accuracy: 77.64423370361328\n",
      "TRAINING - Step: 530, loss: 0.1310950517654419, rolling accuracy: 77.83019256591797\n",
      "TRAINING - Step: 540, loss: 0.9988076686859131, rolling accuracy: 77.77777862548828\n",
      "TRAINING - Step: 550, loss: 0.14151518046855927, rolling accuracy: 77.5\n",
      "TRAINING - Step: 560, loss: 0.05929207056760788, rolling accuracy: 77.90178680419922\n",
      "TRAINING - Step: 570, loss: 0.0518982820212841, rolling accuracy: 78.07017517089844\n",
      "TRAINING - Step: 580, loss: 0.06227172911167145, rolling accuracy: 78.23275756835938\n",
      "TRAINING - Step: 590, loss: 0.05367695540189743, rolling accuracy: 78.38983154296875\n",
      "TRAINING - Step: 600, loss: 0.0634552389383316, rolling accuracy: 78.33333587646484\n",
      "TRAINING - Step: 610, loss: 0.05867377668619156, rolling accuracy: 78.48360443115234\n",
      "TRAINING - Step: 620, loss: 0.11426448076963425, rolling accuracy: 78.42742156982422\n",
      "TRAINING - Step: 630, loss: 0.04976474121212959, rolling accuracy: 78.76984405517578\n",
      "TRAINING - Step: 640, loss: 0.09013582020998001, rolling accuracy: 78.7109375\n",
      "+++ FEDERATED MODEL 1, EPOCH: 3 +++++++++\n",
      "TRAINING - Step: 10, loss: 7.132652759552002, rolling accuracy: 75.0\n",
      "TRAINING - Step: 20, loss: 6.112869739532471, rolling accuracy: 81.25\n",
      "TRAINING - Step: 30, loss: 5.701635837554932, rolling accuracy: 83.33333587646484\n",
      "TRAINING - Step: 40, loss: 4.617551803588867, rolling accuracy: 78.125\n",
      "TRAINING - Step: 50, loss: 4.526793479919434, rolling accuracy: 72.5\n",
      "TRAINING - Step: 60, loss: 3.9419517517089844, rolling accuracy: 72.91667175292969\n",
      "TRAINING - Step: 70, loss: 0.21910759806632996, rolling accuracy: 71.42857360839844\n",
      "TRAINING - Step: 80, loss: 2.7499706745147705, rolling accuracy: 70.3125\n",
      "TRAINING - Step: 90, loss: 2.766157388687134, rolling accuracy: 70.83333587646484\n",
      "TRAINING - Step: 100, loss: 1.7912869453430176, rolling accuracy: 72.5\n",
      "TRAINING - Step: 110, loss: 2.0320894718170166, rolling accuracy: 71.59090423583984\n",
      "TRAINING - Step: 120, loss: 1.5461093187332153, rolling accuracy: 72.91667175292969\n",
      "TRAINING - Step: 130, loss: 1.1940594911575317, rolling accuracy: 73.07691955566406\n",
      "TRAINING - Step: 140, loss: 1.0965538024902344, rolling accuracy: 74.10713958740234\n",
      "TRAINING - Step: 150, loss: 0.9991765022277832, rolling accuracy: 73.33333587646484\n",
      "TRAINING - Step: 160, loss: 0.7734891176223755, rolling accuracy: 72.65625\n",
      "TRAINING - Step: 170, loss: 0.48559898138046265, rolling accuracy: 72.79412078857422\n",
      "TRAINING - Step: 180, loss: 0.1542525291442871, rolling accuracy: 71.52777862548828\n",
      "TRAINING - Step: 190, loss: 0.05594808980822563, rolling accuracy: 72.36842346191406\n",
      "TRAINING - Step: 200, loss: 0.15214698016643524, rolling accuracy: 72.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 210, loss: 0.06326134502887726, rolling accuracy: 73.21428680419922\n",
      "TRAINING - Step: 220, loss: 0.05593350902199745, rolling accuracy: 74.43181610107422\n",
      "TRAINING - Step: 230, loss: 0.36827027797698975, rolling accuracy: 74.4565200805664\n",
      "TRAINING - Step: 240, loss: 0.14862331748008728, rolling accuracy: 75.00000762939453\n",
      "TRAINING - Step: 250, loss: 0.04954316467046738, rolling accuracy: 76.0\n",
      "TRAINING - Step: 260, loss: 0.1444822996854782, rolling accuracy: 76.44230651855469\n",
      "TRAINING - Step: 270, loss: 0.14565879106521606, rolling accuracy: 76.38888549804688\n",
      "TRAINING - Step: 280, loss: 0.05785021185874939, rolling accuracy: 76.78571319580078\n",
      "TRAINING - Step: 290, loss: 0.04486551135778427, rolling accuracy: 77.5862045288086\n",
      "TRAINING - Step: 300, loss: 0.2688705325126648, rolling accuracy: 77.08333587646484\n",
      "TRAINING - Step: 310, loss: 0.05311489850282669, rolling accuracy: 77.82257843017578\n",
      "TRAINING - Step: 320, loss: 0.15565015375614166, rolling accuracy: 77.734375\n",
      "TRAINING - Step: 330, loss: 0.07138849049806595, rolling accuracy: 77.2727279663086\n",
      "TRAINING - Step: 340, loss: 0.04562573507428169, rolling accuracy: 77.94117736816406\n",
      "TRAINING - Step: 350, loss: 0.17409619688987732, rolling accuracy: 77.85713958740234\n",
      "TRAINING - Step: 360, loss: 0.04586299881339073, rolling accuracy: 78.47222137451172\n",
      "TRAINING - Step: 370, loss: 0.23894721269607544, rolling accuracy: 78.37837982177734\n",
      "TRAINING - Step: 380, loss: 0.056595876812934875, rolling accuracy: 78.61842346191406\n",
      "TRAINING - Step: 390, loss: 0.05534351244568825, rolling accuracy: 78.84615325927734\n",
      "TRAINING - Step: 400, loss: 0.23888236284255981, rolling accuracy: 78.4375\n",
      "TRAINING - Step: 410, loss: 0.24314527213573456, rolling accuracy: 78.04878234863281\n",
      "TRAINING - Step: 420, loss: 0.16819384694099426, rolling accuracy: 77.9761962890625\n",
      "TRAINING - Step: 430, loss: 0.0506269708275795, rolling accuracy: 78.48837280273438\n",
      "TRAINING - Step: 440, loss: 0.09272977709770203, rolling accuracy: 78.69317626953125\n",
      "TRAINING - Step: 450, loss: 0.05408661812543869, rolling accuracy: 79.16667175292969\n",
      "TRAINING - Step: 460, loss: 0.06448661535978317, rolling accuracy: 79.34782409667969\n",
      "TRAINING - Step: 470, loss: 0.05941867455840111, rolling accuracy: 79.7872314453125\n",
      "TRAINING - Step: 480, loss: 0.16259515285491943, rolling accuracy: 79.42708587646484\n",
      "TRAINING - Step: 490, loss: 0.06049669533967972, rolling accuracy: 79.59183502197266\n",
      "TRAINING - Step: 500, loss: 0.06279922276735306, rolling accuracy: 79.5\n",
      "TRAINING - Step: 510, loss: 0.05920898914337158, rolling accuracy: 79.4117660522461\n",
      "TRAINING - Step: 520, loss: 0.3363460302352905, rolling accuracy: 79.08654022216797\n",
      "TRAINING - Step: 530, loss: 0.06765986979007721, rolling accuracy: 79.00943756103516\n",
      "TRAINING - Step: 540, loss: 0.06616958975791931, rolling accuracy: 79.16666412353516\n",
      "TRAINING - Step: 550, loss: 0.22124144434928894, rolling accuracy: 78.86363983154297\n",
      "TRAINING - Step: 560, loss: 0.1436993032693863, rolling accuracy: 78.79463958740234\n",
      "TRAINING - Step: 570, loss: 0.06746670603752136, rolling accuracy: 78.72807312011719\n",
      "TRAINING - Step: 580, loss: 0.05778162181377411, rolling accuracy: 79.0948257446289\n",
      "TRAINING - Step: 590, loss: 0.23425763845443726, rolling accuracy: 79.02542114257812\n",
      "TRAINING - Step: 600, loss: 0.14831162989139557, rolling accuracy: 78.95833587646484\n",
      "TRAINING - Step: 610, loss: 0.06684654951095581, rolling accuracy: 79.09835815429688\n",
      "TRAINING - Step: 620, loss: 0.06880049407482147, rolling accuracy: 79.03225708007812\n",
      "TRAINING - Step: 630, loss: 0.063033327460289, rolling accuracy: 79.16666412353516\n",
      "TRAINING - Step: 640, loss: 0.1456175297498703, rolling accuracy: 79.296875\n",
      "+++ FEDERATED MODEL 2, EPOCH: 3 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.06086057052016258, rolling accuracy: 87.5\n",
      "TRAINING - Step: 20, loss: 6.453845024108887, rolling accuracy: 43.75\n",
      "TRAINING - Step: 30, loss: 0.4215735197067261, rolling accuracy: 58.333335876464844\n",
      "TRAINING - Step: 40, loss: 4.945441722869873, rolling accuracy: 65.625\n",
      "TRAINING - Step: 50, loss: 4.026015281677246, rolling accuracy: 67.5\n",
      "TRAINING - Step: 60, loss: 3.761061429977417, rolling accuracy: 66.66667175292969\n",
      "TRAINING - Step: 70, loss: 3.321012020111084, rolling accuracy: 69.64286041259766\n",
      "TRAINING - Step: 80, loss: 2.9808268547058105, rolling accuracy: 67.1875\n",
      "TRAINING - Step: 90, loss: 2.0823044776916504, rolling accuracy: 68.05555725097656\n",
      "TRAINING - Step: 100, loss: 2.13590931892395, rolling accuracy: 70.0\n",
      "TRAINING - Step: 110, loss: 1.4640185832977295, rolling accuracy: 71.59090423583984\n",
      "TRAINING - Step: 120, loss: 1.5286513566970825, rolling accuracy: 70.83333587646484\n",
      "TRAINING - Step: 130, loss: 1.2971727848052979, rolling accuracy: 72.11538696289062\n",
      "TRAINING - Step: 140, loss: 0.9847633838653564, rolling accuracy: 73.21428680419922\n",
      "TRAINING - Step: 150, loss: 0.47402864694595337, rolling accuracy: 73.33333587646484\n",
      "TRAINING - Step: 160, loss: 0.05480174347758293, rolling accuracy: 75.0\n",
      "TRAINING - Step: 170, loss: 0.45326295495033264, rolling accuracy: 72.79412078857422\n",
      "TRAINING - Step: 180, loss: 0.10265891253948212, rolling accuracy: 74.30555725097656\n",
      "TRAINING - Step: 190, loss: 0.1207129955291748, rolling accuracy: 75.0\n",
      "TRAINING - Step: 200, loss: 0.15297839045524597, rolling accuracy: 74.375\n",
      "TRAINING - Step: 210, loss: 0.052574582397937775, rolling accuracy: 75.5952377319336\n",
      "TRAINING - Step: 220, loss: 0.47361481189727783, rolling accuracy: 75.56817626953125\n",
      "TRAINING - Step: 230, loss: 0.060530878603458405, rolling accuracy: 76.08695220947266\n",
      "TRAINING - Step: 240, loss: 0.07298554480075836, rolling accuracy: 76.04167175292969\n",
      "TRAINING - Step: 250, loss: 0.0599016398191452, rolling accuracy: 76.5\n",
      "TRAINING - Step: 260, loss: 0.25572121143341064, rolling accuracy: 75.48076629638672\n",
      "TRAINING - Step: 270, loss: 0.15483501553535461, rolling accuracy: 75.46295928955078\n",
      "TRAINING - Step: 280, loss: 0.22234131395816803, rolling accuracy: 75.0\n",
      "TRAINING - Step: 290, loss: 0.05845194309949875, rolling accuracy: 75.86206817626953\n",
      "TRAINING - Step: 300, loss: 0.06080970913171768, rolling accuracy: 76.25\n",
      "TRAINING - Step: 310, loss: 0.06253720819950104, rolling accuracy: 76.61289978027344\n",
      "TRAINING - Step: 320, loss: 0.166121706366539, rolling accuracy: 76.5625\n",
      "TRAINING - Step: 330, loss: 0.06455273926258087, rolling accuracy: 76.51515197753906\n",
      "TRAINING - Step: 340, loss: 0.16240061819553375, rolling accuracy: 76.10294342041016\n",
      "TRAINING - Step: 350, loss: 0.2641993463039398, rolling accuracy: 75.35713958740234\n",
      "TRAINING - Step: 360, loss: 0.16284745931625366, rolling accuracy: 75.34722137451172\n",
      "TRAINING - Step: 370, loss: 0.24964120984077454, rolling accuracy: 75.0\n",
      "TRAINING - Step: 380, loss: 0.054609086364507675, rolling accuracy: 75.65789794921875\n",
      "TRAINING - Step: 390, loss: 0.062173355370759964, rolling accuracy: 75.96154022216797\n",
      "TRAINING - Step: 400, loss: 0.1621810346841812, rolling accuracy: 75.625\n",
      "TRAINING - Step: 410, loss: 0.059343621134757996, rolling accuracy: 75.91463470458984\n",
      "TRAINING - Step: 420, loss: 0.23761728405952454, rolling accuracy: 75.89286041259766\n",
      "TRAINING - Step: 430, loss: 0.24747881293296814, rolling accuracy: 75.5813980102539\n",
      "TRAINING - Step: 440, loss: 0.23754732310771942, rolling accuracy: 75.28408813476562\n",
      "TRAINING - Step: 450, loss: 0.21706360578536987, rolling accuracy: 75.55555725097656\n",
      "TRAINING - Step: 460, loss: 0.14945662021636963, rolling accuracy: 75.54347229003906\n",
      "TRAINING - Step: 470, loss: 0.06301885843276978, rolling accuracy: 75.79786682128906\n",
      "TRAINING - Step: 480, loss: 0.07861541211605072, rolling accuracy: 75.78125762939453\n",
      "TRAINING - Step: 490, loss: 0.07040715962648392, rolling accuracy: 76.0204086303711\n",
      "TRAINING - Step: 500, loss: 0.06805963814258575, rolling accuracy: 76.25\n",
      "TRAINING - Step: 510, loss: 0.07302296161651611, rolling accuracy: 76.22549438476562\n",
      "TRAINING - Step: 520, loss: 0.07308730483055115, rolling accuracy: 76.20191955566406\n",
      "TRAINING - Step: 530, loss: 0.14682665467262268, rolling accuracy: 76.41509246826172\n",
      "TRAINING - Step: 540, loss: 0.06514178216457367, rolling accuracy: 76.62036895751953\n",
      "TRAINING - Step: 550, loss: 0.06800800561904907, rolling accuracy: 76.81818389892578\n",
      "TRAINING - Step: 560, loss: 0.0697869211435318, rolling accuracy: 76.78571319580078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 570, loss: 0.0633307546377182, rolling accuracy: 77.19298553466797\n",
      "TRAINING - Step: 580, loss: 0.2656254172325134, rolling accuracy: 77.1551742553711\n",
      "TRAINING - Step: 590, loss: 0.0649385154247284, rolling accuracy: 77.54237365722656\n",
      "TRAINING - Step: 600, loss: 0.07185317575931549, rolling accuracy: 77.5\n",
      "TRAINING - Step: 610, loss: 0.1532563865184784, rolling accuracy: 77.66393280029297\n",
      "TRAINING - Step: 620, loss: 0.0645902156829834, rolling accuracy: 78.0241928100586\n",
      "TRAINING - Step: 630, loss: 0.06343637406826019, rolling accuracy: 78.17460632324219\n",
      "TRAINING - Step: 640, loss: 0.0628407746553421, rolling accuracy: 78.3203125\n",
      "+++ FEDERATED MODEL 3, EPOCH: 3 +++++++++\n",
      "TRAINING - Step: 10, loss: 7.370444297790527, rolling accuracy: 12.5\n",
      "TRAINING - Step: 20, loss: 6.51995849609375, rolling accuracy: 50.0\n",
      "TRAINING - Step: 30, loss: 5.647168159484863, rolling accuracy: 54.16666793823242\n",
      "TRAINING - Step: 40, loss: 5.045135974884033, rolling accuracy: 59.375\n",
      "TRAINING - Step: 50, loss: 0.059695765376091, rolling accuracy: 67.5\n",
      "TRAINING - Step: 60, loss: 0.05956041067838669, rolling accuracy: 72.91667175292969\n",
      "TRAINING - Step: 70, loss: 3.3211448192596436, rolling accuracy: 73.21428680419922\n",
      "TRAINING - Step: 80, loss: 3.0958969593048096, rolling accuracy: 75.0\n",
      "TRAINING - Step: 90, loss: 2.415004253387451, rolling accuracy: 76.3888931274414\n",
      "TRAINING - Step: 100, loss: 1.9543991088867188, rolling accuracy: 77.5\n",
      "TRAINING - Step: 110, loss: 1.7025569677352905, rolling accuracy: 77.2727279663086\n",
      "TRAINING - Step: 120, loss: 1.1443814039230347, rolling accuracy: 76.04167175292969\n",
      "TRAINING - Step: 130, loss: 1.3910061120986938, rolling accuracy: 76.92308044433594\n",
      "TRAINING - Step: 140, loss: 1.0314363241195679, rolling accuracy: 75.89286041259766\n",
      "TRAINING - Step: 150, loss: 0.7625002861022949, rolling accuracy: 75.83333587646484\n",
      "TRAINING - Step: 160, loss: 0.8306028842926025, rolling accuracy: 75.0\n",
      "TRAINING - Step: 170, loss: 0.7207172513008118, rolling accuracy: 75.0\n",
      "TRAINING - Step: 180, loss: 0.325332373380661, rolling accuracy: 74.30555725097656\n",
      "TRAINING - Step: 190, loss: 0.05500032380223274, rolling accuracy: 74.34210968017578\n",
      "TRAINING - Step: 200, loss: 0.05277052894234657, rolling accuracy: 75.0\n",
      "TRAINING - Step: 210, loss: 0.3189319372177124, rolling accuracy: 74.4047622680664\n",
      "TRAINING - Step: 220, loss: 0.05434071272611618, rolling accuracy: 75.56817626953125\n",
      "TRAINING - Step: 230, loss: 0.03472493588924408, rolling accuracy: 76.63043212890625\n",
      "TRAINING - Step: 240, loss: 0.15821808576583862, rolling accuracy: 77.08333587646484\n",
      "TRAINING - Step: 250, loss: 0.19660481810569763, rolling accuracy: 77.5\n",
      "TRAINING - Step: 260, loss: 0.13092036545276642, rolling accuracy: 77.88461303710938\n",
      "TRAINING - Step: 270, loss: 0.04227226600050926, rolling accuracy: 78.70370483398438\n",
      "TRAINING - Step: 280, loss: 0.1595977395772934, rolling accuracy: 78.57142639160156\n",
      "TRAINING - Step: 290, loss: 0.053541623055934906, rolling accuracy: 78.87931060791016\n",
      "TRAINING - Step: 300, loss: 0.17701813578605652, rolling accuracy: 78.75\n",
      "TRAINING - Step: 310, loss: 0.11432179063558578, rolling accuracy: 79.43548583984375\n",
      "TRAINING - Step: 320, loss: 0.14753146469593048, rolling accuracy: 79.296875\n",
      "TRAINING - Step: 330, loss: 0.14116236567497253, rolling accuracy: 79.16666412353516\n",
      "TRAINING - Step: 340, loss: 0.17904365062713623, rolling accuracy: 78.67647552490234\n",
      "TRAINING - Step: 350, loss: 0.05298833176493645, rolling accuracy: 79.28571319580078\n",
      "TRAINING - Step: 360, loss: 0.05850786715745926, rolling accuracy: 79.86111450195312\n",
      "TRAINING - Step: 370, loss: 0.40880465507507324, rolling accuracy: 79.0540542602539\n",
      "TRAINING - Step: 380, loss: 0.2777402997016907, rolling accuracy: 78.28947448730469\n",
      "TRAINING - Step: 390, loss: 0.15058861672878265, rolling accuracy: 78.52564239501953\n",
      "TRAINING - Step: 400, loss: 0.15856263041496277, rolling accuracy: 78.4375\n",
      "TRAINING - Step: 410, loss: 0.25453031063079834, rolling accuracy: 78.04878234863281\n",
      "TRAINING - Step: 420, loss: 0.06388811022043228, rolling accuracy: 77.9761962890625\n",
      "TRAINING - Step: 430, loss: 0.16224391758441925, rolling accuracy: 77.90697479248047\n",
      "TRAINING - Step: 440, loss: 0.26447218656539917, rolling accuracy: 77.55681610107422\n",
      "TRAINING - Step: 450, loss: 0.32860127091407776, rolling accuracy: 76.94444274902344\n",
      "TRAINING - Step: 460, loss: 0.14459770917892456, rolling accuracy: 77.17391204833984\n",
      "TRAINING - Step: 470, loss: 0.06578874588012695, rolling accuracy: 76.86170196533203\n",
      "TRAINING - Step: 480, loss: 0.060034316033124924, rolling accuracy: 77.08333587646484\n",
      "TRAINING - Step: 490, loss: 0.16122232377529144, rolling accuracy: 76.78571319580078\n",
      "TRAINING - Step: 500, loss: 0.11890346556901932, rolling accuracy: 76.75\n",
      "TRAINING - Step: 510, loss: 0.06633260846138, rolling accuracy: 76.71569061279297\n",
      "TRAINING - Step: 520, loss: 0.060935407876968384, rolling accuracy: 77.16345977783203\n",
      "TRAINING - Step: 530, loss: 0.14744774997234344, rolling accuracy: 77.12264251708984\n",
      "TRAINING - Step: 540, loss: 0.05745669826865196, rolling accuracy: 77.54629516601562\n",
      "TRAINING - Step: 550, loss: 0.15923680365085602, rolling accuracy: 77.2727279663086\n",
      "TRAINING - Step: 560, loss: 0.21332105994224548, rolling accuracy: 77.45536041259766\n",
      "TRAINING - Step: 570, loss: 0.06959778070449829, rolling accuracy: 77.41228485107422\n",
      "TRAINING - Step: 580, loss: 0.14725624024868011, rolling accuracy: 77.37068939208984\n",
      "TRAINING - Step: 590, loss: 0.06144888326525688, rolling accuracy: 77.54237365722656\n",
      "TRAINING - Step: 600, loss: 0.06426926702260971, rolling accuracy: 77.5\n",
      "TRAINING - Step: 610, loss: 0.0624057874083519, rolling accuracy: 77.66393280029297\n",
      "TRAINING - Step: 620, loss: 0.1437252014875412, rolling accuracy: 77.62096405029297\n",
      "TRAINING - Step: 630, loss: 0.20567424595355988, rolling accuracy: 77.77777862548828\n",
      "TRAINING - Step: 640, loss: 0.15149493515491486, rolling accuracy: 77.5390625\n",
      "TESTING - Loss: 3543791042560.0, Accuracy: 9.006211280822754\n",
      "+++ FEDERATED MODEL 0, EPOCH: 4 +++++++++\n",
      "TRAINING - Step: 10, loss: 4.998091220855713, rolling accuracy: 12.5\n",
      "TRAINING - Step: 20, loss: 4.406008720397949, rolling accuracy: 12.5\n",
      "TRAINING - Step: 30, loss: 3.7453629970550537, rolling accuracy: 8.333333969116211\n",
      "TRAINING - Step: 40, loss: 3.812182664871216, rolling accuracy: 9.375\n",
      "TRAINING - Step: 50, loss: 3.6412606239318848, rolling accuracy: 12.5\n",
      "TRAINING - Step: 60, loss: 3.557197332382202, rolling accuracy: 12.500000953674316\n",
      "TRAINING - Step: 70, loss: 2.4177849292755127, rolling accuracy: 14.285714149475098\n",
      "TRAINING - Step: 80, loss: 1.8297315835952759, rolling accuracy: 12.5\n",
      "TRAINING - Step: 90, loss: 2.473546266555786, rolling accuracy: 12.5\n",
      "TRAINING - Step: 100, loss: 1.4231966733932495, rolling accuracy: 13.75\n",
      "TRAINING - Step: 110, loss: 1.9735469818115234, rolling accuracy: 12.5\n",
      "TRAINING - Step: 120, loss: 1.485978364944458, rolling accuracy: 12.500000953674316\n",
      "TRAINING - Step: 130, loss: 0.13560113310813904, rolling accuracy: 13.461538314819336\n",
      "TRAINING - Step: 140, loss: 1.0966496467590332, rolling accuracy: 13.392857551574707\n",
      "TRAINING - Step: 150, loss: 0.12028129398822784, rolling accuracy: 15.0\n",
      "TRAINING - Step: 160, loss: 0.9190980195999146, rolling accuracy: 14.84375\n",
      "TRAINING - Step: 170, loss: 0.8634998798370361, rolling accuracy: 18.382352828979492\n",
      "TRAINING - Step: 180, loss: 0.3698712885379791, rolling accuracy: 19.44444465637207\n",
      "TRAINING - Step: 190, loss: 0.5417052507400513, rolling accuracy: 20.394737243652344\n",
      "TRAINING - Step: 200, loss: 0.3010275959968567, rolling accuracy: 20.625\n",
      "TRAINING - Step: 210, loss: 0.1057952493429184, rolling accuracy: 22.0238094329834\n",
      "TRAINING - Step: 220, loss: 0.12526947259902954, rolling accuracy: 22.727272033691406\n",
      "TRAINING - Step: 230, loss: 1.2215042114257812, rolling accuracy: 22.282608032226562\n",
      "TRAINING - Step: 240, loss: 0.25613871216773987, rolling accuracy: 21.875001907348633\n",
      "TRAINING - Step: 250, loss: 0.25793296098709106, rolling accuracy: 21.500001907348633\n",
      "TRAINING - Step: 260, loss: 0.0855192020535469, rolling accuracy: 21.634614944458008\n",
      "TRAINING - Step: 270, loss: 0.14400655031204224, rolling accuracy: 21.759258270263672\n",
      "TRAINING - Step: 280, loss: 0.12184934318065643, rolling accuracy: 22.321428298950195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 290, loss: 0.16172467172145844, rolling accuracy: 22.84482765197754\n",
      "TRAINING - Step: 300, loss: 0.22463029623031616, rolling accuracy: 22.5\n",
      "TRAINING - Step: 310, loss: 0.5421658158302307, rolling accuracy: 22.177419662475586\n",
      "TRAINING - Step: 320, loss: 0.32869142293930054, rolling accuracy: 21.484375\n",
      "TRAINING - Step: 330, loss: 0.5017850399017334, rolling accuracy: 21.59090805053711\n",
      "TRAINING - Step: 340, loss: 0.260666161775589, rolling accuracy: 21.691177368164062\n",
      "TRAINING - Step: 350, loss: 0.11426043510437012, rolling accuracy: 22.5\n",
      "TRAINING - Step: 360, loss: 0.12502487003803253, rolling accuracy: 22.916667938232422\n",
      "TRAINING - Step: 370, loss: 0.15163320302963257, rolling accuracy: 23.31081199645996\n",
      "TRAINING - Step: 380, loss: 0.11800909787416458, rolling accuracy: 23.68421173095703\n",
      "TRAINING - Step: 390, loss: 0.0793057233095169, rolling accuracy: 24.038461685180664\n",
      "TRAINING - Step: 400, loss: 0.15283195674419403, rolling accuracy: 24.0625\n",
      "TRAINING - Step: 410, loss: 0.3012329041957855, rolling accuracy: 24.390243530273438\n",
      "TRAINING - Step: 420, loss: 0.09934099018573761, rolling accuracy: 24.10714340209961\n",
      "TRAINING - Step: 430, loss: 0.1313949078321457, rolling accuracy: 24.127906799316406\n",
      "TRAINING - Step: 440, loss: 0.0851711556315422, rolling accuracy: 24.14772605895996\n",
      "TRAINING - Step: 450, loss: 0.23905840516090393, rolling accuracy: 24.72222328186035\n",
      "TRAINING - Step: 460, loss: 0.09739485383033752, rolling accuracy: 24.456520080566406\n",
      "TRAINING - Step: 470, loss: 0.1255529224872589, rolling accuracy: 24.46808433532715\n",
      "TRAINING - Step: 480, loss: 0.13356807827949524, rolling accuracy: 24.479167938232422\n",
      "TRAINING - Step: 490, loss: 0.10765687376260757, rolling accuracy: 24.999998092651367\n",
      "TRAINING - Step: 500, loss: 0.11371985077857971, rolling accuracy: 25.250001907348633\n",
      "TRAINING - Step: 510, loss: 0.08396345376968384, rolling accuracy: 25.245100021362305\n",
      "TRAINING - Step: 520, loss: 0.17796024680137634, rolling accuracy: 25.0\n",
      "TRAINING - Step: 530, loss: 0.13050112128257751, rolling accuracy: 25.0\n",
      "TRAINING - Step: 540, loss: 0.1516377031803131, rolling accuracy: 25.462963104248047\n",
      "TRAINING - Step: 550, loss: 0.09766331315040588, rolling accuracy: 25.227272033691406\n",
      "TRAINING - Step: 560, loss: 0.07308303564786911, rolling accuracy: 25.446428298950195\n",
      "TRAINING - Step: 570, loss: 0.08349709212779999, rolling accuracy: 25.438596725463867\n",
      "TRAINING - Step: 580, loss: 0.19201011955738068, rolling accuracy: 26.077585220336914\n",
      "TRAINING - Step: 590, loss: 0.11203351616859436, rolling accuracy: 26.483051300048828\n",
      "TRAINING - Step: 600, loss: 0.08774016797542572, rolling accuracy: 26.45833396911621\n",
      "TRAINING - Step: 610, loss: 0.10959461331367493, rolling accuracy: 26.02458953857422\n",
      "TRAINING - Step: 620, loss: 0.07882224768400192, rolling accuracy: 26.20967674255371\n",
      "TRAINING - Step: 630, loss: 0.18633733689785004, rolling accuracy: 26.19047737121582\n",
      "TRAINING - Step: 640, loss: 0.0909426361322403, rolling accuracy: 25.9765625\n",
      "+++ FEDERATED MODEL 1, EPOCH: 4 +++++++++\n",
      "TRAINING - Step: 10, loss: 4.94356107711792, rolling accuracy: 25.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f34909458459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_federated_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfederated_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_optimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_training_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-3e31f605d0e9>\u001b[0m in \u001b[0;36mrun_federated_training\u001b[1;34m(federated_model, client_models, client_optimizers, client_params, client_training_loader)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mclient_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient_optimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mtrain_federated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_training_loader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfederated_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mclient_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend_params_to_global_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-781a5effb863>\u001b[0m in \u001b[0;36mtrain_federated\u001b[1;34m(model, data_loader, optimizer, loss)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_federated_training(federated_model, client_models, client_optimizers, client_params, client_training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
