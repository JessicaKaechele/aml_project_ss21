{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zwZJFe5-pCLV",
    "outputId": "5b270b81-a9e7-43d9-cc42-378c0fed3740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from click->opendatasets) (0.4.4)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=514e82bccb09ad66a25a018a02df371acbb91e3cd7ea61b35d18ad62cfb6026a\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: urllib3, text-unidecode, idna, charset-normalizer, tqdm, requests, python-slugify, kaggle, click, opendatasets\n",
      "Successfully installed charset-normalizer-2.0.4 click-8.0.1 idna-3.2 kaggle-1.5.12 opendatasets-0.1.20 python-slugify-5.0.2 requests-2.26.0 text-unidecode-1.3 tqdm-4.62.2 urllib3-1.26.6\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.40.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-win_amd64.whl (909 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=cba4853eab105290088fff00de7b6e4c7f189b2d1e3e9094458f98170f6102df\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=9aa1f355bbc18674d560a501e6ff2e800f6b0896e532c633acd35f327272bdda\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33696 sha256=bd42bf4719ea3009f798680ef6aa900c02b2065cbfb750b42b0272a443e4851b\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-2.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_federated\n",
      "  Downloading tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
      "  Downloading tensorflow_federated-0.18.0-py2.py3-none-any.whl (578 kB)\n",
      "  Downloading tensorflow_federated-0.17.0-py2.py3-none-any.whl (517 kB)\n",
      "Collecting retrying~=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting absl-py~=0.9.0\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting grpcio~=1.29.0\n",
      "  Downloading grpcio-1.29.0-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "Collecting tensorflow-model-optimization~=0.4.0\n",
      "  Downloading tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172 kB)\n",
      "Collecting numpy~=1.18.4\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Collecting tensorflow-privacy~=0.5.0\n",
      "  Downloading tensorflow_privacy-0.5.2-py3-none-any.whl (192 kB)\n",
      "Collecting portpicker~=1.3.1\n",
      "  Downloading portpicker-1.3.9-py3-none-any.whl (13 kB)\n",
      "Collecting tensorflow-addons~=0.11.1\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-win_amd64.whl (911 kB)\n",
      "Collecting tensorflow~=2.3.0\n",
      "  Downloading tensorflow-2.3.4-cp38-cp38-win_amd64.whl (342.7 MB)\n",
      "Collecting semantic-version~=2.8.5\n",
      "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
      "Collecting attrs~=19.3.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting cachetools~=3.1.1\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "Requirement already satisfied: six in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from absl-py~=0.9.0->tensorflow_federated) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.36.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.2)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorflow~=2.3.0->tensorflow_federated) (1.6.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.26.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow_federated) (3.1.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Collecting scipy>=0.17\n",
      "  Downloading scipy-1.7.1-cp38-cp38-win_amd64.whl (33.7 MB)\n",
      "Collecting mpmath\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Building wheels for collected packages: absl-py, retrying\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121930 sha256=d48858ed8d8c44579afff41bc5db58d440a8598b2bf8ccf023c3260a634e1ea7\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\1d\\10\\8e\\2f79b924179ff1e6510933d63eb851bea01054fff262343b7a\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=76388d1e8bf5d5b6e5964a04050c5aad93635b4b8aa1690deb4f6e60f34ed0c6\n",
      "  Stored in directory: c:\\users\\donat\\appdata\\local\\pip\\cache\\wheels\\c4\\a7\\48\\0a434133f6d56e878ca511c0e6c38326907c0792f67b476e56\n",
      "Successfully built absl-py retrying\n",
      "Installing collected packages: cachetools, numpy, grpcio, absl-py, typeguard, tensorflow-estimator, scipy, mpmath, h5py, gast, dm-tree, tensorflow-privacy, tensorflow-model-optimization, tensorflow-addons, tensorflow, semantic-version, retrying, portpicker, attrs, tensorflow-federated\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.40.0\n",
      "    Uninstalling grpcio-1.40.0:\n",
      "      Successfully uninstalled grpcio-1.40.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "Successfully installed absl-py-0.9.0 attrs-19.3.0 cachetools-3.1.1 dm-tree-0.1.6 gast-0.3.3 grpcio-1.29.0 h5py-2.10.0 mpmath-1.2.1 numpy-1.18.5 portpicker-1.3.9 retrying-1.3.3 scipy-1.7.1 semantic-version-2.8.5 tensorflow-2.3.4 tensorflow-addons-0.11.2 tensorflow-estimator-2.3.0 tensorflow-federated-0.17.0 tensorflow-model-optimization-0.4.1 tensorflow-privacy-0.5.2 typeguard-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.9.0-cp38-cp38-win_amd64.whl (222.0 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp38-cp38-win_amd64.whl (920 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: torch==1.9.0 in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from torchvision) (1.9.0)\n",
      "Collecting pillow>=5.3.0\n",
      "  Downloading Pillow-8.3.2-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\donat\\anaconda3\\envs\\python 3_8\\lib\\site-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-8.3.2 torchvision-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jpd3iU1El1ZT"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TraicDeCpHii",
    "outputId": "ec940e7b-0a35-4c28-9a85-98f7d7b3f6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\chest-xray-covid19-pneumonia\" (use force=True to force download)\n",
      "Skipping, found downloaded files in \".\\novel-corona-virus-2019-dataset\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\")\n",
    "od.download(\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JPFiGGjbhta_"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001 # 0.0001\n",
    "MAX_EPOCHS = 25\n",
    "TARGET_FOLDER = \"weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WvAJ8eTnTRkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW1RF4LvUUsG"
   },
   "source": [
    "Resize Images to 244, 244. By using to_tensor the images are already normalized between 0 and 1. \"The image object is an array of (244, 244, 3) should be flattened to be list (178, 608).\" What? wie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7Z_GY07pSzQT"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((244, 244))\n",
    "                                , transforms.ToTensor()]\n",
    "                               #, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # find mean and std of dataset\n",
    "                              )\n",
    "\n",
    "test_set = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/test', transform=transform)\n",
    "\n",
    "train_set = dataset = datasets.ImageFolder('chest-xray-covid19-pneumonia/Data/train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_preparation(labels):\n",
    "    labels = np.array(labels)\n",
    "    labels[labels > 0] = 1\n",
    "    return list(labels)\n",
    "\n",
    "def label_preparation_tensor(labels):\n",
    "    labels[labels > 0] = 1\n",
    "    return labels\n",
    "\n",
    "train_set.targets = label_preparation(train_set.targets)\n",
    "\n",
    "test_set.targets = label_preparation(test_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VAi62OkRTZfV"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVxct9thFVz2"
   },
   "source": [
    "In Paper aus References haben sie unter anderen ResNet18 benutzt: https://arxiv.org/pdf/2007.05592.pdf\n",
    "\n",
    "Deshalb wÃ¼rde ich das probieren.\n",
    "\n",
    "TODO: test different ones, e.g. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWhXD92ZRzNP",
    "outputId": "b473ca2e-2e15-4480-dab7-4fd210df225a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "icqqTTRN-ht6"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(result, labels):\n",
    "    result = torch.sigmoid(result).round()\n",
    "    \n",
    "    correct_results_sum = (result == labels).sum().float()\n",
    "    acc = correct_results_sum/labels.shape[0]\n",
    "    acc *= 100\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "A5Me-0EHR_md"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, loss):\n",
    "    \"\"\"\n",
    "    model -- neural net\n",
    "    data_loader -- dataloader for train images\n",
    "    optimizer -- optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(data_loader, 1):\n",
    "        images = images.to(device)\n",
    "        labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result = model(images)\n",
    "        targets = labels.unsqueeze(1).float()\n",
    "\n",
    "        loss_value = loss(result.float(), targets)\n",
    "\n",
    "        # backpropagation\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        if step % 10 == 0:\n",
    "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
    "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Xk2CR_whWTxw"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss):\n",
    "    \"\"\"    \n",
    "    model -- neural net \n",
    "    test_loader -- dataloader of test images\n",
    "    epoch -- current epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value = 0\n",
    "        accuracy = 0\n",
    "        for step, [images, labels] in enumerate(test_loader, 1):\n",
    "            images = images.to(device)\n",
    "            labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "            result = model(images)\n",
    "            targets = labels.detach().unsqueeze(1).float()\n",
    "\n",
    "            loss_value += loss(result.detach(), targets)\n",
    "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
    "\n",
    "        loss_value /= step\n",
    "        accuracy /=  step\n",
    "  \n",
    "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
    "    return accuracy > 93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "jzfyMxdBVoB1",
    "outputId": "c3a6aff3-d0a6-4a92-fd67-8d72310e227d"
   },
   "outputs": [],
   "source": [
    "def run_local_training():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "    model.to(device)\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # use pos weights because of unbalanced data set\n",
    "    loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        print(f\"+++ EPOCH: {epoch+1} +++++++++\")\n",
    "        torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy (non-federated)\n",
    "        train(model, train_loader, optimizer, loss)\n",
    "        break\n",
    "        # save interim weights\n",
    "        torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "        if test(model, test_loader, loss) and epoch > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_local_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc9Z1kAKniiG"
   },
   "source": [
    "# Federated\n",
    "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\n",
    "\n",
    "i don't think we need syft if we just simulate the federation on one machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clients = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_model = torchvision.models.resnet18(pretrained=False, num_classes=1)\n",
    "federated_model.to(device)\n",
    "\n",
    "def reset_client_param_list():\n",
    "    return [None] * number_of_clients\n",
    "    \n",
    "client_params = reset_client_param_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_models = [torchvision.models.resnet18(pretrained=False, num_classes=1).to(device) for _ in range(number_of_clients)]\n",
    "client_optimizers = [torch.optim.Adam(client_models[idx].parameters()) for idx in range(number_of_clients)]\n",
    "client_training_loader = [torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4) for _ in range(number_of_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_params_to_global_model(client_params, client_idx, params):\n",
    "    client_params[client_idx] = params\n",
    "    for idx in range (number_of_clients):\n",
    "        if client_params[idx] is None:\n",
    "            return client_params, False\n",
    "        \n",
    "    return client_params, True\n",
    "\n",
    "def send_params_to_client(federated_model, client):\n",
    "    client.load_state_dict(federated_model.state_dict(), True)\n",
    "    return client\n",
    "    \n",
    "def federated_average(federated_model, client_params):\n",
    "    with torch.no_grad():\n",
    "        average_weights = OrderedDict()\n",
    "\n",
    "        # check if client_params has no Nones?\n",
    "        for client_param in client_params:\n",
    "            for key, value in client_param.items():\n",
    "                if key in average_weights:\n",
    "                    average_weights[key] += value.clone()\n",
    "                else:\n",
    "                    average_weights[key] = value.clone()\n",
    "\n",
    "        for key, value in average_weights.items():\n",
    "            average_weights[key] = torch.div(average_weights[key], len(client_params), rounding_mode='floor')\n",
    "                \n",
    "                \n",
    "    federated_model.load_state_dict(average_weights, True)\n",
    "    return federated_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_federated(model, data_loader, optimizer, loss):\n",
    "    \"\"\"\n",
    "    model -- neural net\n",
    "    data_loader -- dataloader for train images\n",
    "    optimizer -- optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    accuracy = 0\n",
    "    for step, [images, labels] in enumerate(data_loader, 1):\n",
    "        images = images.to(device)\n",
    "        labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result = model(images)\n",
    "        targets = labels.unsqueeze(1).float()\n",
    "\n",
    "        loss_value = loss(result.float(), targets)\n",
    "\n",
    "        # backpropagation\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "                                    \n",
    "        if step % 10 == 0:\n",
    "            accuracy += calc_accuracy(result, labels.unsqueeze(1))\n",
    "            print(f\"TRAINING - Step: {step}, loss: {loss_value}, rolling accuracy: {accuracy*10/step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_federated(model, test_loader, loss):\n",
    "    \"\"\"    \n",
    "    model -- neural net \n",
    "    test_loader -- dataloader of test images\n",
    "    epoch -- current epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # use pos weights because of unbalanced data set\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy (non-federated)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_value = 0\n",
    "        accuracy = 0\n",
    "        for step, [images, labels] in enumerate(test_loader, 1):\n",
    "            images = images.to(device)\n",
    "            labels = label_preparation_tensor(labels.to(device))\n",
    "\n",
    "            result = model(images)\n",
    "            targets = labels.detach().unsqueeze(1).float()\n",
    "\n",
    "            loss_value += loss(result.detach(), targets)\n",
    "            accuracy += calc_accuracy(result.detach(), labels.detach().unsqueeze(1))\n",
    "\n",
    "        loss_value /= step\n",
    "        accuracy /=  step\n",
    "  \n",
    "    print(f\"TESTING - Loss: {loss_value}, Accuracy: {accuracy}\")\n",
    "    return accuracy > 93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "hR2YMmTLs49U",
    "outputId": "f2c51ce1-3579-46de-9458-7b03e73c8cec"
   },
   "outputs": [],
   "source": [
    "def run_federated_training(federated_model, client_models, client_optimizers, client_params, client_training_loader):\n",
    "    # use pos weights because of unbalanced data set\n",
    "    federated_loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1./10])).to(device) # binary crossentropy\n",
    "    # federated_loss = torch.nn.CrossEntropyLoss(weight=torch.tensor([1./10])).to(device) # sparce categorical crossentropy (federated)\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        for client_idx in range (number_of_clients):\n",
    "            print(f\"+++ FEDERATED MODEL {client_idx}, EPOCH: {epoch+1} +++++++++\")\n",
    "\n",
    "            client_model = client_models[client_idx]\n",
    "            client_model = send_params_to_client(federated_model, client_model)\n",
    "\n",
    "            client_optimizer = client_optimizers[client_idx]\n",
    "\n",
    "            train_federated(client_model, client_training_loader[client_idx], client_optimizer, federated_loss)\n",
    "\n",
    "            client_params, _ = send_params_to_global_model(client_params, client_idx, client_model.state_dict())\n",
    "\n",
    "\n",
    "            # save interim weights\n",
    "            #torch.save(model.state_dict(), f'./{TARGET_FOLDER}/epoch_{epoch}.ckpt')\n",
    "\n",
    "\n",
    "        federated_model = federated_average(federated_model, client_params)\n",
    "        client_params = reset_client_param_list()\n",
    "        \n",
    "        test_federated(federated_model, test_loader, federated_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLC89DQ2rKQe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ FEDERATED MODEL 0, EPOCH: 1 +++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Donat\\Anaconda3\\envs\\Python_38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Step: 10, loss: 0.04027961939573288, rolling accuracy: 89.0625\n",
      "TRAINING - Step: 20, loss: 0.015832576900720596, rolling accuracy: 90.625\n",
      "TRAINING - Step: 30, loss: 0.10454931855201721, rolling accuracy: 92.18750762939453\n",
      "TRAINING - Step: 40, loss: 0.05478958040475845, rolling accuracy: 89.84375\n",
      "TRAINING - Step: 50, loss: 0.05593022704124451, rolling accuracy: 91.25\n",
      "TRAINING - Step: 60, loss: 0.055436667054891586, rolling accuracy: 90.88542175292969\n",
      "TRAINING - Step: 70, loss: 0.009545935317873955, rolling accuracy: 91.74107360839844\n",
      "TRAINING - Step: 80, loss: 0.017689980566501617, rolling accuracy: 92.1875\n",
      "+++ FEDERATED MODEL 1, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.03219511732459068, rolling accuracy: 93.75\n",
      "TRAINING - Step: 20, loss: 0.057915907353162766, rolling accuracy: 91.40625\n",
      "TRAINING - Step: 30, loss: 0.026234066113829613, rolling accuracy: 90.62500762939453\n",
      "TRAINING - Step: 40, loss: 0.010355543345212936, rolling accuracy: 91.796875\n",
      "TRAINING - Step: 50, loss: 0.022795990109443665, rolling accuracy: 91.5625\n",
      "TRAINING - Step: 60, loss: 0.03349801152944565, rolling accuracy: 90.88542175292969\n",
      "TRAINING - Step: 70, loss: 0.013544569723308086, rolling accuracy: 91.29463958740234\n",
      "TRAINING - Step: 80, loss: 0.008162589743733406, rolling accuracy: 92.1875\n",
      "+++ FEDERATED MODEL 2, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.055531103163957596, rolling accuracy: 89.0625\n",
      "TRAINING - Step: 20, loss: 0.054730843752622604, rolling accuracy: 90.625\n",
      "TRAINING - Step: 30, loss: 0.014510618522763252, rolling accuracy: 91.66667175292969\n",
      "TRAINING - Step: 40, loss: 0.08889628946781158, rolling accuracy: 91.015625\n",
      "TRAINING - Step: 50, loss: 0.018216107040643692, rolling accuracy: 92.1875\n",
      "TRAINING - Step: 60, loss: 0.07140302658081055, rolling accuracy: 92.70833587646484\n",
      "TRAINING - Step: 70, loss: 0.017583202570676804, rolling accuracy: 93.52678680419922\n",
      "TRAINING - Step: 80, loss: 0.10313040018081665, rolling accuracy: 93.359375\n",
      "+++ FEDERATED MODEL 3, EPOCH: 1 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.036830492317676544, rolling accuracy: 85.9375\n",
      "TRAINING - Step: 20, loss: 0.03989318385720253, rolling accuracy: 86.71875\n",
      "TRAINING - Step: 30, loss: 0.02244202420115471, rolling accuracy: 89.58333587646484\n",
      "TRAINING - Step: 40, loss: 0.05152153596282005, rolling accuracy: 89.84375\n",
      "TRAINING - Step: 50, loss: 0.012881388887763023, rolling accuracy: 90.9375\n",
      "TRAINING - Step: 60, loss: 0.022447951138019562, rolling accuracy: 91.40625762939453\n",
      "TRAINING - Step: 70, loss: 0.02162584662437439, rolling accuracy: 91.29463958740234\n",
      "TRAINING - Step: 80, loss: 0.017762262374162674, rolling accuracy: 91.40625\n",
      "TESTING - Loss: 0.12315740436315536, Accuracy: 8.630952835083008\n",
      "+++ FEDERATED MODEL 0, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.07974245399236679, rolling accuracy: 96.875\n",
      "TRAINING - Step: 20, loss: 0.08247608691453934, rolling accuracy: 96.09375\n",
      "TRAINING - Step: 30, loss: 0.12551921606063843, rolling accuracy: 94.79167175292969\n",
      "TRAINING - Step: 40, loss: 0.14606255292892456, rolling accuracy: 77.34375\n",
      "TRAINING - Step: 50, loss: 0.17367011308670044, rolling accuracy: 79.375\n",
      "TRAINING - Step: 60, loss: 0.09820565581321716, rolling accuracy: 74.73958587646484\n",
      "TRAINING - Step: 70, loss: 0.12395155429840088, rolling accuracy: 76.11607360839844\n",
      "TRAINING - Step: 80, loss: 0.06941373646259308, rolling accuracy: 78.515625\n",
      "+++ FEDERATED MODEL 1, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.10703621804714203, rolling accuracy: 93.75\n",
      "TRAINING - Step: 20, loss: 0.08596550673246384, rolling accuracy: 95.3125\n",
      "TRAINING - Step: 30, loss: 0.12128991633653641, rolling accuracy: 94.27083587646484\n",
      "TRAINING - Step: 40, loss: 0.10445992648601532, rolling accuracy: 93.359375\n",
      "TRAINING - Step: 50, loss: 0.12163230776786804, rolling accuracy: 93.125\n",
      "TRAINING - Step: 60, loss: 0.11704020202159882, rolling accuracy: 92.44792175292969\n",
      "TRAINING - Step: 70, loss: 0.1741764098405838, rolling accuracy: 87.05357360839844\n",
      "TRAINING - Step: 80, loss: 0.10240212827920914, rolling accuracy: 88.0859375\n",
      "+++ FEDERATED MODEL 2, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.1119571402668953, rolling accuracy: 93.75\n",
      "TRAINING - Step: 20, loss: 0.10018961131572723, rolling accuracy: 92.96875\n",
      "TRAINING - Step: 30, loss: 0.13753797113895416, rolling accuracy: 88.02083587646484\n",
      "TRAINING - Step: 40, loss: 0.06239961087703705, rolling accuracy: 90.234375\n",
      "TRAINING - Step: 50, loss: 0.1332203447818756, rolling accuracy: 90.3125\n",
      "TRAINING - Step: 60, loss: 0.12608397006988525, rolling accuracy: 89.84375762939453\n",
      "TRAINING - Step: 70, loss: 0.17661234736442566, rolling accuracy: 89.0625\n",
      "TRAINING - Step: 80, loss: 0.12753704190254211, rolling accuracy: 87.109375\n",
      "+++ FEDERATED MODEL 3, EPOCH: 2 +++++++++\n",
      "TRAINING - Step: 10, loss: 0.11505038291215897, rolling accuracy: 92.1875\n",
      "TRAINING - Step: 20, loss: 0.11539363861083984, rolling accuracy: 92.96875\n",
      "TRAINING - Step: 30, loss: 0.10421323776245117, rolling accuracy: 90.62500762939453\n",
      "TRAINING - Step: 40, loss: 0.11957062780857086, rolling accuracy: 91.015625\n",
      "TRAINING - Step: 50, loss: 0.10547623038291931, rolling accuracy: 87.1875\n",
      "TRAINING - Step: 60, loss: 0.17591659724712372, rolling accuracy: 86.97917175292969\n",
      "TRAINING - Step: 70, loss: 0.09873811900615692, rolling accuracy: 87.27678680419922\n",
      "TRAINING - Step: 80, loss: 0.11826722323894501, rolling accuracy: 85.15625\n",
      "TESTING - Loss: 102076596224.0, Accuracy: 8.630952835083008\n",
      "+++ FEDERATED MODEL 0, EPOCH: 3 +++++++++\n",
      "TRAINING - Step: 10, loss: 1.6721434593200684, rolling accuracy: 10.9375\n",
      "TRAINING - Step: 20, loss: 1.9792674779891968, rolling accuracy: 9.375\n"
     ]
    }
   ],
   "source": [
    "run_federated_training(federated_model, client_models, client_optimizers, client_params, client_training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
